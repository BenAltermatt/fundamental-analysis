{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "I have successfully been able to build a sort of data set for this model. Now I want to try training on my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cupy-cuda11x in c:\\python38\\lib\\site-packages (11.2.0)\n",
      "Requirement already satisfied: numpy<1.26,>=1.20 in c:\\python38\\lib\\site-packages (from cupy-cuda11x) (1.23.3)\n",
      "Requirement already satisfied: fastrlock>=0.5 in c:\\python38\\lib\\site-packages (from cupy-cuda11x) (0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp38-cp38-win_amd64.whl (167.3 MB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.13.0-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python38\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\python38\\lib\\site-packages (from torchvision) (1.23.3)\n",
      "Requirement already satisfied: requests in c:\\python38\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python38\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python38\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests->torchvision) (2022.9.24)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: typing-extensions, torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.13.0 torchaudio-0.13.0 torchvision-0.14.0 typing-extensions-4.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install cupy-cuda11x\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037304</td>\n",
       "      <td>0.081930</td>\n",
       "      <td>68.692936</td>\n",
       "      <td>16.198334</td>\n",
       "      <td>3.792215</td>\n",
       "      <td>1.051065</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.366850</td>\n",
       "      <td>0.337912</td>\n",
       "      <td>8.135137</td>\n",
       "      <td>18.134324</td>\n",
       "      <td>3.515821</td>\n",
       "      <td>0.918390</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.108044</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.132520</td>\n",
       "      <td>0.173255</td>\n",
       "      <td>18.516105</td>\n",
       "      <td>6.586352</td>\n",
       "      <td>1.420348</td>\n",
       "      <td>0.761143</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.236057</td>\n",
       "      <td>0.019177</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071330</td>\n",
       "      <td>0.172091</td>\n",
       "      <td>29.580969</td>\n",
       "      <td>5.532793</td>\n",
       "      <td>1.205178</td>\n",
       "      <td>0.746057</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.436893</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.133795</td>\n",
       "      <td>0.211265</td>\n",
       "      <td>12.743329</td>\n",
       "      <td>4.438462</td>\n",
       "      <td>0.957572</td>\n",
       "      <td>0.715781</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.018786</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.265324</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>3.700391</td>\n",
       "      <td>0.383926</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.948329</td>\n",
       "      <td>4.281250</td>\n",
       "      <td>3.332726</td>\n",
       "      <td>0.322385</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>4.395096</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>0.792549</td>\n",
       "      <td>1.878183</td>\n",
       "      <td>2.160751</td>\n",
       "      <td>2.286619</td>\n",
       "      <td>0.339581</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.039290</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.440450</td>\n",
       "      <td>1.614583</td>\n",
       "      <td>2.256612</td>\n",
       "      <td>0.362996</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.056206</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.242760</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.904923</td>\n",
       "      <td>0.341091</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.257109</td>\n",
       "      <td>2.313978</td>\n",
       "      <td>0.037899</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                0.037304              0.081930  68.692936          16.198334   \n",
       "1                0.366850              0.337912   8.135137          18.134324   \n",
       "2                0.132520              0.173255  18.516105           6.586352   \n",
       "3                0.071330              0.172091  29.580969           5.532793   \n",
       "4                0.133795              0.211265  12.743329           4.438462   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           0.600000              1.265324   3.083333           3.700391   \n",
       "136444           0.400000              0.948329   4.281250           3.332726   \n",
       "136445           0.792549              1.878183   2.160751           2.286619   \n",
       "136446           1.200000              1.440450   1.614583           2.256612   \n",
       "136447           0.800000              1.242760   2.250000           2.904923   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       3.792215      1.051065       0.000404     0.111111  0.013801   CMCSA   \n",
       "1       3.515821      0.918390       0.000015     0.000485  0.108044   CMCSA   \n",
       "2       1.420348      0.761143       0.003187     0.236057  0.019177   CMCSA   \n",
       "3       1.205178      0.746057       0.003692     0.436893  0.010185   CMCSA   \n",
       "4       0.957572      0.715781       0.004574     0.233161  0.018786   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  0.383926      0.001104       0.000011     0.000135  0.031129  SNGR.L   \n",
       "136444  0.322385      0.001067       0.256648     4.395096  0.018825  SNGR.L   \n",
       "136445  0.339581      0.001007       0.000037     0.000320  0.039290  SNGR.L   \n",
       "136446  0.362996      0.000928       0.000067     0.000433  0.056206  SNGR.L   \n",
       "136447  0.341091      0.000955       0.257109     2.313978  0.037899  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# first we need to read the data in\n",
    "\n",
    "data_name = 'dataset.csv'\n",
    "\n",
    "all_data = pd.read_csv(data_name)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a new strategy for normalization\n",
    "# we'll update each value to be the z-score of the value within the overall distribution\n",
    "def zscore_data(data, cols_to_normalize=None, feature_size=9):\n",
    "    cols_for_normalizing = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_normalize:\n",
    "        cols_for_normalizing = cols_to_normalize\n",
    "    \n",
    "    for col in cols_for_normalizing:\n",
    "        col_data = data[col]\n",
    "        mu = np.mean(col_data)\n",
    "        sigma = np.std(col_data)\n",
    "\n",
    "        data[col] = (data[col] - mu) / sigma\n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_data(data, cols_to_normalize=None, feature_size=9):\n",
    "    cols_for_norm = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_normalize:\n",
    "        cols_for_norm = cols_to_normalize\n",
    "    \n",
    "    for col in cols_for_norm:\n",
    "        col_data = data[col]\n",
    "        min_val = np.min(col_data)\n",
    "        max_val = np.max(col_data)\n",
    "\n",
    "        data[col] = (data[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return data\n",
    "\n",
    "all_data = normalize_data(zscore_data(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.789873</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518706</td>\n",
       "      <td>11.567448</td>\n",
       "      <td>8.644031</td>\n",
       "      <td>11.667213</td>\n",
       "      <td>-7.813165</td>\n",
       "      <td>9.288283</td>\n",
       "      <td>5.373945</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.789891</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567466</td>\n",
       "      <td>8.643982</td>\n",
       "      <td>11.667212</td>\n",
       "      <td>-11.112948</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374382</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567357</td>\n",
       "      <td>8.643613</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.748617</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373970</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.789875</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567347</td>\n",
       "      <td>8.643575</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.601492</td>\n",
       "      <td>9.288314</td>\n",
       "      <td>5.373929</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567337</td>\n",
       "      <td>8.643531</td>\n",
       "      <td>11.667210</td>\n",
       "      <td>-5.387330</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>9.789904</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567330</td>\n",
       "      <td>8.643430</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-11.419512</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374026</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>9.789893</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567326</td>\n",
       "      <td>8.643419</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.360050</td>\n",
       "      <td>9.288680</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-10.202989</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374064</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>9.789938</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643426</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-9.609178</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374142</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567322</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.358257</td>\n",
       "      <td>9.288487</td>\n",
       "      <td>5.374057</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                9.789873               14.7112  18.518706          11.567448   \n",
       "1                9.789891               14.7112  18.518705          11.567466   \n",
       "2                9.789878               14.7112  18.518705          11.567357   \n",
       "3                9.789875               14.7112  18.518705          11.567347   \n",
       "4                9.789878               14.7112  18.518705          11.567337   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           9.789904               14.7112  18.518705          11.567330   \n",
       "136444           9.789893               14.7112  18.518705          11.567326   \n",
       "136445           9.789915               14.7112  18.518705          11.567316   \n",
       "136446           9.789938               14.7112  18.518705          11.567316   \n",
       "136447           9.789915               14.7112  18.518705          11.567322   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       8.644031     11.667213      -7.813165     9.288283  5.373945   CMCSA   \n",
       "1       8.643982     11.667212     -11.112948     9.288273  5.374382   CMCSA   \n",
       "2       8.643613     11.667211      -5.748617     9.288295  5.373970   CMCSA   \n",
       "3       8.643575     11.667211      -5.601492     9.288314  5.373929   CMCSA   \n",
       "4       8.643531     11.667210      -5.387330     9.288295  5.373969   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  8.643430     11.667204     -11.419512     9.288273  5.374026  SNGR.L   \n",
       "136444  8.643419     11.667204      -1.360050     9.288680  5.373969  SNGR.L   \n",
       "136445  8.643422     11.667204     -10.202989     9.288273  5.374064  SNGR.L   \n",
       "136446  8.643426     11.667204      -9.609178     9.288273  5.374142  SNGR.L   \n",
       "136447  8.643422     11.667204      -1.358257     9.288487  5.374057  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We may need to consolidate larger values by taking the log of our data before\n",
    "# trying to pull out outliers\n",
    "\n",
    "def log_data(data, cols_to_log=None, feature_size=9):\n",
    "    cols_for_logging = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_log:\n",
    "        cols_for_logging = cols_to_log\n",
    "\n",
    "    for col in cols_for_logging:\n",
    "        # we have to offset all data by the absolute value of it if its negative\n",
    "        info = data[col].describe()\n",
    "        if info['min'] < 0:\n",
    "            data[col] = data[col] + np.abs(info['min']) + 1\n",
    "\n",
    "        data[col] = np.log(data[col])\n",
    "    \n",
    "    return data\n",
    "\n",
    "all_data = log_data(all_data)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length: 136448\n",
      "New length: 108152\n"
     ]
    }
   ],
   "source": [
    "# We need to attempt to handle outliers in our data.\n",
    "# this removes outliers based on the interquartile range\n",
    "def remove_outliers_iqr(data, iqr_mod=1.5, feature_size=9):\n",
    "    cols_for_trimming = data.columns[:feature_size]\n",
    "\n",
    "    # we need to go through each feature and check the inter quartile ranges.\n",
    "    # we'll drop values with info outside of the multiplier on the range we\n",
    "    # provided\n",
    "    print('Old length: {:d}'.format(len(data)))\n",
    "    for col in cols_for_trimming:\n",
    "        info = data[col].describe()\n",
    "        range_add = (info['75%'] - info['25%']) * iqr_mod\n",
    "        data = data[(data[col] >= info['25%'] - range_add)]\n",
    "        data = data[(data[col] <= info['75%'] + range_add)]\n",
    "    print('New length: {:d}'.format(len(data)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# it might be useful to try making one for the z-score, but I don't know\n",
    "# if our distribution is normalized.\n",
    "\n",
    "all_data = remove_outliers_iqr(all_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.790051250741316\n",
      "14.711201294824106\n",
      "18.518706486141376\n",
      "11.568015320151485\n",
      "8.645939524629462\n",
      "11.667243503526587\n",
      "3.6137942376991385\n",
      "9.288617587790053\n",
      "5.374716997680058\n"
     ]
    }
   ],
   "source": [
    "# there are a couple of preliminary steps we need to take. \n",
    "\n",
    "# 1) handle outliers\n",
    "# 2) normalize all of the data (0 to 1)\n",
    "# 3) separate testing and training sets\n",
    "\n",
    "\n",
    "def normalize_data(data, feature_size=9):\n",
    "    cols_for_normalization = data.columns[:feature_size]\n",
    "    \n",
    "    for col in cols_for_normalization:\n",
    "        max = data[col].max()\n",
    "        min = data[col].min()\n",
    "        print(max)\n",
    "\n",
    "        data[col] = (data[col] - min) / (max - min)\n",
    "\n",
    "normalize_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate training and test sets\n",
    "\n",
    "def separate_sets(data, prop_test, prop_train=0, feature_size=9):\n",
    "\n",
    "    # we need to drop superfluous info like tickers\n",
    "    data = data.drop(['ticker'], axis=1)\n",
    "\n",
    "    if prop_train == 0 or prop_train + prop_test > 1:\n",
    "        prop_train = 1 - prop_test\n",
    "    \n",
    "    # we find out the ratio to take from the remaining portion\n",
    "    prop_train = (1 - prop_test) / prop_train\n",
    "    if prop_train > 1:\n",
    "        prop_train = 1\n",
    "\n",
    "    test_set = data.sample(frac=prop_test)\n",
    "    data = data.drop(test_set.index)\n",
    "    train_set = data.sample(frac=prop_train)\n",
    "\n",
    "    test_f = np.array(test_set[test_set.columns[:feature_size]])\n",
    "    test_l = np.concatenate(np.array(test_set[test_set.columns[feature_size:]]))\n",
    "\n",
    "    train_f = np.array(train_set[train_set.columns[:feature_size]])\n",
    "    train_l = np.concatenate(np.array(train_set[train_set.columns[feature_size:]]))\n",
    "\n",
    "    return (test_f, test_l, train_f, train_l)\n",
    "\n",
    "test_features, test_labels, train_features, train_labels = separate_sets(all_data, .2)\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "Now that we have theoretcially written everything needed to properly prep the data, we need to choose a model to implement. I think for now I will attempt to start off with basic logistic regression and increase complexity as needed.\n",
    "\n",
    "This means we will need a: \n",
    "* predictor\n",
    "* loss function\n",
    "* derivative of loss function\n",
    "* gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05838603145816248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_prediction(weights, features):\n",
    "    return 1 / (1 + np.exp(-np.dot(weights[1:], features) - weights[0]))\n",
    "\n",
    "make_prediction([-.5] * 10, train_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Functions\n",
    "\n",
    "I didn't actually remember the math behind this so I looked up a good logistic regression algorithm loss and gradient of the loss function. That way I can guarantee, or at least better guarantee, that if something is wrong it's not this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4028250947610938"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think a simple 1 or 0 single loss function might be the best for now\n",
    "# we can come back and change it later if need be\n",
    "\n",
    "# I got this from a youtube video. Math is hard. \n",
    "def single_loss_log(weights, features, label):\n",
    "    y_hat = make_prediction(weights, features)\n",
    "    return label * np.log(y_hat) + (1 - label) * np.log(1 - y_hat)\n",
    "\n",
    "def batch_loss(batch_start_ind, batch_size, loss_func, weights, train_f, train_l):\n",
    "    total_loss = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        total_loss += loss_func(weights, train_f[(point_ind + batch_start_ind) % len(train_f)], train_l[(point_ind + batch_start_ind) % len(train_l)])\n",
    "    return total_loss / batch_size\n",
    "\n",
    "single_loss_log([-.5] * 10, train_features[0], train_labels[0])\n",
    "batch_loss(0, len(train_labels), single_loss_log, [-.5] * 10, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4220811 , -0.21797118, -0.21489901, -0.21230596, -0.2152478 ,\n",
       "       -0.21921876, -0.21574955, -0.20650487, -0.21491941, -0.21695384])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gradient(batch_start_ind, batch_size, weights, train_f, train_l):\n",
    "    total_diff_theta = np.array([0.0] * (len(weights) - 1))\n",
    "    total_diff_b = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        y_diff = make_prediction(weights, train_f[(point_ind + batch_start_ind) % len(train_f)]) - train_l[(point_ind + batch_start_ind) % len(train_l)]\n",
    "        total_diff_theta +=  y_diff * train_f[(point_ind + batch_start_ind) % len(train_f)]\n",
    "        total_diff_b += y_diff\n",
    "    total_diff_theta = np.insert(total_diff_theta, 0, total_diff_b)\n",
    "\n",
    "    total_diff_theta /= batch_size\n",
    "\n",
    "    return total_diff_theta\n",
    "\n",
    "grad = batch_gradient(0, len(train_labels), [-.5] * 10, train_features, train_labels)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08236496  0.04829173  0.02696741 -0.02340224 -0.01702758  0.08225315\n",
      " -0.02956382 -0.07715079 -0.08913265  0.10682792]\n"
     ]
    }
   ],
   "source": [
    "def batch_descent(train_l, train_f, batch_size, step_size, threshold, weights=np.array([0] * 10)):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.8f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights\n",
    "\n",
    "trained_model = batch_descent(train_labels, train_features, len(train_labels), 2, .00002723)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47970411465557095"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_error(test_f, test_l, weights):\n",
    "    total = 0\n",
    "    test_total = 0\n",
    "    for point_ind in range(len(test_l)):\n",
    "        expectation = make_prediction(weights, test_f[point_ind])\n",
    "        \n",
    "        test_total += test_l[point_ind]\n",
    "\n",
    "        if (expectation - .5) * ( test_l[point_ind] - .5) < 0:\n",
    "            total += 1\n",
    "    \n",
    "    return total / len(test_l)\n",
    "\n",
    "calc_error(test_features, test_labels, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.92536226  0.44003903  0.22536483  0.02768027 -0.08584893  0.99527823\n",
      "  0.02950703 -0.27913576 -0.49569378  0.81888162]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.4820157189089228,\n",
       " 0.48206195099398985,\n",
       " 0.4820157189089228,\n",
       " 0.48183079056865463,\n",
       " 0.48169209431345356,\n",
       " 0.4816458622283865,\n",
       " 0.4814147018030513,\n",
       " 0.4805362921867776,\n",
       " 0.4802126675913084,\n",
       " 0.47970411465557095,\n",
       " 0.47840961627369394,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.4778548312528895,\n",
       " 0.4778085991678225,\n",
       " 0.47739251040221914,\n",
       " 0.477161349976884,\n",
       " 0.4770688858067499,\n",
       " 0.47693018955154876,\n",
       " 0.4766065649560795,\n",
       " 0.47637540453074434,\n",
       " 0.4761442441054092,\n",
       " 0.47535829865926954,\n",
       " 0.47452612112806286,\n",
       " 0.47350901525658806,\n",
       " 0.4728155339805825,\n",
       " 0.4728155339805825,\n",
       " 0.4720295885344429,\n",
       " 0.47133610725843733,\n",
       " 0.47031900138696253,\n",
       " 0.4698566805362922,\n",
       " 0.46916319926028666,\n",
       " 0.46842348589921406,\n",
       " 0.4672214516874711,\n",
       " 0.4668515950069348,\n",
       " 0.46638927415626447,\n",
       " 0.4656495607951919,\n",
       " 0.46560332871012483,\n",
       " 0.4649098474341193,\n",
       " 0.46412390198797965,\n",
       " 0.46310679611650485,\n",
       " 0.4631530282015719,\n",
       " 0.4629680998613038,\n",
       " 0.46208969024503005,\n",
       " 0.46236708275543226,\n",
       " 0.46245954692556634,\n",
       " 0.46218215441516414,\n",
       " 0.46199722607489596,\n",
       " 0.46199722607489596,\n",
       " 0.4615811373092926,\n",
       " 0.46116504854368934,\n",
       " 0.4613962089690245,\n",
       " 0.46107258437355525,\n",
       " 0.46042533518261675,\n",
       " 0.4600092464170134,\n",
       " 0.4598243180767453,\n",
       " 0.45959315765141007,\n",
       " 0.459454461396209,\n",
       " 0.459500693481276,\n",
       " 0.4596393897364771,\n",
       " 0.4597318539066112,\n",
       " 0.4598243180767453,\n",
       " 0.45903837263060565,\n",
       " 0.4588996763754045,\n",
       " 0.4588534442903375,\n",
       " 0.45880721220527043,\n",
       " 0.45894590846047156,\n",
       " 0.4589921405455386,\n",
       " 0.45866851595006936,\n",
       " 0.45894590846047156,\n",
       " 0.45866851595006936,\n",
       " 0.458252427184466,\n",
       " 0.458252427184466,\n",
       " 0.45779010633379563,\n",
       " 0.45802126675913085,\n",
       " 0.4578825705039297,\n",
       " 0.4572353213129912,\n",
       " 0.45728155339805826,\n",
       " 0.4572353213129912,\n",
       " 0.45737401756819235,\n",
       " 0.4575127138233934,\n",
       " 0.4575127138233934,\n",
       " 0.45737401756819235,\n",
       " 0.4573277854831253,\n",
       " 0.45728155339805826,\n",
       " 0.4571890892279242,\n",
       " 0.45742024965325934,\n",
       " 0.4573277854831253,\n",
       " 0.4568192325473879,\n",
       " 0.4566805362921868,\n",
       " 0.4566805362921868,\n",
       " 0.4564956079519186,\n",
       " 0.45635691169671755,\n",
       " 0.4564956079519186,\n",
       " 0.45612575127138233,\n",
       " 0.4558021266759131,\n",
       " 0.45589459084604717,\n",
       " 0.45552473416551087,\n",
       " 0.45524734165510866,\n",
       " 0.4552011095700416,\n",
       " 0.4551548774849746,\n",
       " 0.4552011095700416,\n",
       " 0.45510864539990753,\n",
       " 0.4554322699953768,\n",
       " 0.45538603791030974,\n",
       " 0.4554322699953768,\n",
       " 0.4558483587609801,\n",
       " 0.4560795191863153,\n",
       " 0.4562182154415164,\n",
       " 0.4558483587609801,\n",
       " 0.45561719833564496,\n",
       " 0.4555709662505779,\n",
       " 0.4552935737401757,\n",
       " 0.45501618122977344,\n",
       " 0.4550624133148405,\n",
       " 0.45510864539990753,\n",
       " 0.45501618122977344,\n",
       " 0.4552935737401757,\n",
       " 0.4552011095700416,\n",
       " 0.4552935737401757,\n",
       " 0.4554322699953768,\n",
       " 0.4554785020804438,\n",
       " 0.455663430420712,\n",
       " 0.455663430420712,\n",
       " 0.4558483587609801,\n",
       " 0.45603328710124824,\n",
       " 0.45598705501618125,\n",
       " 0.4560795191863153,\n",
       " 0.4558021266759131,\n",
       " 0.45575589459084603,\n",
       " 0.4559408229311142,\n",
       " 0.45589459084604717,\n",
       " 0.45603328710124824,\n",
       " 0.45589459084604717,\n",
       " 0.45575589459084603,\n",
       " 0.45589459084604717,\n",
       " 0.4558483587609801,\n",
       " 0.4559408229311142,\n",
       " 0.45598705501618125,\n",
       " 0.4561719833564494,\n",
       " 0.4563106796116505,\n",
       " 0.4562182154415164,\n",
       " 0.45598705501618125,\n",
       " 0.45603328710124824,\n",
       " 0.4560795191863153,\n",
       " 0.45603328710124824,\n",
       " 0.4560795191863153,\n",
       " 0.45598705501618125,\n",
       " 0.45598705501618125,\n",
       " 0.45598705501618125,\n",
       " 0.45589459084604717,\n",
       " 0.4560795191863153,\n",
       " 0.45589459084604717,\n",
       " 0.45552473416551087,\n",
       " 0.4555709662505779,\n",
       " 0.45552473416551087,\n",
       " 0.4554322699953768,\n",
       " 0.45561719833564496,\n",
       " 0.455709662505779,\n",
       " 0.45561719833564496,\n",
       " 0.4558021266759131,\n",
       " 0.45598705501618125,\n",
       " 0.45598705501618125,\n",
       " 0.45612575127138233,\n",
       " 0.45598705501618125,\n",
       " 0.45598705501618125,\n",
       " 0.45589459084604717,\n",
       " 0.45598705501618125,\n",
       " 0.4558483587609801,\n",
       " 0.4558021266759131,\n",
       " 0.4558021266759131,\n",
       " 0.45561719833564496,\n",
       " 0.455663430420712,\n",
       " 0.45552473416551087,\n",
       " 0.45552473416551087,\n",
       " 0.4554322699953768,\n",
       " 0.4554785020804438,\n",
       " 0.4555709662505779,\n",
       " 0.45552473416551087,\n",
       " 0.45561719833564496,\n",
       " 0.4555709662505779,\n",
       " 0.4554322699953768,\n",
       " 0.4555709662505779,\n",
       " 0.4555709662505779,\n",
       " 0.45552473416551087,\n",
       " 0.45561719833564496,\n",
       " 0.45552473416551087,\n",
       " 0.45552473416551087,\n",
       " 0.4554322699953768,\n",
       " 0.45538603791030974,\n",
       " 0.4553398058252427,\n",
       " 0.4554785020804438,\n",
       " 0.4552935737401757,\n",
       " 0.4551548774849746,\n",
       " 0.4551548774849746,\n",
       " 0.4550624133148405,\n",
       " 0.45510864539990753,\n",
       " 0.4551548774849746,\n",
       " 0.45501618122977344,\n",
       " 0.45496994914470645,\n",
       " 0.45496994914470645,\n",
       " 0.45487748497457237,\n",
       " 0.4548312528895053,\n",
       " 0.45464632454923715,\n",
       " 0.45473878871937123,\n",
       " 0.45460009246417016,\n",
       " 0.454461396208969,\n",
       " 0.454415164123902,\n",
       " 0.454461396208969,\n",
       " 0.4542764678687009,\n",
       " 0.4539528432732316,\n",
       " 0.4539066111881646,\n",
       " 0.4537679149329635,\n",
       " 0.45386037910309757,\n",
       " 0.45386037910309757,\n",
       " 0.45386037910309757,\n",
       " 0.4538141470180305,\n",
       " 0.4536754507628294,\n",
       " 0.45349052242256127,\n",
       " 0.4535829865926953,\n",
       " 0.4535367545076283,\n",
       " 0.4537679149329635,\n",
       " 0.4536754507628294,\n",
       " 0.45372168284789643,\n",
       " 0.4536754507628294,\n",
       " 0.45362921867776235,\n",
       " 0.4535829865926953,\n",
       " 0.45362921867776235,\n",
       " 0.4538141470180305,\n",
       " 0.45386037910309757,\n",
       " 0.4536754507628294,\n",
       " 0.45362921867776235,\n",
       " 0.4536754507628294,\n",
       " 0.4536754507628294,\n",
       " 0.4535829865926953,\n",
       " 0.45349052242256127,\n",
       " 0.45335182616736014,\n",
       " 0.4533055940822931,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.45325936199722605,\n",
       " 0.453166897827092,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.453120665742025,\n",
       " 0.45307443365695793,\n",
       " 0.453166897827092,\n",
       " 0.453120665742025,\n",
       " 0.45307443365695793,\n",
       " 0.45298196948682384,\n",
       " 0.45288950531668976,\n",
       " 0.45284327323162277,\n",
       " 0.45288950531668976,\n",
       " 0.4527970411465557,\n",
       " 0.4527970411465557,\n",
       " 0.45284327323162277,\n",
       " 0.45284327323162277,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.45298196948682384,\n",
       " 0.45288950531668976,\n",
       " 0.45288950531668976,\n",
       " 0.4529357374017568,\n",
       " 0.45288950531668976,\n",
       " 0.45284327323162277,\n",
       " 0.45288950531668976,\n",
       " 0.45284327323162277,\n",
       " 0.45270457697642164,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " 0.4527970411465557,\n",
       " 0.45288950531668976,\n",
       " 0.45284327323162277,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.4530282015718909,\n",
       " 0.45298196948682384,\n",
       " 0.45288950531668976,\n",
       " 0.4527970411465557,\n",
       " 0.45284327323162277,\n",
       " 0.4527970411465557,\n",
       " 0.4527508090614887,\n",
       " 0.45284327323162277,\n",
       " 0.4527970411465557,\n",
       " 0.45284327323162277,\n",
       " 0.4527970411465557,\n",
       " 0.4527508090614887,\n",
       " 0.45270457697642164,\n",
       " 0.4526583448913546,\n",
       " 0.45261211280628755,\n",
       " 0.4526583448913546,\n",
       " 0.45261211280628755,\n",
       " 0.4526583448913546,\n",
       " 0.45270457697642164,\n",
       " 0.45261211280628755,\n",
       " 0.45261211280628755,\n",
       " 0.45261211280628755,\n",
       " 0.45247341655108647,\n",
       " 0.4525658807212205,\n",
       " 0.45261211280628755,\n",
       " 0.45261211280628755,\n",
       " 0.4525658807212205,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4523809523809524,\n",
       " 0.4524271844660194,\n",
       " 0.4524271844660194,\n",
       " 0.4523809523809524,\n",
       " 0.4522884882108183,\n",
       " 0.45224225612575125,\n",
       " 0.45233472029588534,\n",
       " 0.45233472029588534,\n",
       " 0.4524271844660194,\n",
       " 0.45233472029588534,\n",
       " 0.45233472029588534,\n",
       " 0.4523809523809524,\n",
       " 0.45233472029588534,\n",
       " 0.4522884882108183,\n",
       " 0.4522884882108183,\n",
       " 0.4522884882108183,\n",
       " 0.4522884882108183,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.45233472029588534,\n",
       " 0.4524271844660194,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.45270457697642164,\n",
       " 0.4527508090614887,\n",
       " 0.45270457697642164,\n",
       " 0.45261211280628755,\n",
       " 0.4525658807212205,\n",
       " 0.4526583448913546,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.4525658807212205,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.4524271844660194,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.45233472029588534,\n",
       " 0.45233472029588534,\n",
       " 0.45233472029588534,\n",
       " 0.4524271844660194,\n",
       " 0.4525658807212205,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.4526583448913546,\n",
       " 0.4525658807212205,\n",
       " 0.4525658807212205,\n",
       " 0.4525658807212205,\n",
       " 0.4525196486361535,\n",
       " 0.4525658807212205,\n",
       " 0.4525658807212205,\n",
       " 0.4525658807212205,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.45261211280628755,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.4523809523809524,\n",
       " 0.45233472029588534,\n",
       " 0.45224225612575125,\n",
       " 0.4522884882108183,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.4524271844660194,\n",
       " 0.45247341655108647,\n",
       " 0.4525196486361535,\n",
       " 0.45261211280628755,\n",
       " 0.45261211280628755,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.45270457697642164,\n",
       " 0.4527508090614887,\n",
       " 0.4526583448913546,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4523809523809524,\n",
       " 0.45233472029588534,\n",
       " 0.4523809523809524,\n",
       " 0.45224225612575125,\n",
       " 0.4521960240406842,\n",
       " 0.4521497919556172,\n",
       " 0.45224225612575125,\n",
       " 0.4521497919556172,\n",
       " 0.4521035598705502,\n",
       " 0.4521960240406842,\n",
       " 0.4522884882108183,\n",
       " 0.45233472029588534,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.4522884882108183,\n",
       " 0.4523809523809524,\n",
       " 0.45233472029588534,\n",
       " 0.45233472029588534,\n",
       " 0.4522884882108183,\n",
       " 0.4522884882108183,\n",
       " 0.4522884882108183,\n",
       " 0.4522884882108183,\n",
       " 0.45233472029588534,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.45247341655108647,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4524271844660194,\n",
       " 0.4524271844660194,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4524271844660194,\n",
       " 0.4522884882108183,\n",
       " 0.4523809523809524,\n",
       " 0.4524271844660194,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4523809523809524,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.4525658807212205,\n",
       " 0.4525658807212205,\n",
       " 0.45270457697642164,\n",
       " 0.4527970411465557,\n",
       " 0.45298196948682384,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.4529357374017568,\n",
       " 0.45288950531668976,\n",
       " 0.4527970411465557,\n",
       " 0.4527508090614887,\n",
       " 0.45270457697642164,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.45270457697642164,\n",
       " 0.4525658807212205,\n",
       " 0.4525196486361535,\n",
       " 0.4525658807212205,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4524271844660194,\n",
       " 0.4525196486361535,\n",
       " 0.45261211280628755,\n",
       " 0.4525658807212205,\n",
       " 0.45247341655108647,\n",
       " 0.4525658807212205,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4523809523809524,\n",
       " 0.4522884882108183,\n",
       " 0.45224225612575125,\n",
       " 0.45224225612575125,\n",
       " 0.45224225612575125,\n",
       " 0.45224225612575125,\n",
       " 0.4522884882108183,\n",
       " 0.45224225612575125,\n",
       " 0.4522884882108183,\n",
       " 0.45224225612575125,\n",
       " 0.4522884882108183,\n",
       " 0.45224225612575125,\n",
       " 0.4522884882108183,\n",
       " 0.4521960240406842,\n",
       " 0.4521497919556172,\n",
       " 0.4521497919556172,\n",
       " 0.4521497919556172,\n",
       " 0.4521035598705502,\n",
       " 0.4521035598705502,\n",
       " 0.4521035598705502,\n",
       " 0.45224225612575125,\n",
       " 0.4522884882108183,\n",
       " 0.4524271844660194,\n",
       " 0.4525658807212205,\n",
       " 0.45261211280628755,\n",
       " 0.45261211280628755,\n",
       " 0.4525658807212205,\n",
       " 0.4525658807212205,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4524271844660194,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.4525658807212205,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.4524271844660194,\n",
       " 0.4525196486361535,\n",
       " 0.4525658807212205,\n",
       " 0.4525658807212205,\n",
       " 0.4525196486361535,\n",
       " 0.4525658807212205,\n",
       " 0.45261211280628755,\n",
       " 0.4525196486361535,\n",
       " 0.4525658807212205,\n",
       " 0.45261211280628755,\n",
       " 0.4526583448913546,\n",
       " 0.45261211280628755,\n",
       " 0.45261211280628755,\n",
       " 0.45261211280628755,\n",
       " 0.45270457697642164,\n",
       " 0.4527970411465557,\n",
       " 0.4527970411465557,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.45288950531668976,\n",
       " 0.45284327323162277,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " 0.4526583448913546,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.4527508090614887,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.4526583448913546,\n",
       " 0.45270457697642164,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.4525658807212205,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " 0.45284327323162277,\n",
       " 0.45288950531668976,\n",
       " 0.45284327323162277,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.45288950531668976,\n",
       " 0.4529357374017568,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to be able to make a graph of the error over epochs\n",
    "# so I am going to do that here\n",
    "def batch_descent_error_tracking(train_l, train_f, test_f, test_l, batch_size, step_size, threshold, weights=np.array([0] * 10)):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "    test_errors = list()\n",
    "    train_errors = list()\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "            test_errors.append(calc_error(test_f, test_l, weights))\n",
    "            train_errors.append(calc_error(train_f, train_l, weights))\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.8f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights, test_errors, train_errors\n",
    "\n",
    "trained_model, test_errors, train_errors = batch_descent_error_tracking(train_labels, train_features, test_features, test_labels, len(train_labels), 2, .0005)\n",
    "test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we need to make a graph of this\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(test_errors)), test_errors)\n",
    "plt.plot(range(len(train_errors)), train_errors, color='orange')\n",
    "plt.xlabel('Epochs of training (whole data set)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Test and Training error over time')\n",
    "plt.savefig('model1perf.jpg')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Analysis\n",
    "\n",
    "I am going to build a 9-10-10-1 fully connected network with a basic sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's make the activation function for simplicity sake\n",
    "def activate(x):\n",
    "    return 1 / (1 + np.exp(-1 * x)) \n",
    "\n",
    "def active_grad(x):\n",
    "    return np.exp(-1 * x) / (1 + np.exp(-1 * x)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49420331 0.58369711 0.55093786 ... 0.46858582 0.4899978  0.47202463]\n",
      " [0.46292652 0.37304276 0.62724563 ... 0.47236457 0.5239749  0.45200056]\n",
      " [0.45098687 0.41837307 0.60661045 ... 0.40238183 0.46157213 0.42498836]\n",
      " ...\n",
      " [0.47999593 0.39616514 0.48014235 ... 0.3357062  0.44270399 0.51367435]\n",
      " [0.51059909 0.4223003  0.43497771 ... 0.61016178 0.52216708 0.64601989]\n",
      " [0.45259873 0.44015791 0.50073155 ... 0.45841361 0.466105   0.46836553]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# we need to prepare our data to be run through the network in the correct shape\n",
    "nt_labels = train_labels.reshape(len(train_labels), 1)\n",
    "nt_features = train_features\n",
    "print(nt_features)\n",
    "print(nt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll also select a starting seed\n",
    "# np.random.seed(2022)\n",
    "\n",
    "# now we should initialize our neurons in our layers\n",
    "layer1 = 2 * np.random.random((9, 10)) - 1 \n",
    "layer2 = 2 * np.random.random((10, 10)) - 1\n",
    "layer3 = 2 * np.random.random((10, 1)) - 1\n",
    "\n",
    "# I will write a function that trains for a set number of epochs\n",
    "def train_epochs(num_epochs, alpha, layer1, layer2, layer3, features, labels, printfreq=10):\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        l0 = features\n",
    "        l1 = activate(np.dot(l0, layer1))\n",
    "        l2 = activate(np.dot(l1, layer2))\n",
    "        l3 = activate(np.dot(l2, layer3))\n",
    "\n",
    "        if e % printfreq == 0:\n",
    "            l3_error = labels - l3 \n",
    "\n",
    "            # we should print our current training error every once in awhile\n",
    "            print(\"Epoch {:d} Error:{:2.4f}%\".format(e, np.mean(np.abs(l3_error)) * 100))\n",
    "\n",
    "        # let's go through and backpropagate\n",
    "\n",
    "        l3_grad = l3_error * active_grad(l3)\n",
    "        \n",
    "        l2_error = l3_grad.dot(layer3.T)\n",
    "\n",
    "        l2_grad = l2_error * active_grad(l2)\n",
    "\n",
    "        l1_error = l2_grad.dot(layer2.T)\n",
    "\n",
    "        l1_grad = l1_error * active_grad(l1)\n",
    "\n",
    "        # now we want to update our weights\n",
    "        layer3 = layer3 + alpha * l2.T.dot(l3_grad)\n",
    "        layer2 = layer2 + alpha * l1.T.dot(l2_grad)\n",
    "        layer1 = layer1 + alpha * l0.T.dot(l1_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Error:50.5623%\n",
      "Epoch 10 Error:48.2421%\n",
      "Epoch 20 Error:51.7579%\n",
      "Epoch 30 Error:48.2423%\n",
      "Epoch 40 Error:51.7577%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/629854626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/3374923772.py\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(num_epochs, alpha, layer1, layer2, layer3, features, labels, printfreq)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# let's go through and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0ml3_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactive_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0ml2_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/902923763.py\u001b[0m in \u001b[0;36mactive_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mactive_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_epochs(8000, .0001, layer1, layer2, layer3, nt_features, nt_labels, printfreq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dwarf\\Documents\\School\\2022\\Fall\\CS4824\\semester-proj\\fundamental-analysis\\model.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#Y235sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ntest_features \u001b[39m=\u001b[39m test_features\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#Y235sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m l0 \u001b[39m=\u001b[39m ntest_features\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#Y235sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m l1 \u001b[39m=\u001b[39m activate(np\u001b[39m.\u001b[39mdot(l0, layer1))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#Y235sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m l2 \u001b[39m=\u001b[39m activate(np\u001b[39m.\u001b[39mdot(l1, layer2))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#Y235sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m l3 \u001b[39m=\u001b[39m activate(np\u001b[39m.\u001b[39mdot(l2, layer3))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'activate' is not defined"
     ]
    }
   ],
   "source": [
    "# let's check our test error\n",
    "ntest_labels = test_labels.reshape(len(test_labels), 1)\n",
    "ntest_features = test_features\n",
    "\n",
    "l0 = ntest_features\n",
    "l1 = activate(np.dot(l0, layer1))\n",
    "l2 = activate(np.dot(l1, layer2))\n",
    "l3 = activate(np.dot(l2, layer3))\n",
    "\n",
    "errors = ntest_labels - l3\n",
    "total_error = np.mean(np.abs(errors))\n",
    "print('Test Error: {:2.4f}%'.format(total_error * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make our pytorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "NeuralNetwork(\n",
      "  (layer_1): Linear(in_features=9, out_features=10, bias=True)\n",
      "  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# let's build our model\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input, h1, h2, out):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(input, h1)\n",
    "        nn.init.uniform_(self.layer_1.weight)\n",
    "        self.layer_2 = nn.Linear(h1, h2)\n",
    "        nn.init.uniform_(self.layer_2.weight)\n",
    "        self.layer_3 = nn.Linear(h2, out)\n",
    "        nn.init.uniform_(self.layer_3.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = torch.sigmoid(self.layer_1(x))\n",
    "        y = torch.sigmoid(self.layer_2(y))\n",
    "        y = torch.sigmoid(self.layer_3(y))\n",
    "\n",
    "        return y\n",
    "        \n",
    "\n",
    "model = NeuralNetwork(9, 10, 10, 1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set our parameters like loss and learning rate\n",
    "learn_rate = .1\n",
    "batch_size = 32\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to make a special data object to pass\n",
    "# our data into our network\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.X = torch.from_numpy(x.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "train_data = Data(train_features, train_labels)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "\n",
    "test_data = Data(test_features, test_labels)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size)\n",
    "\n",
    "# for batch, (X, y) in enumerate(train_dataloader):\n",
    "#     print(X.shape)\n",
    "#     print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate the number of misclassifications in a set\n",
    "def calc_frac_wrong(cur_model, dataloader):\n",
    "    wrong = 0\n",
    "    total = 0\n",
    "    \n",
    "\n",
    "    for X, y in dataloader:\n",
    "        pred = cur_model(X)\n",
    "        predicted = np.where(pred < 0.5, 0, 1)\n",
    "        predicted = list(itertools.chain(*predicted))\n",
    "        wrong += (predicted != y.numpy()).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    return wrong / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 training error: 0.53843432664871215820\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dwarf\\Documents\\School\\2022\\Fall\\CS4824\\semester-proj\\fundamental-analysis\\model.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dwarf/Documents/School/2022/Fall/CS4824/semester-proj/fundamental-analysis/model.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49ml1_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:3261\u001b[0m, in \u001b[0;36ml1_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3258\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m   3260\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[1;32m-> 3261\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49ml1_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's train!\n",
    "\n",
    "num_epochs = 25\n",
    "losses = list()\n",
    "test_loss = list()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(-1))\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    test_loss.append(calc_frac_wrong(model, test_dataloader))\n",
    "\n",
    "    print('Epoch {:d} training error: {:.20f}'.format(epoch + 1, loss), end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 48.233934350439206185\n"
     ]
    }
   ],
   "source": [
    "incorrect = 0\n",
    "total = 0\n",
    "# Now let's evaluate our performance\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)\n",
    "        predicted = np.where(outputs < 0.5, 0, 1)\n",
    "        predicted = list(itertools.chain(*predicted))\n",
    "        total += y.size(0)\n",
    "        incorrect += (predicted != y.numpy()).sum().item()\n",
    "\n",
    "print('Final Error: {:2.18f}'.format(100 * incorrect / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Status\n",
    "\n",
    "## Logistic Regression\n",
    "I feel as though the Logistic Regression Model is likely being held back by \n",
    "1. Weaknesses in the data set\n",
    "2. Uncertainty\n",
    "\n",
    "The best bang for my buck improvements are likely\n",
    "\n",
    "1. Implementing abstension\n",
    "2. Build a new data set (time and money sink)\n",
    "3. Try out SVM\n",
    "\n",
    "## ANN\n",
    "The artificial neural network model might just be too deep or complicated. The current state of it is leading to overfitting. I'm getting training errors of like 38% and test errors of like 48% which is no bueno.\n",
    "\n",
    "1. Simplify model\n",
    "2. Implement Abstension\n",
    "3. Build new data set\n",
    "\n",
    "# Next steps\n",
    "I need some deliverables so I plan on making sure I have well documented evidence of my current results before moving on. I think I'll do things in this order.\n",
    "\n",
    "1. Build training error vs. testing error graph for both existing models.\n",
    "2. Add abstension to both models, make new graphs.\n",
    "3. Simplyify ANN\n",
    "4. Look into building new data set\n",
    "5. Make SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
