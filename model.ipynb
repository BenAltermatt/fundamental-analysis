{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "I have successfully been able to build a sort of data set for this model. Now I want to try training on my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cupy-cuda11x (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cupy-cuda11x\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install cupy-cuda11x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037304</td>\n",
       "      <td>0.081930</td>\n",
       "      <td>68.692936</td>\n",
       "      <td>16.198334</td>\n",
       "      <td>3.792215</td>\n",
       "      <td>1.051065</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.366850</td>\n",
       "      <td>0.337912</td>\n",
       "      <td>8.135137</td>\n",
       "      <td>18.134324</td>\n",
       "      <td>3.515821</td>\n",
       "      <td>0.918390</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.108044</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.132520</td>\n",
       "      <td>0.173255</td>\n",
       "      <td>18.516105</td>\n",
       "      <td>6.586352</td>\n",
       "      <td>1.420348</td>\n",
       "      <td>0.761143</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.236057</td>\n",
       "      <td>0.019177</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071330</td>\n",
       "      <td>0.172091</td>\n",
       "      <td>29.580969</td>\n",
       "      <td>5.532793</td>\n",
       "      <td>1.205178</td>\n",
       "      <td>0.746057</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.436893</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.133795</td>\n",
       "      <td>0.211265</td>\n",
       "      <td>12.743329</td>\n",
       "      <td>4.438462</td>\n",
       "      <td>0.957572</td>\n",
       "      <td>0.715781</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.018786</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.265324</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>3.700391</td>\n",
       "      <td>0.383926</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.948329</td>\n",
       "      <td>4.281250</td>\n",
       "      <td>3.332726</td>\n",
       "      <td>0.322385</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>4.395096</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>0.792549</td>\n",
       "      <td>1.878183</td>\n",
       "      <td>2.160751</td>\n",
       "      <td>2.286619</td>\n",
       "      <td>0.339581</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.039290</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.440450</td>\n",
       "      <td>1.614583</td>\n",
       "      <td>2.256612</td>\n",
       "      <td>0.362996</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.056206</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.242760</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.904923</td>\n",
       "      <td>0.341091</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.257109</td>\n",
       "      <td>2.313978</td>\n",
       "      <td>0.037899</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                0.037304              0.081930  68.692936          16.198334   \n",
       "1                0.366850              0.337912   8.135137          18.134324   \n",
       "2                0.132520              0.173255  18.516105           6.586352   \n",
       "3                0.071330              0.172091  29.580969           5.532793   \n",
       "4                0.133795              0.211265  12.743329           4.438462   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           0.600000              1.265324   3.083333           3.700391   \n",
       "136444           0.400000              0.948329   4.281250           3.332726   \n",
       "136445           0.792549              1.878183   2.160751           2.286619   \n",
       "136446           1.200000              1.440450   1.614583           2.256612   \n",
       "136447           0.800000              1.242760   2.250000           2.904923   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       3.792215      1.051065       0.000404     0.111111  0.013801   CMCSA   \n",
       "1       3.515821      0.918390       0.000015     0.000485  0.108044   CMCSA   \n",
       "2       1.420348      0.761143       0.003187     0.236057  0.019177   CMCSA   \n",
       "3       1.205178      0.746057       0.003692     0.436893  0.010185   CMCSA   \n",
       "4       0.957572      0.715781       0.004574     0.233161  0.018786   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  0.383926      0.001104       0.000011     0.000135  0.031129  SNGR.L   \n",
       "136444  0.322385      0.001067       0.256648     4.395096  0.018825  SNGR.L   \n",
       "136445  0.339581      0.001007       0.000037     0.000320  0.039290  SNGR.L   \n",
       "136446  0.362996      0.000928       0.000067     0.000433  0.056206  SNGR.L   \n",
       "136447  0.341091      0.000955       0.257109     2.313978  0.037899  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# first we need to read the data in\n",
    "\n",
    "data_name = 'dataset.csv'\n",
    "\n",
    "all_data = pd.read_csv(data_name)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.789873</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518706</td>\n",
       "      <td>11.567448</td>\n",
       "      <td>8.644031</td>\n",
       "      <td>11.667213</td>\n",
       "      <td>-7.813165</td>\n",
       "      <td>9.288283</td>\n",
       "      <td>5.373945</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.789891</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567466</td>\n",
       "      <td>8.643982</td>\n",
       "      <td>11.667212</td>\n",
       "      <td>-11.112948</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374382</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567357</td>\n",
       "      <td>8.643613</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.748617</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373970</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.789875</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567347</td>\n",
       "      <td>8.643575</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.601492</td>\n",
       "      <td>9.288314</td>\n",
       "      <td>5.373929</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567337</td>\n",
       "      <td>8.643531</td>\n",
       "      <td>11.667210</td>\n",
       "      <td>-5.387330</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>9.789904</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567330</td>\n",
       "      <td>8.643430</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-11.419512</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374026</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>9.789893</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567326</td>\n",
       "      <td>8.643419</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.360050</td>\n",
       "      <td>9.288680</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-10.202989</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374064</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>9.789938</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643426</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-9.609178</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374142</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567322</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.358257</td>\n",
       "      <td>9.288487</td>\n",
       "      <td>5.374057</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                9.789873               14.7112  18.518706          11.567448   \n",
       "1                9.789891               14.7112  18.518705          11.567466   \n",
       "2                9.789878               14.7112  18.518705          11.567357   \n",
       "3                9.789875               14.7112  18.518705          11.567347   \n",
       "4                9.789878               14.7112  18.518705          11.567337   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           9.789904               14.7112  18.518705          11.567330   \n",
       "136444           9.789893               14.7112  18.518705          11.567326   \n",
       "136445           9.789915               14.7112  18.518705          11.567316   \n",
       "136446           9.789938               14.7112  18.518705          11.567316   \n",
       "136447           9.789915               14.7112  18.518705          11.567322   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       8.644031     11.667213      -7.813165     9.288283  5.373945   CMCSA   \n",
       "1       8.643982     11.667212     -11.112948     9.288273  5.374382   CMCSA   \n",
       "2       8.643613     11.667211      -5.748617     9.288295  5.373970   CMCSA   \n",
       "3       8.643575     11.667211      -5.601492     9.288314  5.373929   CMCSA   \n",
       "4       8.643531     11.667210      -5.387330     9.288295  5.373969   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  8.643430     11.667204     -11.419512     9.288273  5.374026  SNGR.L   \n",
       "136444  8.643419     11.667204      -1.360050     9.288680  5.373969  SNGR.L   \n",
       "136445  8.643422     11.667204     -10.202989     9.288273  5.374064  SNGR.L   \n",
       "136446  8.643426     11.667204      -9.609178     9.288273  5.374142  SNGR.L   \n",
       "136447  8.643422     11.667204      -1.358257     9.288487  5.374057  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We may need to consolidate larger values by taking the log of our data before\n",
    "# trying to pull out outliers\n",
    "\n",
    "def log_data(data, cols_to_log=None, feature_size=9):\n",
    "    cols_for_logging = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_log:\n",
    "        cols_for_logging = cols_to_log\n",
    "\n",
    "    for col in cols_for_logging:\n",
    "        # we have to offset all data by the absolute value of it if its negative\n",
    "        info = data[col].describe()\n",
    "        if info['min'] < 0:\n",
    "            data[col] = data[col] + np.abs(info['min']) + 1\n",
    "\n",
    "        data[col] = np.log(data[col])\n",
    "    \n",
    "    return data\n",
    "\n",
    "all_data = log_data(all_data)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length: 136448\n",
      "New length: 108152\n"
     ]
    }
   ],
   "source": [
    "# We need to attempt to handle outliers in our data.\n",
    "# this removes outliers based on the interquartile range\n",
    "def remove_outliers_iqr(data, iqr_mod=1.5, feature_size=9):\n",
    "    cols_for_trimming = data.columns[:feature_size]\n",
    "\n",
    "    # we need to go through each feature and check the inter quartile ranges.\n",
    "    # we'll drop values with info outside of the multiplier on the range we\n",
    "    # provided\n",
    "    print('Old length: {:d}'.format(len(data)))\n",
    "    for col in cols_for_trimming:\n",
    "        info = data[col].describe()\n",
    "        range_add = (info['75%'] - info['25%']) * iqr_mod\n",
    "        data = data[(data[col] >= info['25%'] - range_add)]\n",
    "        data = data[(data[col] <= info['75%'] + range_add)]\n",
    "    print('New length: {:d}'.format(len(data)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# it might be useful to try making one for the z-score, but I don't know\n",
    "# if our distribution is normalized.\n",
    "\n",
    "all_data = remove_outliers_iqr(all_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.790051250741316\n",
      "14.711201294824106\n",
      "18.518706486141376\n",
      "11.568015320151485\n",
      "8.645939524629462\n",
      "11.667243503526587\n",
      "3.6137942376991385\n",
      "9.288617587790053\n",
      "5.374716997680058\n"
     ]
    }
   ],
   "source": [
    "# there are a couple of preliminary steps we need to take. \n",
    "\n",
    "# 1) handle outliers\n",
    "# 2) normalize all of the data (0 to 1)\n",
    "# 3) separate testing and training sets\n",
    "\n",
    "\n",
    "def normalize_data(data, feature_size=9):\n",
    "    cols_for_normalization = data.columns[:feature_size]\n",
    "    \n",
    "    for col in cols_for_normalization:\n",
    "        max = data[col].max()\n",
    "        min = data[col].min()\n",
    "        print(max)\n",
    "\n",
    "        data[col] = (data[col] - min) / (max - min)\n",
    "\n",
    "normalize_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate training and test sets\n",
    "\n",
    "def separate_sets(data, prop_test, prop_train=0, feature_size=9):\n",
    "\n",
    "    # we need to drop superfluous info like tickers\n",
    "    data = data.drop(['ticker'], axis=1)\n",
    "\n",
    "    if prop_train == 0 or prop_train + prop_test > 1:\n",
    "        prop_train = 1 - prop_test\n",
    "    \n",
    "    # we find out the ratio to take from the remaining portion\n",
    "    prop_train = (1 - prop_test) / prop_train\n",
    "    if prop_train > 1:\n",
    "        prop_train = 1\n",
    "\n",
    "    test_set = data.sample(frac=prop_test)\n",
    "    data = data.drop(test_set.index)\n",
    "    train_set = data.sample(frac=prop_train)\n",
    "\n",
    "    test_f = np.array(test_set[test_set.columns[:feature_size]])\n",
    "    test_l = np.concatenate(np.array(test_set[test_set.columns[feature_size:]]))\n",
    "\n",
    "    train_f = np.array(train_set[train_set.columns[:feature_size]])\n",
    "    train_l = np.concatenate(np.array(train_set[train_set.columns[feature_size:]]))\n",
    "\n",
    "    return (test_f, test_l, train_f, train_l)\n",
    "\n",
    "test_features, test_labels, train_features, train_labels = separate_sets(all_data, .2)\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "Now that we have theoretcially written everything needed to properly prep the data, we need to choose a model to implement. I think for now I will attempt to start off with basic logistic regression and increase complexity as needed.\n",
    "\n",
    "This means we will need a: \n",
    "* predictor\n",
    "* loss function\n",
    "* derivative of loss function\n",
    "* gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0629476561442571"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_prediction(weights, features):\n",
    "    return 1 / (1 + np.exp(-np.dot(weights[1:], features) - weights[0]))\n",
    "\n",
    "make_prediction([-.5] * 10, train_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Functions\n",
    "\n",
    "I didn't actually remember the math behind this so I looked up a good logistic regression algorithm loss and gradient of the loss function. That way I can guarantee, or at least better guarantee, that if something is wrong it's not this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4029469349307278"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think a simple 1 or 0 single loss function might be the best for now\n",
    "# we can come back and change it later if need be\n",
    "\n",
    "# I got this from a youtube video. Math is hard. \n",
    "def single_loss_log(weights, features, label):\n",
    "    y_hat = make_prediction(weights, features)\n",
    "    return label * np.log(y_hat) + (1 - label) * np.log(1 - y_hat)\n",
    "\n",
    "def batch_loss(batch_start_ind, batch_size, loss_func, weights, train_f, train_l):\n",
    "    total_loss = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        total_loss += loss_func(weights, train_f[(point_ind + batch_start_ind) % len(train_f)], train_l[(point_ind + batch_start_ind) % len(train_l)])\n",
    "    return total_loss / batch_size\n",
    "\n",
    "single_loss_log([-.5] * 10, train_features[0], train_labels[0])\n",
    "batch_loss(0, len(train_labels), single_loss_log, [-.5] * 10, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42215646, -0.21803966, -0.21478412, -0.21225898, -0.21508605,\n",
       "       -0.21919136, -0.21539591, -0.20662341, -0.21525234, -0.21718698])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gradient(batch_start_ind, batch_size, weights, train_f, train_l):\n",
    "    total_diff_theta = np.array([0.0] * (len(weights) - 1))\n",
    "    total_diff_b = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        y_diff = make_prediction(weights, train_f[(point_ind + batch_start_ind) % len(train_f)]) - train_l[(point_ind + batch_start_ind) % len(train_l)]\n",
    "        total_diff_theta +=  y_diff * train_f[(point_ind + batch_start_ind) % len(train_f)]\n",
    "        total_diff_b += y_diff\n",
    "    total_diff_theta = np.insert(total_diff_theta, 0, total_diff_b)\n",
    "\n",
    "    total_diff_theta /= batch_size\n",
    "\n",
    "    return total_diff_theta\n",
    "\n",
    "grad = batch_gradient(0, len(train_labels), [-.5] * 10, train_features, train_labels)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.898137    0.43651079  0.23028434 -0.07874614 -0.1264305   1.02195984\n",
      " -0.06663804 -0.3022636  -0.35424905  0.86079359]\n"
     ]
    }
   ],
   "source": [
    "def batch_descent(train_l, train_f, batch_size, step_size, threshold, weights=np.array([0] * 10)):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.8f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights\n",
    "\n",
    "trained_model = batch_descent(train_labels, train_features, len(train_labels), 2, .0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4521497919556172"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_error(test_f, test_l, weights):\n",
    "    total = 0\n",
    "    test_total = 0\n",
    "    for point_ind in range(len(test_l)):\n",
    "        expectation = make_prediction(weights, test_f[point_ind])\n",
    "        \n",
    "        test_total += test_l[point_ind]\n",
    "\n",
    "        if (expectation - .5) * ( test_l[point_ind] - .5) < 0:\n",
    "            total += 1\n",
    "    \n",
    "    return total / len(test_l)\n",
    "\n",
    "calc_error(test_features, test_labels, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.898137    0.43651079  0.23028434 -0.07874614 -0.1264305   1.02195984\n",
      " -0.06663804 -0.3022636  -0.35424905  0.86079359]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.4816458622283865,\n",
       " 0.48169209431345356,\n",
       " 0.4815533980582524,\n",
       " 0.4815071659731854,\n",
       " 0.4813684697179843,\n",
       " 0.48132223763291726,\n",
       " 0.4809061488673139,\n",
       " 0.48072122052704575,\n",
       " 0.4804900601017106,\n",
       " 0.48030513176144246,\n",
       " 0.4798428109107721,\n",
       " 0.4795191863153028,\n",
       " 0.47891816920943137,\n",
       " 0.47840961627369394,\n",
       " 0.47776236708275543,\n",
       " 0.477161349976884,\n",
       " 0.47711511789181693,\n",
       " 0.4769764216366158,\n",
       " 0.4764216366158114,\n",
       " 0.4754045307443366,\n",
       " 0.4749422098936662,\n",
       " 0.4754045307443366,\n",
       " 0.4751271382339344,\n",
       " 0.47406380027739253,\n",
       " 0.473462783171521,\n",
       " 0.4732778548312529,\n",
       " 0.473416551086454,\n",
       " 0.4730004623208507,\n",
       " 0.472168284789644,\n",
       " 0.4707813222376329,\n",
       " 0.46971798428109107,\n",
       " 0.46856218215441514,\n",
       " 0.4684697179842811,\n",
       " 0.4672214516874711,\n",
       " 0.46689782709200184,\n",
       " 0.46726768377253813,\n",
       " 0.46689782709200184,\n",
       " 0.46675913083680076,\n",
       " 0.4662043458159963,\n",
       " 0.4659731853906611,\n",
       " 0.46560332871012483,\n",
       " 0.46449375866851594,\n",
       " 0.4638927415626445,\n",
       " 0.46412390198797965,\n",
       " 0.4634304207119741,\n",
       " 0.4636153490522423,\n",
       " 0.4635228848821082,\n",
       " 0.4635228848821082,\n",
       " 0.46384650947757744,\n",
       " 0.46384650947757744,\n",
       " 0.4638927415626445,\n",
       " 0.46356911696717523,\n",
       " 0.46282940360610264,\n",
       " 0.4634304207119741,\n",
       " 0.46292186777623673,\n",
       " 0.46273693943596855,\n",
       " 0.4624133148404993,\n",
       " 0.4623208506703652,\n",
       " 0.4623208506703652,\n",
       " 0.46208969024503005,\n",
       " 0.46245954692556634,\n",
       " 0.4617660656495608,\n",
       " 0.4616736014794267,\n",
       " 0.4618585298196949,\n",
       " 0.4613962089690245,\n",
       " 0.46144244105409155,\n",
       " 0.46199722607489596,\n",
       " 0.4621359223300971,\n",
       " 0.46153490522422563,\n",
       " 0.4614886731391586,\n",
       " 0.46070272769301895,\n",
       " 0.4606564956079519,\n",
       " 0.46051779935275083,\n",
       " 0.4608414239482201,\n",
       " 0.4608414239482201,\n",
       " 0.46070272769301895,\n",
       " 0.45977808599167824,\n",
       " 0.4597318539066112,\n",
       " 0.459454461396209,\n",
       " 0.4596393897364771,\n",
       " 0.4593619972260749,\n",
       " 0.45880721220527043,\n",
       " 0.45839112343966715,\n",
       " 0.45792880258899676,\n",
       " 0.4579750346740638,\n",
       " 0.4576976421636616,\n",
       " 0.45737401756819235,\n",
       " 0.45737401756819235,\n",
       " 0.45728155339805826,\n",
       " 0.4575127138233934,\n",
       " 0.4574664817383264,\n",
       " 0.45728155339805826,\n",
       " 0.45737401756819235,\n",
       " 0.4572353213129912,\n",
       " 0.4572353213129912,\n",
       " 0.45691169671752196,\n",
       " 0.45677300046232083,\n",
       " 0.4566805362921868,\n",
       " 0.4566805362921868,\n",
       " 0.45640314378178454,\n",
       " 0.4559408229311142,\n",
       " 0.45598705501618125,\n",
       " 0.4562182154415164,\n",
       " 0.45626444752658346,\n",
       " 0.4559408229311142,\n",
       " 0.4560795191863153,\n",
       " 0.45612575127138233,\n",
       " 0.4555709662505779,\n",
       " 0.455663430420712,\n",
       " 0.455709662505779,\n",
       " 0.45552473416551087,\n",
       " 0.455663430420712,\n",
       " 0.4558483587609801,\n",
       " 0.45589459084604717,\n",
       " 0.455663430420712,\n",
       " 0.45575589459084603,\n",
       " 0.45603328710124824,\n",
       " 0.4559408229311142,\n",
       " 0.45635691169671755,\n",
       " 0.45635691169671755,\n",
       " 0.4560795191863153,\n",
       " 0.45575589459084603,\n",
       " 0.455709662505779,\n",
       " 0.45561719833564496,\n",
       " 0.455663430420712,\n",
       " 0.4555709662505779,\n",
       " 0.45561719833564496,\n",
       " 0.4554785020804438,\n",
       " 0.4552011095700416,\n",
       " 0.4550624133148405,\n",
       " 0.45496994914470645,\n",
       " 0.4550624133148405,\n",
       " 0.45510864539990753,\n",
       " 0.4549237170596394,\n",
       " 0.45501618122977344,\n",
       " 0.45510864539990753,\n",
       " 0.4552011095700416,\n",
       " 0.45538603791030974,\n",
       " 0.45524734165510866,\n",
       " 0.45561719833564496,\n",
       " 0.4558021266759131,\n",
       " 0.45561719833564496,\n",
       " 0.455709662505779,\n",
       " 0.45575589459084603,\n",
       " 0.455663430420712,\n",
       " 0.45561719833564496,\n",
       " 0.455709662505779,\n",
       " 0.4559408229311142,\n",
       " 0.4560795191863153,\n",
       " 0.4560795191863153,\n",
       " 0.4562182154415164,\n",
       " 0.45612575127138233,\n",
       " 0.45598705501618125,\n",
       " 0.45603328710124824,\n",
       " 0.45603328710124824,\n",
       " 0.45575589459084603,\n",
       " 0.45552473416551087,\n",
       " 0.45561719833564496,\n",
       " 0.45561719833564496,\n",
       " 0.455663430420712,\n",
       " 0.455663430420712,\n",
       " 0.4555709662505779,\n",
       " 0.455663430420712,\n",
       " 0.455709662505779,\n",
       " 0.4558021266759131,\n",
       " 0.4558021266759131,\n",
       " 0.45575589459084603,\n",
       " 0.455663430420712,\n",
       " 0.4555709662505779,\n",
       " 0.4555709662505779,\n",
       " 0.4555709662505779,\n",
       " 0.4555709662505779,\n",
       " 0.4554785020804438,\n",
       " 0.4554785020804438,\n",
       " 0.4555709662505779,\n",
       " 0.4554322699953768,\n",
       " 0.4554785020804438,\n",
       " 0.4554785020804438,\n",
       " 0.4554322699953768,\n",
       " 0.45538603791030974,\n",
       " 0.45538603791030974,\n",
       " 0.45524734165510866,\n",
       " 0.4552011095700416,\n",
       " 0.4551548774849746,\n",
       " 0.4551548774849746,\n",
       " 0.45501618122977344,\n",
       " 0.45496994914470645,\n",
       " 0.45496994914470645,\n",
       " 0.45487748497457237,\n",
       " 0.4548312528895053,\n",
       " 0.45487748497457237,\n",
       " 0.4549237170596394,\n",
       " 0.45496994914470645,\n",
       " 0.4550624133148405,\n",
       " 0.45524734165510866,\n",
       " 0.45524734165510866,\n",
       " 0.4553398058252427,\n",
       " 0.4553398058252427,\n",
       " 0.4552011095700416,\n",
       " 0.4552935737401757,\n",
       " 0.45524734165510866,\n",
       " 0.4550624133148405,\n",
       " 0.45501618122977344,\n",
       " 0.45501618122977344,\n",
       " 0.45501618122977344,\n",
       " 0.45496994914470645,\n",
       " 0.4550624133148405,\n",
       " 0.45496994914470645,\n",
       " 0.4548312528895053,\n",
       " 0.45473878871937123,\n",
       " 0.4547850208044383,\n",
       " 0.4548312528895053,\n",
       " 0.4549237170596394,\n",
       " 0.45473878871937123,\n",
       " 0.45450762829403607,\n",
       " 0.45464632454923715,\n",
       " 0.4546925566343042,\n",
       " 0.45464632454923715,\n",
       " 0.45460009246417016,\n",
       " 0.4545538603791031,\n",
       " 0.45464632454923715,\n",
       " 0.45460009246417016,\n",
       " 0.45450762829403607,\n",
       " 0.45460009246417016,\n",
       " 0.45460009246417016,\n",
       " 0.4545538603791031,\n",
       " 0.454461396208969,\n",
       " 0.454461396208969,\n",
       " 0.45460009246417016,\n",
       " 0.45460009246417016,\n",
       " 0.45460009246417016,\n",
       " 0.4545538603791031,\n",
       " 0.4545538603791031,\n",
       " 0.454461396208969,\n",
       " 0.454415164123902,\n",
       " 0.454461396208969,\n",
       " 0.45450762829403607,\n",
       " 0.454415164123902,\n",
       " 0.4543226999537679,\n",
       " 0.45450762829403607,\n",
       " 0.454415164123902,\n",
       " 0.454415164123902,\n",
       " 0.454461396208969,\n",
       " 0.454461396208969,\n",
       " 0.45450762829403607,\n",
       " 0.45450762829403607,\n",
       " 0.454461396208969,\n",
       " 0.4545538603791031,\n",
       " 0.4543226999537679,\n",
       " 0.45436893203883494,\n",
       " 0.45436893203883494,\n",
       " 0.454415164123902,\n",
       " 0.454461396208969,\n",
       " 0.45436893203883494,\n",
       " 0.4542764678687009,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.454461396208969,\n",
       " 0.45450762829403607,\n",
       " 0.4545538603791031,\n",
       " 0.45450762829403607,\n",
       " 0.454415164123902,\n",
       " 0.454461396208969,\n",
       " 0.454461396208969,\n",
       " 0.45450762829403607,\n",
       " 0.454461396208969,\n",
       " 0.454461396208969,\n",
       " 0.454415164123902,\n",
       " 0.45436893203883494,\n",
       " 0.45436893203883494,\n",
       " 0.4543226999537679,\n",
       " 0.4542764678687009,\n",
       " 0.4541377716134998,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4541377716134998,\n",
       " 0.45399907535829864,\n",
       " 0.4539066111881646,\n",
       " 0.45386037910309757,\n",
       " 0.4539528432732316,\n",
       " 0.45386037910309757,\n",
       " 0.45386037910309757,\n",
       " 0.45386037910309757,\n",
       " 0.45399907535829864,\n",
       " 0.45399907535829864,\n",
       " 0.45386037910309757,\n",
       " 0.4538141470180305,\n",
       " 0.45386037910309757,\n",
       " 0.4539528432732316,\n",
       " 0.4539066111881646,\n",
       " 0.4539528432732316,\n",
       " 0.45372168284789643,\n",
       " 0.4536754507628294,\n",
       " 0.45372168284789643,\n",
       " 0.4536754507628294,\n",
       " 0.45372168284789643,\n",
       " 0.4536754507628294,\n",
       " 0.4537679149329635,\n",
       " 0.45372168284789643,\n",
       " 0.45372168284789643,\n",
       " 0.45372168284789643,\n",
       " 0.4536754507628294,\n",
       " 0.4536754507628294,\n",
       " 0.4535829865926953,\n",
       " 0.4535367545076283,\n",
       " 0.4534442903374942,\n",
       " 0.4534442903374942,\n",
       " 0.4535829865926953,\n",
       " 0.45349052242256127,\n",
       " 0.45349052242256127,\n",
       " 0.4535829865926953,\n",
       " 0.4535367545076283,\n",
       " 0.45349052242256127,\n",
       " 0.4534442903374942,\n",
       " 0.45349052242256127,\n",
       " 0.4535367545076283,\n",
       " 0.4535829865926953,\n",
       " 0.4534442903374942,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.45335182616736014,\n",
       " 0.45335182616736014,\n",
       " 0.4533055940822931,\n",
       " 0.45335182616736014,\n",
       " 0.45335182616736014,\n",
       " 0.45335182616736014,\n",
       " 0.4533055940822931,\n",
       " 0.4533055940822931,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.453166897827092,\n",
       " 0.45321312991215906,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.4533055940822931,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.45335182616736014,\n",
       " 0.4533055940822931,\n",
       " 0.4533055940822931,\n",
       " 0.45335182616736014,\n",
       " 0.4533980582524272,\n",
       " 0.4533055940822931,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.45349052242256127,\n",
       " 0.45349052242256127,\n",
       " 0.45349052242256127,\n",
       " 0.45349052242256127,\n",
       " 0.45349052242256127,\n",
       " 0.4534442903374942,\n",
       " 0.45335182616736014,\n",
       " 0.4534442903374942,\n",
       " 0.4534442903374942,\n",
       " 0.4535829865926953,\n",
       " 0.4536754507628294,\n",
       " 0.45362921867776235,\n",
       " 0.4536754507628294,\n",
       " 0.45362921867776235,\n",
       " 0.45362921867776235,\n",
       " 0.4536754507628294,\n",
       " 0.45349052242256127,\n",
       " 0.4534442903374942,\n",
       " 0.4534442903374942,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.4534442903374942,\n",
       " 0.4534442903374942,\n",
       " 0.45349052242256127,\n",
       " 0.45349052242256127,\n",
       " 0.4535367545076283,\n",
       " 0.4535367545076283,\n",
       " 0.4535367545076283,\n",
       " 0.4535367545076283,\n",
       " 0.4535367545076283,\n",
       " 0.45362921867776235,\n",
       " 0.45362921867776235,\n",
       " 0.45362921867776235,\n",
       " 0.4535829865926953,\n",
       " 0.45349052242256127,\n",
       " 0.45349052242256127,\n",
       " 0.4534442903374942,\n",
       " 0.4534442903374942,\n",
       " 0.4533980582524272,\n",
       " 0.45335182616736014,\n",
       " 0.4533055940822931,\n",
       " 0.45335182616736014,\n",
       " 0.45325936199722605,\n",
       " 0.45321312991215906,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.45321312991215906,\n",
       " 0.45321312991215906,\n",
       " 0.45321312991215906,\n",
       " 0.45321312991215906,\n",
       " 0.45325936199722605,\n",
       " 0.45321312991215906,\n",
       " 0.453166897827092,\n",
       " 0.45321312991215906,\n",
       " 0.45325936199722605,\n",
       " 0.45321312991215906,\n",
       " 0.45335182616736014,\n",
       " 0.4533980582524272,\n",
       " 0.45349052242256127,\n",
       " 0.4534442903374942,\n",
       " 0.4535367545076283,\n",
       " 0.45349052242256127,\n",
       " 0.4535829865926953,\n",
       " 0.4535829865926953,\n",
       " 0.4535829865926953,\n",
       " 0.45362921867776235,\n",
       " 0.45349052242256127,\n",
       " 0.45349052242256127,\n",
       " 0.4534442903374942,\n",
       " 0.45335182616736014,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.45335182616736014,\n",
       " 0.4533055940822931,\n",
       " 0.45335182616736014,\n",
       " 0.4533980582524272,\n",
       " 0.4533980582524272,\n",
       " 0.4534442903374942,\n",
       " 0.4534442903374942,\n",
       " 0.4534442903374942,\n",
       " 0.45349052242256127,\n",
       " 0.4535367545076283,\n",
       " 0.45349052242256127,\n",
       " 0.4534442903374942,\n",
       " 0.4534442903374942,\n",
       " 0.4535367545076283,\n",
       " 0.4535829865926953,\n",
       " 0.4536754507628294,\n",
       " 0.4535367545076283,\n",
       " 0.4535367545076283,\n",
       " 0.4535367545076283,\n",
       " 0.4534442903374942,\n",
       " 0.4534442903374942,\n",
       " 0.4535367545076283,\n",
       " 0.45349052242256127,\n",
       " 0.4534442903374942,\n",
       " 0.4533980582524272,\n",
       " 0.45349052242256127,\n",
       " 0.4533980582524272,\n",
       " 0.45335182616736014,\n",
       " 0.4533980582524272,\n",
       " 0.45335182616736014,\n",
       " 0.4533055940822931,\n",
       " 0.453166897827092,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.45325936199722605,\n",
       " 0.4533055940822931,\n",
       " 0.4533055940822931,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.4533055940822931,\n",
       " 0.453166897827092,\n",
       " 0.45307443365695793,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.45298196948682384,\n",
       " 0.4529357374017568,\n",
       " 0.45284327323162277,\n",
       " 0.45284327323162277,\n",
       " 0.45284327323162277,\n",
       " 0.45284327323162277,\n",
       " 0.45284327323162277,\n",
       " 0.4529357374017568,\n",
       " 0.45298196948682384,\n",
       " 0.4527970411465557,\n",
       " 0.4527970411465557,\n",
       " 0.4527508090614887,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " 0.4526583448913546,\n",
       " 0.4525658807212205,\n",
       " 0.4525658807212205,\n",
       " 0.4525196486361535,\n",
       " 0.4525658807212205,\n",
       " 0.45261211280628755,\n",
       " 0.45261211280628755,\n",
       " 0.4525196486361535,\n",
       " 0.4525196486361535,\n",
       " 0.45247341655108647,\n",
       " 0.4524271844660194,\n",
       " 0.4524271844660194,\n",
       " 0.4524271844660194,\n",
       " 0.4523809523809524,\n",
       " 0.4523809523809524,\n",
       " 0.45233472029588534,\n",
       " 0.4522884882108183,\n",
       " 0.4522884882108183,\n",
       " 0.4522884882108183,\n",
       " 0.45233472029588534,\n",
       " 0.4523809523809524,\n",
       " 0.4522884882108183,\n",
       " 0.45224225612575125,\n",
       " 0.45224225612575125,\n",
       " 0.4521960240406842,\n",
       " 0.45224225612575125,\n",
       " 0.45224225612575125,\n",
       " 0.4521960240406842,\n",
       " 0.4521035598705502,\n",
       " 0.4521035598705502,\n",
       " 0.4521035598705502,\n",
       " 0.4521035598705502,\n",
       " 0.45205732778548313,\n",
       " 0.45205732778548313,\n",
       " 0.4520110957004161,\n",
       " 0.45205732778548313,\n",
       " 0.4521035598705502,\n",
       " 0.4521497919556172,\n",
       " 0.4521035598705502,\n",
       " 0.4521497919556172,\n",
       " 0.45205732778548313,\n",
       " 0.4521035598705502,\n",
       " 0.45205732778548313,\n",
       " 0.4521497919556172,\n",
       " 0.4521497919556172,\n",
       " 0.4521497919556172,\n",
       " 0.4520110957004161,\n",
       " 0.45205732778548313,\n",
       " 0.4521035598705502,\n",
       " 0.4521960240406842,\n",
       " 0.4521960240406842,\n",
       " 0.4521960240406842,\n",
       " 0.4521497919556172,\n",
       " 0.45224225612575125,\n",
       " 0.45224225612575125,\n",
       " 0.4521960240406842,\n",
       " 0.45224225612575125,\n",
       " 0.4521960240406842,\n",
       " 0.4521035598705502,\n",
       " 0.4521960240406842,\n",
       " 0.4521035598705502,\n",
       " 0.45205732778548313,\n",
       " 0.45196486361534904,\n",
       " 0.45196486361534904,\n",
       " 0.45182616736014797,\n",
       " 0.45196486361534904,\n",
       " 0.45196486361534904,\n",
       " 0.4520110957004161,\n",
       " 0.45205732778548313,\n",
       " 0.4521035598705502,\n",
       " 0.45205732778548313,\n",
       " 0.45205732778548313,\n",
       " 0.4521035598705502,\n",
       " 0.4521035598705502,\n",
       " 0.4521035598705502,\n",
       " 0.4521497919556172]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to be able to make a graph of the error over epochs\n",
    "# so I am going to do that here\n",
    "def batch_descent_error_tracking(train_l, train_f, test_f, test_l, batch_size, step_size, threshold, weights=np.array([0] * 10)):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "    errors = list()\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "            errors.append(calc_error(test_f, test_l, weights))\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.8f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights, errors\n",
    "\n",
    "trained_model, errors = batch_descent_error_tracking(train_labels, train_features, test_features, test_labels, len(train_labels), 2, .0005)\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we need to make a graph of this\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(errors)), errors)\n",
    "plt.xlabel('Epochs of training (whole data set)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Testing error over time')\n",
    "plt.savefig('model1perf.jpg')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Analysis\n",
    "\n",
    "I am going to build a 9-10-10-1 fully connected network with a basic sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's make the activation function for simplicity sake\n",
    "def activate(x):\n",
    "    return 1 / (1 + np.exp(-1 * x)) \n",
    "\n",
    "def active_grad(x):\n",
    "    return np.exp(-1 * x) / (1 + np.exp(-1 * x)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49420331 0.58369711 0.55093786 ... 0.46858582 0.4899978  0.47202463]\n",
      " [0.46292652 0.37304276 0.62724563 ... 0.47236457 0.5239749  0.45200056]\n",
      " [0.45098687 0.41837307 0.60661045 ... 0.40238183 0.46157213 0.42498836]\n",
      " ...\n",
      " [0.47999593 0.39616514 0.48014235 ... 0.3357062  0.44270399 0.51367435]\n",
      " [0.51059909 0.4223003  0.43497771 ... 0.61016178 0.52216708 0.64601989]\n",
      " [0.45259873 0.44015791 0.50073155 ... 0.45841361 0.466105   0.46836553]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# we need to prepare our data to be run through the network in the correct shape\n",
    "nt_labels = train_labels.reshape(len(train_labels), 1)\n",
    "nt_features = train_features\n",
    "print(nt_features)\n",
    "print(nt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll also select a starting seed\n",
    "# np.random.seed(2022)\n",
    "\n",
    "# now we should initialize our neurons in our layers\n",
    "layer1 = 2 * np.random.random((9, 10)) - 1 \n",
    "layer2 = 2 * np.random.random((10, 10)) - 1\n",
    "layer3 = 2 * np.random.random((10, 1)) - 1\n",
    "\n",
    "# I will write a function that trains for a set number of epochs\n",
    "def train_epochs(num_epochs, alpha, layer1, layer2, layer3, features, labels, printfreq=10):\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        l0 = features\n",
    "        l1 = activate(np.dot(l0, layer1))\n",
    "        l2 = activate(np.dot(l1, layer2))\n",
    "        l3 = activate(np.dot(l2, layer3))\n",
    "\n",
    "        if e % printfreq == 0:\n",
    "            l3_error = labels - l3 \n",
    "\n",
    "            # we should print our current training error every once in awhile\n",
    "            print(\"Epoch {:d} Error:{:2.4f}%\".format(e, np.mean(np.abs(l3_error)) * 100))\n",
    "\n",
    "        # let's go through and backpropagate\n",
    "\n",
    "        l3_grad = l3_error * active_grad(l3)\n",
    "        \n",
    "        l2_error = l3_grad.dot(layer3.T)\n",
    "\n",
    "        l2_grad = l2_error * active_grad(l2)\n",
    "\n",
    "        l1_error = l2_grad.dot(layer2.T)\n",
    "\n",
    "        l1_grad = l1_error * active_grad(l1)\n",
    "\n",
    "        # now we want to update our weights\n",
    "        layer3 = layer3 + alpha * l2.T.dot(l3_grad)\n",
    "        layer2 = layer2 + alpha * l1.T.dot(l2_grad)\n",
    "        layer1 = layer1 + alpha * l0.T.dot(l1_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Error:50.5623%\n",
      "Epoch 10 Error:48.2421%\n",
      "Epoch 20 Error:51.7579%\n",
      "Epoch 30 Error:48.2423%\n",
      "Epoch 40 Error:51.7577%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/629854626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/3374923772.py\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(num_epochs, alpha, layer1, layer2, layer3, features, labels, printfreq)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# let's go through and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0ml3_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactive_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0ml2_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/902923763.py\u001b[0m in \u001b[0;36mactive_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mactive_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_epochs(8000, .0001, layer1, layer2, layer3, nt_features, nt_labels, printfreq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 48.9585%\n"
     ]
    }
   ],
   "source": [
    "# let's check our test error\n",
    "ntest_labels = test_labels.reshape(len(test_labels), 1)\n",
    "ntest_features = test_features\n",
    "\n",
    "l0 = ntest_features\n",
    "l1 = activate(np.dot(l0, layer1))\n",
    "l2 = activate(np.dot(l1, layer2))\n",
    "l3 = activate(np.dot(l2, layer3))\n",
    "\n",
    "errors = ntest_labels - l3\n",
    "total_error = np.mean(np.abs(errors))\n",
    "print('Test Error: {:2.4f}%'.format(total_error * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
