{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "I have successfully been able to build a sort of data set for this model. Now I want to try training on my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037304</td>\n",
       "      <td>0.081930</td>\n",
       "      <td>68.692936</td>\n",
       "      <td>16.198334</td>\n",
       "      <td>3.792215</td>\n",
       "      <td>1.051065</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.366850</td>\n",
       "      <td>0.337912</td>\n",
       "      <td>8.135137</td>\n",
       "      <td>18.134324</td>\n",
       "      <td>3.515821</td>\n",
       "      <td>0.918390</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.108044</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.132520</td>\n",
       "      <td>0.173255</td>\n",
       "      <td>18.516105</td>\n",
       "      <td>6.586352</td>\n",
       "      <td>1.420348</td>\n",
       "      <td>0.761143</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.236057</td>\n",
       "      <td>0.019177</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071330</td>\n",
       "      <td>0.172091</td>\n",
       "      <td>29.580969</td>\n",
       "      <td>5.532793</td>\n",
       "      <td>1.205178</td>\n",
       "      <td>0.746057</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.436893</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.133795</td>\n",
       "      <td>0.211265</td>\n",
       "      <td>12.743329</td>\n",
       "      <td>4.438462</td>\n",
       "      <td>0.957572</td>\n",
       "      <td>0.715781</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.018786</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.265324</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>3.700391</td>\n",
       "      <td>0.383926</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.948329</td>\n",
       "      <td>4.281250</td>\n",
       "      <td>3.332726</td>\n",
       "      <td>0.322385</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>4.395096</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>0.792549</td>\n",
       "      <td>1.878183</td>\n",
       "      <td>2.160751</td>\n",
       "      <td>2.286619</td>\n",
       "      <td>0.339581</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.039290</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.440450</td>\n",
       "      <td>1.614583</td>\n",
       "      <td>2.256612</td>\n",
       "      <td>0.362996</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.056206</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.242760</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.904923</td>\n",
       "      <td>0.341091</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.257109</td>\n",
       "      <td>2.313978</td>\n",
       "      <td>0.037899</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                0.037304              0.081930  68.692936          16.198334   \n",
       "1                0.366850              0.337912   8.135137          18.134324   \n",
       "2                0.132520              0.173255  18.516105           6.586352   \n",
       "3                0.071330              0.172091  29.580969           5.532793   \n",
       "4                0.133795              0.211265  12.743329           4.438462   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           0.600000              1.265324   3.083333           3.700391   \n",
       "136444           0.400000              0.948329   4.281250           3.332726   \n",
       "136445           0.792549              1.878183   2.160751           2.286619   \n",
       "136446           1.200000              1.440450   1.614583           2.256612   \n",
       "136447           0.800000              1.242760   2.250000           2.904923   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       3.792215      1.051065       0.000404     0.111111  0.013801   CMCSA   \n",
       "1       3.515821      0.918390       0.000015     0.000485  0.108044   CMCSA   \n",
       "2       1.420348      0.761143       0.003187     0.236057  0.019177   CMCSA   \n",
       "3       1.205178      0.746057       0.003692     0.436893  0.010185   CMCSA   \n",
       "4       0.957572      0.715781       0.004574     0.233161  0.018786   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  0.383926      0.001104       0.000011     0.000135  0.031129  SNGR.L   \n",
       "136444  0.322385      0.001067       0.256648     4.395096  0.018825  SNGR.L   \n",
       "136445  0.339581      0.001007       0.000037     0.000320  0.039290  SNGR.L   \n",
       "136446  0.362996      0.000928       0.000067     0.000433  0.056206  SNGR.L   \n",
       "136447  0.341091      0.000955       0.257109     2.313978  0.037899  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# first we need to read the data in\n",
    "\n",
    "data_name = 'dataset.csv'\n",
    "\n",
    "all_data = pd.read_csv(data_name)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.992347e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888704</td>\n",
       "      <td>2.999003e-06</td>\n",
       "      <td>0.349871</td>\n",
       "      <td>0.256471</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.992457e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888703</td>\n",
       "      <td>1.106367e-07</td>\n",
       "      <td>0.349867</td>\n",
       "      <td>0.256583</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.991801e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888701</td>\n",
       "      <td>2.363735e-05</td>\n",
       "      <td>0.349875</td>\n",
       "      <td>0.256477</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.991742e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888701</td>\n",
       "      <td>2.738384e-05</td>\n",
       "      <td>0.349881</td>\n",
       "      <td>0.256467</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.991680e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888701</td>\n",
       "      <td>3.392375e-05</td>\n",
       "      <td>0.349875</td>\n",
       "      <td>0.256477</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.991638e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888696</td>\n",
       "      <td>8.142551e-08</td>\n",
       "      <td>0.349867</td>\n",
       "      <td>0.256492</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.991617e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888696</td>\n",
       "      <td>1.903396e-03</td>\n",
       "      <td>0.350010</td>\n",
       "      <td>0.256477</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.991557e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888696</td>\n",
       "      <td>2.748462e-07</td>\n",
       "      <td>0.349867</td>\n",
       "      <td>0.256501</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.991556e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888696</td>\n",
       "      <td>4.977124e-07</td>\n",
       "      <td>0.349867</td>\n",
       "      <td>0.256522</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>5.991593e-11</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.888696</td>\n",
       "      <td>1.906813e-03</td>\n",
       "      <td>0.349942</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare   peRatio  priceToSalesRatio  \\\n",
       "0                0.004202              0.973658  0.527439       5.992347e-11   \n",
       "1                0.004202              0.973658  0.527439       5.992457e-11   \n",
       "2                0.004202              0.973658  0.527439       5.991801e-11   \n",
       "3                0.004202              0.973658  0.527439       5.991742e-11   \n",
       "4                0.004202              0.973658  0.527439       5.991680e-11   \n",
       "...                   ...                   ...       ...                ...   \n",
       "136443           0.004202              0.973658  0.527439       5.991638e-11   \n",
       "136444           0.004202              0.973658  0.527439       5.991617e-11   \n",
       "136445           0.004202              0.973658  0.527439       5.991557e-11   \n",
       "136446           0.004202              0.973658  0.527439       5.991556e-11   \n",
       "136447           0.004202              0.973658  0.527439       5.991593e-11   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       0.000225      0.888704   2.999003e-06     0.349871  0.256471   CMCSA   \n",
       "1       0.000225      0.888703   1.106367e-07     0.349867  0.256583   CMCSA   \n",
       "2       0.000225      0.888701   2.363735e-05     0.349875  0.256477   CMCSA   \n",
       "3       0.000225      0.888701   2.738384e-05     0.349881  0.256467   CMCSA   \n",
       "4       0.000225      0.888701   3.392375e-05     0.349875  0.256477   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  0.000225      0.888696   8.142551e-08     0.349867  0.256492  SNGR.L   \n",
       "136444  0.000225      0.888696   1.903396e-03     0.350010  0.256477  SNGR.L   \n",
       "136445  0.000225      0.888696   2.748462e-07     0.349867  0.256501  SNGR.L   \n",
       "136446  0.000225      0.888696   4.977124e-07     0.349867  0.256522  SNGR.L   \n",
       "136447  0.000225      0.888696   1.906813e-03     0.349942  0.256500  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try a new strategy for normalization\n",
    "# we'll update each value to be the z-score of the value within the overall distribution\n",
    "def zscore_data(data, cols_to_normalize=None, feature_size=9):\n",
    "    cols_for_normalizing = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_normalize:\n",
    "        cols_for_normalizing = cols_to_normalize\n",
    "    \n",
    "    for col in cols_for_normalizing:\n",
    "        col_data = data[col]\n",
    "        mu = np.mean(col_data)\n",
    "        sigma = np.std(col_data)\n",
    "\n",
    "        data[col] = (data[col] - mu) / sigma\n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_data(data, cols_to_normalize=None, feature_size=9):\n",
    "    cols_for_norm = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_normalize:\n",
    "        cols_for_norm = cols_to_normalize\n",
    "    \n",
    "    for col in cols_for_norm:\n",
    "        col_data = data[col]\n",
    "        min_val = np.min(col_data)\n",
    "        max_val = np.max(col_data)\n",
    "\n",
    "        data[col] = (data[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return data\n",
    "\n",
    "all_data = normalize_data(zscore_data(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.789873</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518706</td>\n",
       "      <td>11.567448</td>\n",
       "      <td>8.644031</td>\n",
       "      <td>11.667213</td>\n",
       "      <td>-7.813165</td>\n",
       "      <td>9.288283</td>\n",
       "      <td>5.373945</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.789891</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567466</td>\n",
       "      <td>8.643982</td>\n",
       "      <td>11.667212</td>\n",
       "      <td>-11.112948</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374382</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567357</td>\n",
       "      <td>8.643613</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.748617</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373970</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.789875</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567347</td>\n",
       "      <td>8.643575</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.601492</td>\n",
       "      <td>9.288314</td>\n",
       "      <td>5.373929</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567337</td>\n",
       "      <td>8.643531</td>\n",
       "      <td>11.667210</td>\n",
       "      <td>-5.387330</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>9.789904</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567330</td>\n",
       "      <td>8.643430</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-11.419512</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374026</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>9.789893</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567326</td>\n",
       "      <td>8.643419</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.360050</td>\n",
       "      <td>9.288680</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-10.202989</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374064</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>9.789938</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643426</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-9.609178</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374142</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567322</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.358257</td>\n",
       "      <td>9.288487</td>\n",
       "      <td>5.374057</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                9.789873               14.7112  18.518706          11.567448   \n",
       "1                9.789891               14.7112  18.518705          11.567466   \n",
       "2                9.789878               14.7112  18.518705          11.567357   \n",
       "3                9.789875               14.7112  18.518705          11.567347   \n",
       "4                9.789878               14.7112  18.518705          11.567337   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           9.789904               14.7112  18.518705          11.567330   \n",
       "136444           9.789893               14.7112  18.518705          11.567326   \n",
       "136445           9.789915               14.7112  18.518705          11.567316   \n",
       "136446           9.789938               14.7112  18.518705          11.567316   \n",
       "136447           9.789915               14.7112  18.518705          11.567322   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       8.644031     11.667213      -7.813165     9.288283  5.373945   CMCSA   \n",
       "1       8.643982     11.667212     -11.112948     9.288273  5.374382   CMCSA   \n",
       "2       8.643613     11.667211      -5.748617     9.288295  5.373970   CMCSA   \n",
       "3       8.643575     11.667211      -5.601492     9.288314  5.373929   CMCSA   \n",
       "4       8.643531     11.667210      -5.387330     9.288295  5.373969   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  8.643430     11.667204     -11.419512     9.288273  5.374026  SNGR.L   \n",
       "136444  8.643419     11.667204      -1.360050     9.288680  5.373969  SNGR.L   \n",
       "136445  8.643422     11.667204     -10.202989     9.288273  5.374064  SNGR.L   \n",
       "136446  8.643426     11.667204      -9.609178     9.288273  5.374142  SNGR.L   \n",
       "136447  8.643422     11.667204      -1.358257     9.288487  5.374057  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We may need to consolidate larger values by taking the log of our data before\n",
    "# trying to pull out outliers\n",
    "\n",
    "def log_data(data, cols_to_log=None, feature_size=9):\n",
    "    cols_for_logging = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_log:\n",
    "        cols_for_logging = cols_to_log\n",
    "\n",
    "    for col in cols_for_logging:\n",
    "        # we have to offset all data by the absolute value of it if its negative\n",
    "        info = data[col].describe()\n",
    "        if info['min'] < 0:\n",
    "            data[col] = data[col] + np.abs(info['min']) + 1\n",
    "\n",
    "        data[col] = np.log(data[col])\n",
    "    \n",
    "    return data\n",
    "\n",
    "all_data = log_data(all_data)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length: 136448\n",
      "New length: 108152\n"
     ]
    }
   ],
   "source": [
    "# We need to attempt to handle outliers in our data.\n",
    "# this removes outliers based on the interquartile range\n",
    "def remove_outliers_iqr(data, iqr_mod=1.5, feature_size=9):\n",
    "    cols_for_trimming = data.columns[:feature_size]\n",
    "\n",
    "    # we need to go through each feature and check the inter quartile ranges.\n",
    "    # we'll drop values with info outside of the multiplier on the range we\n",
    "    # provided\n",
    "    print('Old length: {:d}'.format(len(data)))\n",
    "    for col in cols_for_trimming:\n",
    "        info = data[col].describe()\n",
    "        range_add = (info['75%'] - info['25%']) * iqr_mod\n",
    "        data = data[(data[col] >= info['25%'] - range_add)]\n",
    "        data = data[(data[col] <= info['75%'] + range_add)]\n",
    "    print('New length: {:d}'.format(len(data)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# it might be useful to try making one for the z-score, but I don't know\n",
    "# if our distribution is normalized.\n",
    "\n",
    "all_data = remove_outliers_iqr(all_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.78987113, 14.71119965, 18.51870621, ..., -8.05034455,\n",
       "         9.28828851,  5.37389466],\n",
       "       [ 9.78990736, 14.71119986, 18.51870514, ..., -3.79912487,\n",
       "         9.2883712 ,  5.37393636],\n",
       "       [ 9.78991406, 14.71120007, 18.51870515, ..., -5.61140057,\n",
       "         9.28829131,  5.37422957],\n",
       "       ...,\n",
       "       [ 9.78992823, 14.71119958, 18.5187052 , ..., -4.62691553,\n",
       "         9.28834076,  5.37407082],\n",
       "       [ 9.78987188, 14.71119972, 18.5187058 , ..., -5.33821812,\n",
       "         9.28842373,  5.37393402],\n",
       "       [ 9.78989564, 14.71119976, 18.51870509, ..., -6.15571217,\n",
       "         9.28827837,  5.37422371]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate training and test sets\n",
    "\n",
    "def separate_sets(data, prop_test, prop_train=0, feature_size=9):\n",
    "\n",
    "    # we need to drop superfluous info like tickers\n",
    "    data = data.drop(['ticker'], axis=1)\n",
    "\n",
    "    if prop_train == 0 or prop_train + prop_test > 1:\n",
    "        prop_train = 1 - prop_test\n",
    "    \n",
    "    # we find out the ratio to take from the remaining portion\n",
    "    prop_train = (1 - prop_test) / prop_train\n",
    "    if prop_train > 1:\n",
    "        prop_train = 1\n",
    "\n",
    "    test_set = data.sample(frac=prop_test)\n",
    "    data = data.drop(test_set.index)\n",
    "    train_set = data.sample(frac=prop_train)\n",
    "\n",
    "    test_f = np.array(test_set[test_set.columns[:feature_size]])\n",
    "    test_l = np.concatenate(np.array(test_set[test_set.columns[feature_size:]]))\n",
    "\n",
    "    train_f = np.array(train_set[train_set.columns[:feature_size]])\n",
    "    train_l = np.concatenate(np.array(train_set[train_set.columns[feature_size:]]))\n",
    "\n",
    "    return (test_f, test_l, train_f, train_l)\n",
    "\n",
    "test_features, test_labels, train_features, train_labels = separate_sets(all_data, .2)\n",
    "\n",
    "train_labels\n",
    "test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "Now that we have theoretcially written everything needed to properly prep the data, we need to choose a model to implement. I think for now I will attempt to start off with basic logistic regression and increase complexity as needed.\n",
    "\n",
    "This means we will need a: \n",
    "* predictor\n",
    "* loss function\n",
    "* derivative of loss function\n",
    "* gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.888487476333953e-19"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_prediction(weights, features):\n",
    "    return 1 / (1 + np.exp(-1 * np.dot(weights[1:], features) - weights[0]))\n",
    "\n",
    "make_prediction([-.5] * 10, train_features[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Functions\n",
    "\n",
    "I didn't actually remember the math behind this so I looked up a good logistic regression algorithm loss and gradient of the loss function. That way I can guarantee, or at least better guarantee, that if something is wrong it's not this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.453581840409075"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think a simple 1 or 0 single loss function might be the best for now\n",
    "# we can come back and change it later if need be\n",
    "\n",
    "# I got this from a youtube video. Math is hard. \n",
    "def single_loss_log(weights, features, label):\n",
    "    y_hat = make_prediction(weights, features)\n",
    "    return label * np.log(y_hat) + (1 - label) * np.log(1 - y_hat)\n",
    "\n",
    "def batch_loss(batch_start_ind, batch_size, loss_func, weights, train_f, train_l):\n",
    "    total_loss = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        total_loss += loss_func(weights, train_f[(point_ind + batch_start_ind) % len(train_f)], train_l[(point_ind + batch_start_ind) % len(train_l)])\n",
    "    return total_loss / batch_size\n",
    "\n",
    "single_loss_log([-.5] * 10, train_features[0], train_labels[0])\n",
    "batch_loss(0, len(train_labels), single_loss_log, [-.5] * 10, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48223573, -4.72103596, -7.09426618, -8.93038138, -5.57821833,\n",
       "       -4.16838405, -5.62634486,  2.76440924, -4.47915726, -2.59154918])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gradient(batch_start_ind, batch_size, weights, train_f, train_l):\n",
    "    total_diff_theta = np.array([0.0] * (len(weights) - 1))\n",
    "    total_diff_b = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        y_diff = make_prediction(weights, train_f[(point_ind + batch_start_ind) % len(train_f)]) - train_l[(point_ind + batch_start_ind) % len(train_l)]\n",
    "        total_diff_theta +=  y_diff * train_f[(point_ind + batch_start_ind) % len(train_f)]\n",
    "        total_diff_b += y_diff\n",
    "    total_diff_theta = np.insert(total_diff_theta, 0, total_diff_b)\n",
    "\n",
    "    total_diff_theta /= batch_size\n",
    "\n",
    "    return total_diff_theta\n",
    "\n",
    "grad = batch_gradient(0, len(train_labels), [-.5] * 10, train_features, train_labels)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06105998  0.59777213  0.89826556  1.13075176  0.706307    0.52782035\n",
      "  0.71239952 -0.39221628  0.56714288  0.32814883]\n"
     ]
    }
   ],
   "source": [
    "def batch_descent(train_l, train_f, batch_size, step_size, threshold, weights=np.array([0] * 10)):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.20f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights\n",
    "\n",
    "trained_model = batch_descent(train_labels, train_features, len(train_labels), .2, .00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47955294979846097"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_error(test_f, test_l, weights):\n",
    "    total = 0\n",
    "    test_total = 0\n",
    "    for point_ind in range(len(test_l)):\n",
    "        expectation = make_prediction(weights, test_f[point_ind])\n",
    "        \n",
    "        test_total += test_l[point_ind]\n",
    "\n",
    "        if (expectation - .5) * ( test_l[point_ind] - .5) < 0:\n",
    "            total += 1\n",
    "    \n",
    "    return total / len(test_l)\n",
    "\n",
    "calc_error(test_features, test_labels, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.30161614e-02 -2.73764365e-04 -2.24194901e-02 -1.22462760e-02\n",
      "  1.99920009e-04 -9.28599011e-05 -1.95010005e-02 -6.00109929e-05\n",
      " -8.36454414e-03 -5.49276944e-03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097,\n",
       " 0.47955294979846097]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to be able to make a graph of the error over epochs\n",
    "# so I am going to do that here\n",
    "def batch_descent_error_tracking(train_l, train_f, test_f, test_l, batch_size, step_size, threshold, weights=np.array([0] * 10)):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "    errors = list()\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "            errors.append(calc_error(test_f, test_l, weights))\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.8f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights, errors\n",
    "\n",
    "trained_model, errors = batch_descent_error_tracking(train_labels, train_features, test_features, test_labels, len(train_labels), 2, .00001)\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbQklEQVR4nO3de7hmdV338feHGc4eUBgPDEeVMrBS22JaEuYhJBUoTNBKtEK7BLEeS9Iuw5407cknLDGjRDwg4wGdgEhQH0HygAwHlYPUNIIzYDJoiigJM3yfP9bauLjnt2fvGfY9e++Z9+u67mvW4bd+67t/e8/9udda973uVBWSJI3abq4LkCTNTwaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAgtSEnuSPKoua5ja5dkn36sF811LdryDAjNuv4JZfJxT5I7B/Mv3oz+Lk7yu8NlVfWAqlo1e1ULIMmNSZ45OV9V3+jHev1c1qW5sXiuC9DWp6oeMDmd5Ebgd6vqU3NX0XgkWVxV66Zbtql9bAlJAqSq7tnS+9bC4RGEtpgk2yU5Ocl/Jvl2kg8neWi/bqckH+iXfzfJ5UkenuRNwNOAd/RHIO/o21eSx/TTZyY5Lcm/JPl+ksuSPHqw32cnuSHJ95K8M8klo0ckM6xxv36/v5PkG8D/S3Jcks8l+Zsk3wFOSfLgJO9LsjbJTUn+NMl2fR8btG/UsGOSU5Pc0j9OTbJjv+76JM8dtF2c5LYkT+znfz7J5/sx/HKSQwdtL07ypiSfA34IPGpkv+8H9gHO68f6jwc/8+JBH3/R7+OOJOcl2T3JWUlu739v+w36fGySTyb5Tv87+I0Z/KlovqgqHz7G9gBuBJ7ZT78a+CKwF7Aj8A/A2f26lwPnAbsAi4CfAx7Ur7uY7ihk2G8Bj+mnzwS+AxxMd1R8FrCsX7cHcDvwa/26k4C7R/sb9LuxGvfr9/s+YFdgZ+A4YB1wYt//zv36fwYe2G/z78Dv9H1s0L5Rw5/3NTwMWAJ8Hvjf/bo3AGcN2v4q8LV+einwbeBwuhd/z+rnlwzG8RvAQf2+t9/Y72vkZ1486GMl8GjgwcB1/c/3zL7P9wHv6dvuCqwGXtqveyJwG3DQXP9d+pjh/9+5LsDH1v3gvgFxPfCMwbpH9k/Wi4GX9U+EP9Po4+LRJ3Q2DIh/Gqw7fPCk+dvAFwbr0j9pTRUQG6tx8snyUYP1xwHfGMwvAn4EHDhY9nLg4lb7KWr4T+DwwfyvADf2048Bvg/s0s+fBbyhn34t8P6Rvi4EXjIYxz+f6e+rn28FxOsH698G/Otg/nnA1f30C4FLR/r/B+DP5vrv0sfMHl6D0Ja0L/DxJMPz3uuBhwPvB/YGliXZDfgA3RPR3TPs+78G0z8EJq+D7EkXCABUVSVZs5k1Tlp9303uM78HsANw02DZTXSv7qfaftSeje33BKiqlUmuB56X5Dzg+cATBrW/IMnzBttuD3xmE/Y9E98aTN/ZmJ8c+32BJyf57mD9YrrftRYAA0Jb0mrgZVX1uSnWvxF4Y38O+wLgBuDddK9gN9c36U4XAfdenN1r6uZT1zg4tz5az3D+Nrojjn3pTr9Ad17/5inat9zSb3/tYPtbBuvPBo6lO410XVWtHNT+/qr6vY30Pd2+Z/P2zquBS6rqWbPYp7YgL1JrS3oX8KYk+wIkWZLkiH766Ul+Ot377W+ne5KdfGvltxi5oLoJ/gX46SRH9hdaXwk8YnNqnInq3g764b6PB/b9/CHdEdFMnQ38ab/vPeiuOwy3XwY8G/h94IOD5R+gO7L4lSSL+gv/hybZWCCOuj9jPep84CeS/FaS7fvHk5L81Cz1rzEzILQlvR04F7goyffpLsQ+uV/3COCjdOFwPXAJP35SfDtwdJL/TvK3m7LDqroNeAHwV3QXbA8EVtBdJ9jUGmfqROAHwCrg3+iexM/YhO3/oq/xK8BXgSv7ZQBU1TeBLwBPBT40WL4aOAJ4HbCW7hX8H7Fp/8//ki6cvpvkNZuw3Qaq6vt0QXYM3RHQfwFvpbv4rwUg/YUjaZvQv910DfDiqvrMdO2lbZlHENrq9adcdus/S/A6uncyfXGOy5LmPQNC24Kn0L119Da6t2EeWVV3zm1J0vznKSZJUpNHEJKkpq3qcxB77LFH7bfffnNdhiQtGFdcccVtVbWktW6rCoj99tuPFStWzHUZkrRgJLlpqnWeYpIkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqGmtAJDksyQ1JViY5ubH+0CTfS3J1/3jDTLeVJI3X4nF1nGQRcBrwLGANcHmSc6vqupGml1bVczdzW0nSmIzzCOJgYGVVraqqu4BlwBFbYFtJ0iwYZ0AsBVYP5tf0y0Y9JcmXk/xrkoM2cVuSHJ9kRZIVa9eunY26JUmMNyDSWFYj81cC+1bVzwJ/ByzfhG27hVWnV9VEVU0sWbJkc2uVJI0YZ0CsAfYezO8F3DJsUFW3V9Ud/fQFwPZJ9pjJtpKk8RpnQFwOHJBk/yQ7AMcA5w4bJHlEkvTTB/f1fHsm20qSxmts72KqqnVJTgAuBBYBZ1TVtUle0a9/F3A08PtJ1gF3AsdUVQHNbcdVqyRpQ+mej7cOExMTtWLFirkuQ5IWjCRXVNVEa52fpJYkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpaawBkeSwJDckWZnk5I20e1KS9UmOHiw7Kck1Sa5N8upx1ilJ2tDYAiLJIuA04DnAgcCxSQ6cot1bgQsHyx4H/B5wMPCzwHOTHDCuWiVJGxrnEcTBwMqqWlVVdwHLgCMa7U4EzgFuHSz7KeCLVfXDqloHXAIcNcZaJUkjxhkQS4HVg/k1/bJ7JVlK98T/rpFtrwEOSbJ7kl2Aw4G9WztJcnySFUlWrF27dtaKl6Rt3TgDIo1lNTJ/KvDaqlp/n0ZV19Oddvok8Angy8C61k6q6vSqmqiqiSVLltzvoiVJncVj7HsN933Vvxdwy0ibCWBZEoA9gMOTrKuq5VX1buDdAEne3PcnSdpCxhkQlwMHJNkfuBk4BnjRsEFV7T85neRM4PyqWt7PP6yqbk2yD/BrwFPGWKskacTYAqKq1iU5ge7dSYuAM6rq2iSv6NePXncYdU6S3YG7gVdW1X+Pq1ZJ0obGeQRBVV0AXDCyrBkMVXXcyPzTxleZJGk6fpJaktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1DRtQCTZLslTt0QxkqT5Y9qAqKp7gLdtgVokSfPITE8xXZTk15NkrNVIkuaNxTNs94fArsD6JHcCAaqqHjS2yiRJc2pGAVFVDxx3IZKk+WWmRxAkeT5wSD97cVWdP56SJEnzwYyuQSR5C3AScF3/OKlfJknaSs30COJw4PH9O5pI8l7gKuDkcRUmSZpbm/JBud0G0w+e5TokSfPMTI8g3gxcleQzdO9gOgT4k7FVtYW98bxrue6W2+e6DEnaLAfu+SD+7HkHzXq/0wZEku2Ae4CfB55EFxCvrar/mvVqJEnzxrQBUVX3JDmhqj4MnLsFatrixpG8krTQzfQaxCeTvCbJ3kkeOvkYa2WSpDk102sQL+v/feVgWQGPmt1yJEnzxUyvQZxcVR/aAvVIkuaJmd7N9ZXTtWtJcliSG5KsTDLlZyaSPCnJ+iRHD5b9QZJrk1yT5OwkO21ODZKkzTO2axBJFgGnAc8BDgSOTXLgFO3eClw4WLYUeBUwUVWPAxYBx8ywVknSLBjnNYiDgZVVtQogyTLgCLpbdQydCJxD9xba0dp2TnI3sAtwywxrlSTNgpnezXX/zeh7KbB6ML8GePKwQX+kcBTwywwCoqpuTvLXwDeAO4GLquqi1k6SHA8cD7DPPvtsRpmSpJaNnmJK8seD6ReMrHvzNH23vlyoRuZPpfvQ3fqRvh9Cd7SxP7AnsGuS32ztpKpOr6qJqppYsmTJNCVJkmZqumsQw/P+o7fWOGyabdcAew/m92LD00QTwLIkNwJHA+9MciTwTODrVbW2qu4GPgb4vdiStAVNd4opU0y35kddDhyQZH/gZrqwedGwwfDUVZIzgfOranmSJwM/n2QXulNMzwBWTLM/SdIsmi4gaorp1vx9V1atS3IC3buTFgFnVNW1SV7Rr3/XRra9LMlHgSuBdXS3Fj99mlolSbMoVVM/zydZD/yA7mhhZ+CHk6uAnapq+7FXuAkmJiZqxQoPNCRpppJcUVUTrXUbPYKoqkXjKUmSNN9tyhcGSZK2IQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUtNYAyLJYUluSLIyyckbafekJOuTHN3P/2SSqweP25O8epy1SpLua/G4Ok6yCDgNeBawBrg8yblVdV2j3VuBCyeXVdUNwOMH628GPj6uWiVJGxrnEcTBwMqqWlVVdwHLgCMa7U4EzgFunaKfZwD/WVU3jadMSVLLOANiKbB6ML+mX3avJEuBo4B3baSfY4Czp1qZ5PgkK5KsWLt27f0oV5I0NM6ASGNZjcyfCry2qtY3O0h2AJ4PfGSqnVTV6VU1UVUTS5Ys2dxaJUkjxnYNgu6IYe/B/F7ALSNtJoBlSQD2AA5Psq6qlvfrnwNcWVXfGmOdkqSGcQbE5cABSfanu8h8DPCiYYOq2n9yOsmZwPmDcAA4lo2cXpIkjc/YAqKq1iU5ge7dSYuAM6rq2iSv6Ndv7LoDSXahewfUy8dVoyRpauM8gqCqLgAuGFnWDIaqOm5k/ofA7mMrTpK0UX6SWpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqSmsQZEksOS3JBkZZKTN9LuSUnWJzl6sGy3JB9N8rUk1yd5yjhrlSTd19gCIski4DTgOcCBwLFJDpyi3VuBC0dWvR34RFU9FvhZ4Ppx1SpJ2tA4jyAOBlZW1aqqugtYBhzRaHcicA5w6+SCJA8CDgHeDVBVd1XVd8dYqyRpxDgDYimwejC/pl92ryRLgaOAd41s+yhgLfCeJFcl+acku7Z2kuT4JCuSrFi7du3sVS9J27hxBkQay2pk/lTgtVW1fmT5YuCJwN9X1ROAHwDNaxhVdXpVTVTVxJIlS+5nyZKkSYvH2PcaYO/B/F7ALSNtJoBlSQD2AA5Psg74IrCmqi7r232UKQJCkjQe4wyIy4EDkuwP3AwcA7xo2KCq9p+cTnImcH5VLe/nVyf5yaq6AXgGcN0Ya5UkjRhbQFTVuiQn0L07aRFwRlVdm+QV/frR6w6jTgTOSrIDsAp46bhqlSRtKFWjlwUWromJiVqxYsVclyFJC0aSK6pqorXOT1JLkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpaau6m2uStcBNm7n5HsBts1jOlrSQa4eFXf9Crh2sfy7Nl9r3rarm13FuVQFxfyRZMdUtb+e7hVw7LOz6F3LtYP1zaSHU7ikmSVKTASFJajIgfuz0uS7gfljItcPCrn8h1w7WP5fmfe1eg5AkNXkEIUlqMiAkSU3bfEAkOSzJDUlWJjl5ruvZVEluTPLVJFcnWTHX9UwnyRlJbk1yzWDZQ5N8Msl/9P8+ZC5rnMoUtZ+S5OZ+/K9Ocvhc1jiVJHsn+UyS65Ncm+SkfvlCGfup6p/3459kpyRfSvLlvvY39svn/dhv09cgkiwC/h14FrAGuBw4tqqum9PCNkGSG4GJqpoPH7iZVpJDgDuA91XV4/plfwV8p6re0of0Q6rqtXNZZ8sUtZ8C3FFVfz2XtU0nySOBR1bVlUkeCFwBHAkcx8IY+6nq/w3m+fgnCbBrVd2RZHvg34CTgF9jno/9tn4EcTCwsqpWVdVdwDLgiDmuaatWVZ8FvjOy+Ajgvf30e+n+4887U9S+IFTVN6vqyn76+8D1wFIWzthPVf+8V507+tnt+0exAMZ+Ww+IpcDqwfwaFsgf3UABFyW5Isnxc13MZnp4VX0TuicC4GFzXM+mOiHJV/pTUPPuNMGoJPsBTwAuYwGO/Uj9sADGP8miJFcDtwKfrKoFMfbbekCksWyhnXP7hap6IvAc4JX9aRBtOX8PPBp4PPBN4G1zWs00kjwAOAd4dVXdPtf1bKpG/Qti/KtqfVU9HtgLODjJ4+a4pBnZ1gNiDbD3YH4v4JY5qmWzVNUt/b+3Ah+nO2220HyrP8c8ea751jmuZ8aq6lv9f/57gH9kHo9/f/77HOCsqvpYv3jBjH2r/oU0/gBV9V3gYuAwFsDYb+sBcTlwQJL9k+wAHAOcO8c1zViSXfsLdiTZFXg2cM3Gt5qXzgVe0k+/BPjnOaxlk0z+B+8dxTwd//5C6buB66vq/w5WLYixn6r+hTD+SZYk2a2f3hl4JvA1FsDYb9PvYgLo3xZ3KrAIOKOq3jS3Fc1ckkfRHTUALAY+ON/rT3I2cCjdrY6/BfwZsBz4MLAP8A3gBVU17y4GT1H7oXSnNwq4EXj55Hnl+STJLwKXAl8F7ukXv47uPP5CGPup6j+WeT7+SX6G7iL0IroX5R+uqj9PsjvzfOy3+YCQJLVt66eYJElTMCAkSU0GhCSpyYCQJDUZEJKkJgNC95Fk/eDOmFfP5h1uk+w3vBPqLPZ7dn+rhT8YWX5kkgM3o7/nT/dzJ9kzyUc3te+N9HdkkjfMsO0pSV6zif3fMX2rKbc9Lsk7pmlzaJKnbu4+ZljH6wbTOyT5bJLF49znts7B1ag7+1sCLAhJHgE8tar2baw+Ejgf2ODuvEkWV9W6Vp9VdS7TfGCy/wT70Ztc8NT+GHj+LPa3pR1Kd6fbz49xH68D3gxQVXcl+TTwQuCsMe5zm+YRhGYk3fdOvLW/r/2XkjymX75vkk/3r+A/nWSffvnDk3y8vwf+lwevLhcl+cf+vvgX9Z8sJcmrklzX97Ossf+dkrwn3XdfXJXk6f2qi4CH9Uc7Txu0fyrdE+7/6dc9OsnFSd6c5BLgpCTPS3JZ39+nkjy83/beV8xJzkzyt0k+n2RVkqP75fceDfXtP5bkE+nu7f9Xgzp+J8m/9/v+x9Yr8SQ/Afyoqm5Ld1O3VensluSe9PfXSnLp5LgDB/Z9rkryqkFff5jkmv7x6il+l3+U5PJ+rN84RZuX9nVfAvzCYPkGY5bu5nmvAP5g8vcw1diO7OOg/m/p6r6WA/rlvzlY/g/9mLwF2LlfNhkIy4EXt+rXLKkqHz7ufQDrgasHjxf2y28EXt9P/zZwfj99HvCSfvplwPJ++kN0N1SD7hOkDwb2A9YBj++Xfxj4zX76FmDHfnq3Rl3/C3hPP/1Yuk+e7tT3ec0UP8uZwNGD+YuBdw7mH8KPPyz6u8Db+unjgHcM+vgI3YupA+luD89wv337Vf3PuBNwE909vvbsx+2hdLd4vnSy35E6Xzq5737+E8BBwHPpbgfzemBH4Ov9+lPoXqnvSPep7m/3/f8c3SeNdwUeAFwLPKHf5o7+32cDp9PdqHI7uiOsQ0bqeWQ/vkuAHYDPDcZjqjE7BXjNdGM7sp+/A17cT+8A7Az8FN3f1Pb98ncCvz38GQbbLwLWzvX/ma354SkmjdrYKaazB//+TT/9FLovPgF4PzD56vmX6YKEqloPfC/drZi/XlVX922uoHuiBfgKcFaS5XSvDEf9It0TClX1tSQ3AT8BbOodST80mN4L+FC6+/nsAHx9im2WV3czuOtar4R7n66q7wEkuQ7Yl+7J+5Lqb5+Q5CN9zaMeCawdzF8KHALsD/wl8HvAJXRhMelfqupHwI+S3Ao8nG6MPl5VP+j39zHgacBVg+2e3T8mlz0AOAD47KDNk4GLq2pt38+HBnXPdMxm0u4LwOuT7AV8rKr+I8kz6ILu8iTQhUbzJnZVtT7JXUkeWN13RGiWeYpJm6KmmJ6qTcuPBtPr+fF1sF8FTqN7crgiG158bN2afXP8YDD9d3SvjH8aeDndq/+WYc1T1dH6uWZa850j+76U7on9YOACYDe6c/zDJ/HN3V+Av6yqx/ePx1TVuxvtpvo9znTMpm1XVR+kOw14J3Bhkl/u63vvoL6frKpTNvLz7Aj8z0bW634wILQpXjj49wv99Ofp7oIL3fngf+unPw38Ptz7ZSkPmqrTJNsBe1fVZ+gu1u5G98p26LN9/5Pn7PcBbpim3u8DD9zI+gcDN/fTL9lIu831JeCXkjykD7xfn6Ld9cBjBvOXAU8F7qmq/6E71fdyuuDYmM8CRybZJd3dfY9qbHMh8LJ036tAkqVJRr+o5jLg0CS7p7vF9gsG66Yas9GxnnZs091sclVV/S3dmwJ+hu7v5ujJmtJ9b/PkGxDu7uuZ3H53ulNMd7f61/1nQGjU5IXAycdbBut2THIZ3ffpTr6l9FXAS5N8Bfitfh39v09P8lW6U0kHbWSfi4AP9G2vAv6muvvmD72T7gL3V+lOEx3Xn2LZmGXAH/UXSh/dWH8K8JEklwKz/p3eVXUz3btuLgM+Rfduqu81mn4WeEL6cyr9z7Ua+GK//lK6J9+vTrO/K+mumXyp3+c/VdVVI20uAj4IfKEfy48yEqLV3Q31FLoXAZ8CrhysPoX2mJ0HHJUfv1lgqnZDLwSuSfdNa4+l+67v64A/pfuWxK8An6Q7BQfdtZOvDC5SP53uCEtj4t1cNSNJbgQmqmrWn0i3ZkkeUN2X1S+muzX7GVX18Ua7twPnVdWntniRC1R/jeVPqmq6I0ltJo8gpPE6pX+FfA3dhdrlU7R7M7DLFqppwUv3BV/LDYfx8ghCktTkEYQkqcmAkCQ1GRCSpCYDQpLUZEBIkpr+P1TsUaIACQMtAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we need to make a graph of this\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(errors)), errors)\n",
    "plt.xlabel('Epochs of training (whole data set)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Testing error over time')\n",
    "plt.savefig('model1perf.jpg')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Analysis\n",
    "\n",
    "I am going to build a 9-10-10-1 fully connected network with a basic sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's make the activation function for simplicity sake\n",
    "def activate(x):\n",
    "    return 1 / (1 + np.exp(-1 * x)) \n",
    "\n",
    "def active_grad(x):\n",
    "    return np.exp(-1 * x) / (1 + np.exp(-1 * x)) ** 2\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49420331 0.58369711 0.55093786 ... 0.46858582 0.4899978  0.47202463]\n",
      " [0.46292652 0.37304276 0.62724563 ... 0.47236457 0.5239749  0.45200056]\n",
      " [0.45098687 0.41837307 0.60661045 ... 0.40238183 0.46157213 0.42498836]\n",
      " ...\n",
      " [0.47999593 0.39616514 0.48014235 ... 0.3357062  0.44270399 0.51367435]\n",
      " [0.51059909 0.4223003  0.43497771 ... 0.61016178 0.52216708 0.64601989]\n",
      " [0.45259873 0.44015791 0.50073155 ... 0.45841361 0.466105   0.46836553]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# we need to prepare our data to be run through the network in the correct shape\n",
    "nt_labels = train_labels.reshape(len(train_labels), 1)\n",
    "nt_features = train_features\n",
    "print(nt_features)\n",
    "print(nt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll also select a starting seed\n",
    "# np.random.seed(2022)\n",
    "\n",
    "# now we should initialize our neurons in our layers\n",
    "layer1 = 2 * np.random.random((9, 10)) - 1 \n",
    "layer2 = 2 * np.random.random((10, 10)) - 1\n",
    "layer3 = 2 * np.random.random((10, 1)) - 1\n",
    "\n",
    "# I will write a function that trains for a set number of epochs\n",
    "def train_epochs(num_epochs, alpha, layer1, layer2, layer3, features, labels, printfreq=10):\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        l0 = features\n",
    "        l1 = activate(np.dot(l0, layer1))\n",
    "        l2 = activate(np.dot(l1, layer2))\n",
    "        l3 = activate(np.dot(l2, layer3))\n",
    "\n",
    "        if e % printfreq == 0:\n",
    "            l3_error = labels - l3 \n",
    "\n",
    "            # we should print our current training error every once in awhile\n",
    "            print(\"Epoch {:d} Error:{:2.4f}%\".format(e, np.mean(np.abs(l3_error)) * 100))\n",
    "\n",
    "        # let's go through and backpropagate\n",
    "\n",
    "        l3_grad = l3_error * active_grad(l3)\n",
    "        \n",
    "        l2_error = l3_grad.dot(layer3.T)\n",
    "\n",
    "        l2_grad = l2_error * active_grad(l2)\n",
    "\n",
    "        l1_error = l2_grad.dot(layer2.T)\n",
    "\n",
    "        l1_grad = l1_error * active_grad(l1)\n",
    "\n",
    "        # now we want to update our weights\n",
    "        layer3 = layer3 + alpha * l2.T.dot(l3_grad)\n",
    "        layer2 = layer2 + alpha * l1.T.dot(l2_grad)\n",
    "        layer1 = layer1 + alpha * l0.T.dot(l1_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Error:50.5623%\n",
      "Epoch 10 Error:48.2421%\n",
      "Epoch 20 Error:51.7579%\n",
      "Epoch 30 Error:48.2423%\n",
      "Epoch 40 Error:51.7577%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/629854626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/3374923772.py\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(num_epochs, alpha, layer1, layer2, layer3, features, labels, printfreq)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# let's go through and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0ml3_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactive_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0ml2_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/902923763.py\u001b[0m in \u001b[0;36mactive_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mactive_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_epochs(8000, .0001, layer1, layer2, layer3, nt_features, nt_labels, printfreq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 48.9585%\n"
     ]
    }
   ],
   "source": [
    "# let's check our test error\n",
    "ntest_labels = test_labels.reshape(len(test_labels), 1)\n",
    "ntest_features = test_features\n",
    "\n",
    "l0 = ntest_features\n",
    "l1 = activate(np.dot(l0, layer1))\n",
    "l2 = activate(np.dot(l1, layer2))\n",
    "l3 = activate(np.dot(l2, layer3))\n",
    "\n",
    "errors = ntest_labels - l3\n",
    "total_error = np.mean(np.abs(errors))\n",
    "print('Test Error: {:2.4f}%'.format(total_error * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bcc2774040953d6dcddaaa4853b610bacc7c292a2bd037a42b0be9e59900d4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
