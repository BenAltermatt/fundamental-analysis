{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "I have successfully been able to build a sort of data set for this model. Now I want to try training on my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cupy-cuda11x in c:\\python38\\lib\\site-packages (11.2.0)\n",
      "Requirement already satisfied: numpy<1.26,>=1.20 in c:\\python38\\lib\\site-packages (from cupy-cuda11x) (1.23.3)\n",
      "Requirement already satisfied: fastrlock>=0.5 in c:\\python38\\lib\\site-packages (from cupy-cuda11x) (0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp38-cp38-win_amd64.whl (167.3 MB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.13.0-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python38\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\python38\\lib\\site-packages (from torchvision) (1.23.3)\n",
      "Requirement already satisfied: requests in c:\\python38\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python38\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python38\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests->torchvision) (2022.9.24)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: typing-extensions, torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.13.0 torchaudio-0.13.0 torchvision-0.14.0 typing-extensions-4.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install cupy-cuda11x\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037304</td>\n",
       "      <td>0.081930</td>\n",
       "      <td>68.692936</td>\n",
       "      <td>16.198334</td>\n",
       "      <td>3.792215</td>\n",
       "      <td>1.051065</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.366850</td>\n",
       "      <td>0.337912</td>\n",
       "      <td>8.135137</td>\n",
       "      <td>18.134324</td>\n",
       "      <td>3.515821</td>\n",
       "      <td>0.918390</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.108044</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.132520</td>\n",
       "      <td>0.173255</td>\n",
       "      <td>18.516105</td>\n",
       "      <td>6.586352</td>\n",
       "      <td>1.420348</td>\n",
       "      <td>0.761143</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.236057</td>\n",
       "      <td>0.019177</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071330</td>\n",
       "      <td>0.172091</td>\n",
       "      <td>29.580969</td>\n",
       "      <td>5.532793</td>\n",
       "      <td>1.205178</td>\n",
       "      <td>0.746057</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.436893</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.133795</td>\n",
       "      <td>0.211265</td>\n",
       "      <td>12.743329</td>\n",
       "      <td>4.438462</td>\n",
       "      <td>0.957572</td>\n",
       "      <td>0.715781</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.018786</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.265324</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>3.700391</td>\n",
       "      <td>0.383926</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.948329</td>\n",
       "      <td>4.281250</td>\n",
       "      <td>3.332726</td>\n",
       "      <td>0.322385</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>4.395096</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>0.792549</td>\n",
       "      <td>1.878183</td>\n",
       "      <td>2.160751</td>\n",
       "      <td>2.286619</td>\n",
       "      <td>0.339581</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.039290</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.440450</td>\n",
       "      <td>1.614583</td>\n",
       "      <td>2.256612</td>\n",
       "      <td>0.362996</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.056206</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.242760</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.904923</td>\n",
       "      <td>0.341091</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.257109</td>\n",
       "      <td>2.313978</td>\n",
       "      <td>0.037899</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                0.037304              0.081930  68.692936          16.198334   \n",
       "1                0.366850              0.337912   8.135137          18.134324   \n",
       "2                0.132520              0.173255  18.516105           6.586352   \n",
       "3                0.071330              0.172091  29.580969           5.532793   \n",
       "4                0.133795              0.211265  12.743329           4.438462   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           0.600000              1.265324   3.083333           3.700391   \n",
       "136444           0.400000              0.948329   4.281250           3.332726   \n",
       "136445           0.792549              1.878183   2.160751           2.286619   \n",
       "136446           1.200000              1.440450   1.614583           2.256612   \n",
       "136447           0.800000              1.242760   2.250000           2.904923   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       3.792215      1.051065       0.000404     0.111111  0.013801   CMCSA   \n",
       "1       3.515821      0.918390       0.000015     0.000485  0.108044   CMCSA   \n",
       "2       1.420348      0.761143       0.003187     0.236057  0.019177   CMCSA   \n",
       "3       1.205178      0.746057       0.003692     0.436893  0.010185   CMCSA   \n",
       "4       0.957572      0.715781       0.004574     0.233161  0.018786   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  0.383926      0.001104       0.000011     0.000135  0.031129  SNGR.L   \n",
       "136444  0.322385      0.001067       0.256648     4.395096  0.018825  SNGR.L   \n",
       "136445  0.339581      0.001007       0.000037     0.000320  0.039290  SNGR.L   \n",
       "136446  0.362996      0.000928       0.000067     0.000433  0.056206  SNGR.L   \n",
       "136447  0.341091      0.000955       0.257109     2.313978  0.037899  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# first we need to read the data in\n",
    "\n",
    "data_name = 'dataset.csv'\n",
    "\n",
    "all_data = pd.read_csv(data_name)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a new strategy for normalization\n",
    "# we'll update each value to be the z-score of the value within the overall distribution\n",
    "def zscore_data(data, cols_to_normalize=None, feature_size=9):\n",
    "    cols_for_normalizing = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_normalize:\n",
    "        cols_for_normalizing = cols_to_normalize\n",
    "    \n",
    "    for col in cols_for_normalizing:\n",
    "        col_data = data[col]\n",
    "        mu = np.mean(col_data)\n",
    "        sigma = np.std(col_data)\n",
    "\n",
    "        data[col] = (data[col] - mu) / sigma\n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_data(data, cols_to_normalize=None, feature_size=9):\n",
    "    cols_for_norm = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_normalize:\n",
    "        cols_for_norm = cols_to_normalize\n",
    "    \n",
    "    for col in cols_for_norm:\n",
    "        col_data = data[col]\n",
    "        min_val = np.min(col_data)\n",
    "        max_val = np.max(col_data)\n",
    "\n",
    "        data[col] = (data[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return data\n",
    "\n",
    "all_data = normalize_data(zscore_data(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.789873</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518706</td>\n",
       "      <td>11.567448</td>\n",
       "      <td>8.644031</td>\n",
       "      <td>11.667213</td>\n",
       "      <td>-7.813165</td>\n",
       "      <td>9.288283</td>\n",
       "      <td>5.373945</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.789891</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567466</td>\n",
       "      <td>8.643982</td>\n",
       "      <td>11.667212</td>\n",
       "      <td>-11.112948</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374382</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567357</td>\n",
       "      <td>8.643613</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.748617</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373970</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.789875</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567347</td>\n",
       "      <td>8.643575</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.601492</td>\n",
       "      <td>9.288314</td>\n",
       "      <td>5.373929</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567337</td>\n",
       "      <td>8.643531</td>\n",
       "      <td>11.667210</td>\n",
       "      <td>-5.387330</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>9.789904</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567330</td>\n",
       "      <td>8.643430</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-11.419512</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374026</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>9.789893</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567326</td>\n",
       "      <td>8.643419</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.360050</td>\n",
       "      <td>9.288680</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-10.202989</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374064</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>9.789938</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643426</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-9.609178</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374142</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567322</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.358257</td>\n",
       "      <td>9.288487</td>\n",
       "      <td>5.374057</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                9.789873               14.7112  18.518706          11.567448   \n",
       "1                9.789891               14.7112  18.518705          11.567466   \n",
       "2                9.789878               14.7112  18.518705          11.567357   \n",
       "3                9.789875               14.7112  18.518705          11.567347   \n",
       "4                9.789878               14.7112  18.518705          11.567337   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           9.789904               14.7112  18.518705          11.567330   \n",
       "136444           9.789893               14.7112  18.518705          11.567326   \n",
       "136445           9.789915               14.7112  18.518705          11.567316   \n",
       "136446           9.789938               14.7112  18.518705          11.567316   \n",
       "136447           9.789915               14.7112  18.518705          11.567322   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       8.644031     11.667213      -7.813165     9.288283  5.373945   CMCSA   \n",
       "1       8.643982     11.667212     -11.112948     9.288273  5.374382   CMCSA   \n",
       "2       8.643613     11.667211      -5.748617     9.288295  5.373970   CMCSA   \n",
       "3       8.643575     11.667211      -5.601492     9.288314  5.373929   CMCSA   \n",
       "4       8.643531     11.667210      -5.387330     9.288295  5.373969   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  8.643430     11.667204     -11.419512     9.288273  5.374026  SNGR.L   \n",
       "136444  8.643419     11.667204      -1.360050     9.288680  5.373969  SNGR.L   \n",
       "136445  8.643422     11.667204     -10.202989     9.288273  5.374064  SNGR.L   \n",
       "136446  8.643426     11.667204      -9.609178     9.288273  5.374142  SNGR.L   \n",
       "136447  8.643422     11.667204      -1.358257     9.288487  5.374057  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We may need to consolidate larger values by taking the log of our data before\n",
    "# trying to pull out outliers\n",
    "\n",
    "def log_data(data, cols_to_log=None, feature_size=9):\n",
    "    cols_for_logging = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_log:\n",
    "        cols_for_logging = cols_to_log\n",
    "\n",
    "    for col in cols_for_logging:\n",
    "        # we have to offset all data by the absolute value of it if its negative\n",
    "        info = data[col].describe()\n",
    "        if info['min'] < 0:\n",
    "            data[col] = data[col] + np.abs(info['min']) + 1\n",
    "\n",
    "        data[col] = np.log(data[col])\n",
    "    \n",
    "    return data\n",
    "\n",
    "all_data = log_data(all_data)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length: 136448\n",
      "New length: 108152\n"
     ]
    }
   ],
   "source": [
    "# We need to attempt to handle outliers in our data.\n",
    "# this removes outliers based on the interquartile range\n",
    "def remove_outliers_iqr(data, iqr_mod=1.5, feature_size=9):\n",
    "    cols_for_trimming = data.columns[:feature_size]\n",
    "\n",
    "    # we need to go through each feature and check the inter quartile ranges.\n",
    "    # we'll drop values with info outside of the multiplier on the range we\n",
    "    # provided\n",
    "    print('Old length: {:d}'.format(len(data)))\n",
    "    for col in cols_for_trimming:\n",
    "        info = data[col].describe()\n",
    "        range_add = (info['75%'] - info['25%']) * iqr_mod\n",
    "        data = data[(data[col] >= info['25%'] - range_add)]\n",
    "        data = data[(data[col] <= info['75%'] + range_add)]\n",
    "    print('New length: {:d}'.format(len(data)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# it might be useful to try making one for the z-score, but I don't know\n",
    "# if our distribution is normalized.\n",
    "\n",
    "all_data = remove_outliers_iqr(all_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.790051250741316\n",
      "14.711201294824106\n",
      "18.518706486141376\n",
      "11.568015320151485\n",
      "8.645939524629462\n",
      "11.667243503526587\n",
      "3.6137942376991385\n",
      "9.288617587790053\n",
      "5.374716997680058\n"
     ]
    }
   ],
   "source": [
    "# there are a couple of preliminary steps we need to take. \n",
    "\n",
    "# 1) handle outliers\n",
    "# 2) normalize all of the data (0 to 1)\n",
    "# 3) separate testing and training sets\n",
    "\n",
    "\n",
    "def normalize_data(data, feature_size=9):\n",
    "    cols_for_normalization = data.columns[:feature_size]\n",
    "    \n",
    "    for col in cols_for_normalization:\n",
    "        max = data[col].max()\n",
    "        min = data[col].min()\n",
    "        print(max)\n",
    "\n",
    "        data[col] = (data[col] - min) / (max - min)\n",
    "\n",
    "normalize_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate training and test sets\n",
    "\n",
    "def separate_sets(data, prop_test, prop_train=0, feature_size=9):\n",
    "\n",
    "    # we need to drop superfluous info like tickers\n",
    "    data = data.drop(['ticker'], axis=1)\n",
    "\n",
    "    if prop_train == 0 or prop_train + prop_test > 1:\n",
    "        prop_train = 1 - prop_test\n",
    "    \n",
    "    # we find out the ratio to take from the remaining portion\n",
    "    prop_train = (1 - prop_test) / prop_train\n",
    "    if prop_train > 1:\n",
    "        prop_train = 1\n",
    "\n",
    "    test_set = data.sample(frac=prop_test)\n",
    "    data = data.drop(test_set.index)\n",
    "    train_set = data.sample(frac=prop_train)\n",
    "\n",
    "    test_f = np.array(test_set[test_set.columns[:feature_size]])\n",
    "    test_l = np.concatenate(np.array(test_set[test_set.columns[feature_size:]]))\n",
    "\n",
    "    train_f = np.array(train_set[train_set.columns[:feature_size]])\n",
    "    train_l = np.concatenate(np.array(train_set[train_set.columns[feature_size:]]))\n",
    "\n",
    "    return (test_f, test_l, train_f, train_l)\n",
    "\n",
    "test_features, test_labels, train_features, train_labels = separate_sets(all_data, .2)\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "Now that we have theoretcially written everything needed to properly prep the data, we need to choose a model to implement. I think for now I will attempt to start off with basic logistic regression and increase complexity as needed.\n",
    "\n",
    "This means we will need a: \n",
    "* predictor\n",
    "* loss function\n",
    "* derivative of loss function\n",
    "* gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050714426032425215"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_prediction(weights, features):\n",
    "    return 1 / (1 + np.exp(-np.dot(weights[1:], features) - weights[0]))\n",
    "\n",
    "make_prediction([-.5] * 10, train_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Functions\n",
    "\n",
    "I didn't actually remember the math behind this so I looked up a good logistic regression algorithm loss and gradient of the loss function. That way I can guarantee, or at least better guarantee, that if something is wrong it's not this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.405312891220998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think a simple 1 or 0 single loss function might be the best for now\n",
    "# we can come back and change it later if need be\n",
    "\n",
    "# I got this from a youtube video. Math is hard. \n",
    "def single_loss_log(weights, features, label):\n",
    "    y_hat = make_prediction(weights, features)\n",
    "    return label * np.log(y_hat) + (1 - label) * np.log(1 - y_hat)\n",
    "\n",
    "def batch_loss(batch_start_ind, batch_size, loss_func, weights, train_f, train_l):\n",
    "    total_loss = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        total_loss += loss_func(weights, train_f[(point_ind + batch_start_ind) % len(train_f)], train_l[(point_ind + batch_start_ind) % len(train_l)])\n",
    "    return total_loss / batch_size\n",
    "\n",
    "single_loss_log([-.5] * 10, train_features[0], train_labels[0])\n",
    "batch_loss(0, len(train_labels), single_loss_log, [-.5] * 10, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42299347, -0.21851647, -0.21540112, -0.21264499, -0.21566311,\n",
       "       -0.21964421, -0.21597139, -0.20694553, -0.2154822 , -0.21748527])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gradient(batch_start_ind, batch_size, weights, train_f, train_l):\n",
    "    total_diff_theta = np.array([0.0] * (len(weights) - 1))\n",
    "    total_diff_b = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        y_diff = make_prediction(weights, train_f[(point_ind + batch_start_ind) % len(train_f)]) - train_l[(point_ind + batch_start_ind) % len(train_l)]\n",
    "        total_diff_theta +=  y_diff * train_f[(point_ind + batch_start_ind) % len(train_f)]\n",
    "        total_diff_b += y_diff\n",
    "    total_diff_theta = np.insert(total_diff_theta, 0, total_diff_b)\n",
    "\n",
    "    total_diff_theta /= batch_size\n",
    "\n",
    "    return total_diff_theta\n",
    "\n",
    "grad = batch_gradient(0, len(train_labels), [-.5] * 10, train_features, train_labels)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05709536  0.02881672  0.01583017 -0.02107941 -0.0150407   0.04700797\n",
      " -0.02403506 -0.05216017 -0.05735908  0.06526121]\n"
     ]
    }
   ],
   "source": [
    "def batch_descent(train_l, train_f, batch_size, step_size, threshold, weights=np.array([0] * 10)):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.8f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights\n",
    "\n",
    "trained_model = batch_descent(train_labels, train_features, len(train_labels), 2, .00002723)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4782709200184928"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_error(test_f, test_l, weights, conf):\n",
    "    total = 0\n",
    "    test_total = 0\n",
    "    for point_ind in range(len(test_l)):\n",
    "        expectation = make_prediction(weights, test_f[point_ind])\n",
    "        \n",
    "        test_total += test_l[point_ind]\n",
    "\n",
    "        if (expectation - .5) * ( test_l[point_ind] - .5) < 0:\n",
    "            total += 1\n",
    "    \n",
    "    return total / len(test_l)\n",
    "\n",
    "calc_error(test_features, test_labels, trained_model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a method which caclulates error based on the ability to abstain for values below a given confidence interval\n",
    "def calc_abs_error(test_f, test_l, weights, interval):\n",
    "    total = 0\n",
    "    test_total = 0\n",
    "    for point_ind in range(len(test_l)):\n",
    "        expectation = make_prediction(weights, test_f[point_ind])\n",
    "        \n",
    "        test_total += test_l[point_ind]\n",
    "\n",
    "        if (expectation - .5) * ( test_l[point_ind] - .5) < 0:\n",
    "            # false positive with confidence below interval\n",
    "            if (expectation > .5):\n",
    "                if(expectation > interval):\n",
    "                    total+= 1\n",
    "            else:\n",
    "                total += 1\n",
    "    \n",
    "    return total / len(test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.88891981  0.4651364   0.24729517 -0.04405216 -0.09707551  0.97741274\n",
      " -0.03164045 -0.32803687 -0.43440791  0.85549133]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.47831715210355985,\n",
       " 0.4781784558483588,\n",
       " 0.4778548312528895,\n",
       " 0.4778085991678225,\n",
       " 0.4770688858067499,\n",
       " 0.4763291724456773,\n",
       " 0.4753120665742025,\n",
       " 0.4738326398520573,\n",
       " 0.4727230698104484,\n",
       " 0.47128987517337034,\n",
       " 0.4698566805362922,\n",
       " 0.46851595006934815,\n",
       " 0.46638927415626447,\n",
       " 0.46449375866851594,\n",
       " 0.46199722607489596,\n",
       " 0.46042533518261675,\n",
       " 0.4579750346740638,\n",
       " 0.455663430420712,\n",
       " 0.4534442903374942,\n",
       " 0.451918631530282,\n",
       " 0.4497457235321313,\n",
       " 0.4472029588534443,\n",
       " 0.4453074433656958,\n",
       " 0.44359685621821543,\n",
       " 0.4416088765603329,\n",
       " 0.43929727230698107,\n",
       " 0.4369856680536292,\n",
       " 0.43546000924641703,\n",
       " 0.4335644937586685,\n",
       " 0.43148404993065187,\n",
       " 0.4293573740175682,\n",
       " 0.42736939435968563,\n",
       " 0.4257050392972723,\n",
       " 0.4239019879796579,\n",
       " 0.42219140083217754,\n",
       " 0.420249653259362,\n",
       " 0.41853906611188163,\n",
       " 0.41692094313453537,\n",
       " 0.4153028201571891,\n",
       " 0.413962089690245,\n",
       " 0.41262135922330095,\n",
       " 0.41160425335182615,\n",
       " 0.41049468331021727,\n",
       " 0.40864539990753584,\n",
       " 0.40744336569579287,\n",
       " 0.40642625982431807,\n",
       " 0.40499306518723993,\n",
       " 0.40397595931576513,\n",
       " 0.4024040684234859,\n",
       " 0.40106333795654187,\n",
       " 0.400046232085067,\n",
       " 0.3991215903837263,\n",
       " 0.39796578825705037,\n",
       " 0.3965325936199723,\n",
       " 0.3953305594082293,\n",
       " 0.3943134535367545,\n",
       " 0.3930189551548775,\n",
       " 0.39172445677300044,\n",
       " 0.3906148867313916,\n",
       " 0.3896440129449838,\n",
       " 0.388626907073509,\n",
       " 0.38751733703190017,\n",
       " 0.3864077669902913,\n",
       " 0.38534442903374944,\n",
       " 0.38423485899214055,\n",
       " 0.383541377716135,\n",
       " 0.3825242718446602,\n",
       " 0.3816458622283865,\n",
       " 0.38081368469717986,\n",
       " 0.3801202034211743,\n",
       " 0.37919556171983354,\n",
       " 0.3782246879334258,\n",
       " 0.3770688858067499,\n",
       " 0.3761904761904762,\n",
       " 0.3752658344891355,\n",
       " 0.3743411927877947,\n",
       " 0.3733240869163199,\n",
       " 0.37281553398058254,\n",
       " 0.37226074895977807,\n",
       " 0.3714748035136385,\n",
       " 0.3703652334720296,\n",
       " 0.3697179842810911,\n",
       " 0.3689782709200185,\n",
       " 0.36833102172908,\n",
       " 0.36763754045307445,\n",
       " 0.3668053629218678,\n",
       " 0.36625057790106336,\n",
       " 0.3655570966250578,\n",
       " 0.36472491909385113,\n",
       " 0.3639389736477115,\n",
       " 0.36347665279704117,\n",
       " 0.3630605640314378,\n",
       " 0.36245954692556637,\n",
       " 0.361997226074896,\n",
       " 0.3613499768839575,\n",
       " 0.36093388811835414,\n",
       " 0.36047156726768376,\n",
       " 0.35991678224687934,\n",
       " 0.3590383726306056,\n",
       " 0.35852981969486825,\n",
       " 0.3581137309292649,\n",
       " 0.35732778548312527,\n",
       " 0.35649560795191865,\n",
       " 0.3561719833564494,\n",
       " 0.355709662505779,\n",
       " 0.35538603791030976,\n",
       " 0.35510864539990755,\n",
       " 0.35450762829403604,\n",
       " 0.3540453074433657,\n",
       " 0.3535367545076283,\n",
       " 0.35307443365695795,\n",
       " 0.35284327323162273,\n",
       " 0.35261211280628757,\n",
       " 0.35219602404068423,\n",
       " 0.35182616736014793,\n",
       " 0.3514100785945446,\n",
       " 0.3509015256588072,\n",
       " 0.3504854368932039,\n",
       " 0.35016181229773463,\n",
       " 0.3498381877022654,\n",
       " 0.349375866851595,\n",
       " 0.3492371705963939,\n",
       " 0.3490060101710587,\n",
       " 0.3488210818307906,\n",
       " 0.3483587609801202,\n",
       " 0.3481738326398521,\n",
       " 0.3478502080443828,\n",
       " 0.3476190476190476,\n",
       " 0.34720295885344427,\n",
       " 0.3466944059177069,\n",
       " 0.34609338881183543,\n",
       " 0.34581599630143317,\n",
       " 0.3454923717059639,\n",
       " 0.3450300508552936,\n",
       " 0.34479889042995837,\n",
       " 0.34461396208969025,\n",
       " 0.3444290337494221,\n",
       " 0.34396671289875175,\n",
       " 0.3436430883032825,\n",
       " 0.343042071197411,\n",
       " 0.3426722145168747,\n",
       " 0.3424410540915395,\n",
       " 0.34234858992140543,\n",
       " 0.34211742949607027,\n",
       " 0.34197873324086914,\n",
       " 0.34170134073046693,\n",
       " 0.34147018030513177,\n",
       " 0.34110032362459547,\n",
       " 0.34082293111419326,\n",
       " 0.34045307443365697,\n",
       " 0.3403606102635229,\n",
       " 0.3400369856680536,\n",
       " 0.33980582524271846,\n",
       " 0.33957466481738324,\n",
       " 0.3393435043920481,\n",
       " 0.33920480813684695,\n",
       " 0.3388811835413777,\n",
       " 0.3385113268608414,\n",
       " 0.33828016643550624,\n",
       " 0.3380952380952381,\n",
       " 0.33791030975496994,\n",
       " 0.33777161349976886,\n",
       " 0.33763291724456773,\n",
       " 0.3375866851595007,\n",
       " 0.3372168284789644,\n",
       " 0.3371243643088303,\n",
       " 0.3370781322237633,\n",
       " 0.33689320388349514,\n",
       " 0.336708275543227,\n",
       " 0.33643088303282476,\n",
       " 0.33615349052242255,\n",
       " 0.33596856218215443,\n",
       " 0.3357374017568192,\n",
       " 0.3356449375866852,\n",
       " 0.3356911696717522,\n",
       " 0.33559870550161813,\n",
       " 0.33541377716134996,\n",
       " 0.3353675450762829,\n",
       " 0.3353213129912159,\n",
       " 0.3352750809061489,\n",
       " 0.3351826167360148,\n",
       " 0.33522884882108184,\n",
       " 0.3349976883957467,\n",
       " 0.33495145631067963,\n",
       " 0.33495145631067963,\n",
       " 0.33485899214054554,\n",
       " 0.3347202958853444,\n",
       " 0.33458159963014333,\n",
       " 0.33448913546000925,\n",
       " 0.3342579750346741,\n",
       " 0.33411927877947295,\n",
       " 0.33402681460933886,\n",
       " 0.33384188626907074,\n",
       " 0.33365695792880257,\n",
       " 0.33365695792880257,\n",
       " 0.3336107258437356,\n",
       " 0.33347202958853445,\n",
       " 0.3333333333333333,\n",
       " 0.3331484049930652,\n",
       " 0.33310217290799815,\n",
       " 0.3330559408229311,\n",
       " 0.33300970873786406,\n",
       " 0.332963476652797,\n",
       " 0.33273231622746186,\n",
       " 0.33273231622746186,\n",
       " 0.33254738788719373,\n",
       " 0.3324086916319926,\n",
       " 0.33236245954692556,\n",
       " 0.3323162274618585,\n",
       " 0.33226999537679147,\n",
       " 0.33226999537679147,\n",
       " 0.3320388349514563,\n",
       " 0.33208506703652335,\n",
       " 0.3320388349514563,\n",
       " 0.33199260286638926,\n",
       " 0.3319463707813222,\n",
       " 0.3319001386962552,\n",
       " 0.3319001386962552,\n",
       " 0.33180767452612114,\n",
       " 0.33166897827092,\n",
       " 0.33162274618585297,\n",
       " 0.3315765141007859,\n",
       " 0.3315765141007859,\n",
       " 0.3315765141007859,\n",
       " 0.3315765141007859,\n",
       " 0.33153028201571894,\n",
       " 0.33134535367545076,\n",
       " 0.33134535367545076,\n",
       " 0.33125288950531667,\n",
       " 0.33116042533518264,\n",
       " 0.3311141932501156,\n",
       " 0.33106796116504855,\n",
       " 0.3311141932501156,\n",
       " 0.3311141932501156,\n",
       " 0.3309292649098474,\n",
       " 0.3309292649098474,\n",
       " 0.3308368007397134,\n",
       " 0.33069810448451226,\n",
       " 0.3306518723994452,\n",
       " 0.3306518723994452,\n",
       " 0.3305594082293111,\n",
       " 0.3305594082293111,\n",
       " 0.33042071197411005,\n",
       " 0.330374479889043,\n",
       " 0.330374479889043,\n",
       " 0.330374479889043,\n",
       " 0.33032824780397596,\n",
       " 0.3302820157189089,\n",
       " 0.3302357836338419,\n",
       " 0.33018955154877483,\n",
       " 0.33018955154877483,\n",
       " 0.3302357836338419,\n",
       " 0.33014331946370784,\n",
       " 0.32995839112343966,\n",
       " 0.3299121590383726,\n",
       " 0.3298659269533056,\n",
       " 0.3298659269533056,\n",
       " 0.3297272306981045,\n",
       " 0.32968099861303746,\n",
       " 0.3297272306981045,\n",
       " 0.32968099861303746,\n",
       " 0.3296347665279704,\n",
       " 0.3295423023578363,\n",
       " 0.32958853444290337,\n",
       " 0.32958853444290337,\n",
       " 0.3295423023578363,\n",
       " 0.3295423023578363,\n",
       " 0.3295423023578363,\n",
       " 0.32958853444290337,\n",
       " 0.3296347665279704,\n",
       " 0.3295423023578363,\n",
       " 0.32940360610263525,\n",
       " 0.3293573740175682,\n",
       " 0.3293573740175682,\n",
       " 0.32940360610263525,\n",
       " 0.3292186777623671,\n",
       " 0.3292186777623671,\n",
       " 0.3292649098474341,\n",
       " 0.329126213592233,\n",
       " 0.32903374942209895,\n",
       " 0.3288950531668978,\n",
       " 0.32894128525196487,\n",
       " 0.3288488210818308,\n",
       " 0.3288488210818308,\n",
       " 0.3288950531668978,\n",
       " 0.32894128525196487,\n",
       " 0.3289875173370319,\n",
       " 0.32894128525196487,\n",
       " 0.32894128525196487,\n",
       " 0.3288950531668978,\n",
       " 0.3287563569116967,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.3287101248266297,\n",
       " 0.3287563569116967,\n",
       " 0.3287101248266297,\n",
       " 0.32866389274156266,\n",
       " 0.32857142857142857,\n",
       " 0.3284789644012945,\n",
       " 0.3284789644012945,\n",
       " 0.32838650023116045,\n",
       " 0.32843273231622744,\n",
       " 0.3284789644012945,\n",
       " 0.32843273231622744,\n",
       " 0.32838650023116045,\n",
       " 0.32843273231622744,\n",
       " 0.32843273231622744,\n",
       " 0.3284789644012945,\n",
       " 0.3285251964863615,\n",
       " 0.3283402681460934,\n",
       " 0.32829403606102636,\n",
       " 0.3282015718908923,\n",
       " 0.32815533980582523,\n",
       " 0.32806287563569114,\n",
       " 0.32806287563569114,\n",
       " 0.32806287563569114,\n",
       " 0.32806287563569114,\n",
       " 0.32801664355062415,\n",
       " 0.3279704114655571,\n",
       " 0.3279704114655571,\n",
       " 0.3279704114655571,\n",
       " 0.32801664355062415,\n",
       " 0.3279704114655571,\n",
       " 0.32792417938049007,\n",
       " 0.327877947295423,\n",
       " 0.32792417938049007,\n",
       " 0.32792417938049007,\n",
       " 0.327877947295423,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.32778548312528893,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.32778548312528893,\n",
       " 0.32778548312528893,\n",
       " 0.3277392510402219,\n",
       " 0.3276930189551549,\n",
       " 0.3276930189551549,\n",
       " 0.32764678687008786,\n",
       " 0.32755432269995377,\n",
       " 0.32755432269995377,\n",
       " 0.3275080906148867,\n",
       " 0.3275080906148867,\n",
       " 0.3276005547850208,\n",
       " 0.3276930189551549,\n",
       " 0.32778548312528893,\n",
       " 0.327831715210356,\n",
       " 0.32778548312528893,\n",
       " 0.32778548312528893,\n",
       " 0.327877947295423,\n",
       " 0.32792417938049007,\n",
       " 0.327877947295423,\n",
       " 0.32792417938049007,\n",
       " 0.32792417938049007,\n",
       " 0.32792417938049007,\n",
       " 0.327831715210356,\n",
       " 0.32778548312528893,\n",
       " 0.3277392510402219,\n",
       " 0.3276930189551549,\n",
       " 0.3276930189551549,\n",
       " 0.3277392510402219,\n",
       " 0.32764678687008786,\n",
       " 0.3276005547850208,\n",
       " 0.32755432269995377,\n",
       " 0.3275080906148867,\n",
       " 0.3275080906148867,\n",
       " 0.3274618585298197,\n",
       " 0.3274618585298197,\n",
       " 0.3273693943596856,\n",
       " 0.3273231622746186,\n",
       " 0.3273231622746186,\n",
       " 0.3272306981044845,\n",
       " 0.3272306981044845,\n",
       " 0.3272306981044845,\n",
       " 0.3271844660194175,\n",
       " 0.3270920018492834,\n",
       " 0.3270920018492834,\n",
       " 0.32699953767914935,\n",
       " 0.32704576976421634,\n",
       " 0.32704576976421634,\n",
       " 0.32699953767914935,\n",
       " 0.32690707350901527,\n",
       " 0.3269533055940823,\n",
       " 0.3269533055940823,\n",
       " 0.3268608414239482,\n",
       " 0.32676837725381414,\n",
       " 0.3268146093388812,\n",
       " 0.3268146093388812,\n",
       " 0.3268146093388812,\n",
       " 0.3268608414239482,\n",
       " 0.3268608414239482,\n",
       " 0.3268608414239482,\n",
       " 0.3269533055940823,\n",
       " 0.32699953767914935,\n",
       " 0.32699953767914935,\n",
       " 0.32704576976421634,\n",
       " 0.32713823393435043,\n",
       " 0.3270920018492834,\n",
       " 0.3270920018492834,\n",
       " 0.32713823393435043,\n",
       " 0.3271844660194175,\n",
       " 0.32727693018955156,\n",
       " 0.3272306981044845,\n",
       " 0.3272306981044845,\n",
       " 0.3271844660194175,\n",
       " 0.3272306981044845,\n",
       " 0.3271844660194175,\n",
       " 0.3271844660194175,\n",
       " 0.3271844660194175,\n",
       " 0.3271844660194175,\n",
       " 0.3271844660194175,\n",
       " 0.32713823393435043,\n",
       " 0.3270920018492834,\n",
       " 0.3270920018492834,\n",
       " 0.32713823393435043,\n",
       " 0.3271844660194175,\n",
       " 0.32727693018955156,\n",
       " 0.3271844660194175,\n",
       " 0.3272306981044845,\n",
       " 0.3272306981044845,\n",
       " 0.3273231622746186,\n",
       " 0.3273693943596856,\n",
       " 0.32741562644475264,\n",
       " 0.3275080906148867,\n",
       " 0.3273693943596856,\n",
       " 0.3273231622746186,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.3275080906148867,\n",
       " 0.32755432269995377,\n",
       " 0.3276005547850208,\n",
       " 0.3276005547850208,\n",
       " 0.32764678687008786,\n",
       " 0.32755432269995377,\n",
       " 0.32755432269995377,\n",
       " 0.3275080906148867,\n",
       " 0.32755432269995377,\n",
       " 0.3275080906148867,\n",
       " 0.3275080906148867,\n",
       " 0.3275080906148867,\n",
       " 0.32755432269995377,\n",
       " 0.3274618585298197,\n",
       " 0.3274618585298197,\n",
       " 0.3274618585298197,\n",
       " 0.3275080906148867,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.3274618585298197,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.32741562644475264,\n",
       " 0.3273693943596856,\n",
       " 0.3275080906148867,\n",
       " 0.32755432269995377,\n",
       " 0.3276005547850208,\n",
       " 0.3276005547850208,\n",
       " 0.3276930189551549,\n",
       " 0.3276930189551549,\n",
       " 0.3276930189551549,\n",
       " 0.3276930189551549,\n",
       " 0.32764678687008786,\n",
       " 0.32764678687008786,\n",
       " 0.3276005547850208,\n",
       " 0.32764678687008786,\n",
       " 0.3276930189551549,\n",
       " 0.3276930189551549,\n",
       " 0.32778548312528893,\n",
       " 0.3277392510402219,\n",
       " 0.3277392510402219,\n",
       " 0.3277392510402219,\n",
       " 0.3276930189551549,\n",
       " 0.3277392510402219,\n",
       " 0.3277392510402219,\n",
       " 0.3277392510402219,\n",
       " 0.32778548312528893,\n",
       " 0.32778548312528893,\n",
       " 0.32778548312528893,\n",
       " 0.32764678687008786,\n",
       " 0.3276930189551549,\n",
       " 0.32778548312528893,\n",
       " 0.32778548312528893,\n",
       " 0.32778548312528893,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.3277392510402219,\n",
       " 0.32778548312528893,\n",
       " 0.32778548312528893,\n",
       " 0.32778548312528893,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.327877947295423,\n",
       " 0.327877947295423,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.327831715210356,\n",
       " 0.3279704114655571,\n",
       " 0.3279704114655571,\n",
       " 0.32801664355062415,\n",
       " 0.3279704114655571,\n",
       " 0.3279704114655571,\n",
       " 0.3279704114655571,\n",
       " 0.3279704114655571,\n",
       " 0.3279704114655571,\n",
       " 0.32801664355062415,\n",
       " 0.32806287563569114,\n",
       " 0.32806287563569114,\n",
       " 0.3281091077207582,\n",
       " 0.3281091077207582,\n",
       " 0.32815533980582523,\n",
       " 0.32838650023116045,\n",
       " 0.32843273231622744,\n",
       " 0.3284789644012945,\n",
       " 0.32857142857142857,\n",
       " 0.3286176606564956,\n",
       " 0.3287563569116967,\n",
       " 0.3287101248266297,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.3286176606564956,\n",
       " 0.3286176606564956,\n",
       " 0.3286176606564956,\n",
       " 0.32857142857142857,\n",
       " 0.3286176606564956,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.32866389274156266,\n",
       " 0.3287563569116967,\n",
       " 0.3287101248266297,\n",
       " 0.3287563569116967,\n",
       " 0.3288488210818308,\n",
       " 0.3288488210818308,\n",
       " 0.3288950531668978,\n",
       " 0.3288950531668978,\n",
       " 0.32894128525196487,\n",
       " 0.32894128525196487,\n",
       " 0.32894128525196487,\n",
       " 0.32894128525196487,\n",
       " 0.3288950531668978,\n",
       " 0.3288488210818308,\n",
       " 0.3287563569116967,\n",
       " 0.3287563569116967,\n",
       " 0.3287563569116967,\n",
       " 0.3287563569116967,\n",
       " 0.3287101248266297,\n",
       " 0.3287101248266297]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to be able to make a graph of the error over epochs\n",
    "# so I am going to do that here\n",
    "def batch_descent_error_tracking(train_l, train_f, test_f, test_l, batch_size, step_size, threshold, error_calculator, weights=np.array([0] * 10), conf_cutoff=.8):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "    test_errors = list()\n",
    "    train_errors = list()\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "            test_errors.append(error_calculator(test_f, test_l, weights, conf_cutoff))\n",
    "            train_errors.append(error_calculator(train_f, train_l, weights, conf_cutoff))\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.8f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights, test_errors, train_errors\n",
    "\n",
    "confidence_cutoff = .6\n",
    "trained_model, test_errors, train_errors = batch_descent_error_tracking(train_labels, train_features, test_features, test_labels, len(train_labels), 2, .0005, calc_abs_error, conf_cutoff=confidence_cutoff)\n",
    "test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we need to make a graph of this\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(test_errors)), test_errors)\n",
    "plt.plot(range(len(train_errors)), train_errors, color='orange')\n",
    "plt.xlabel('Epochs of training (whole data set)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Test and Training error over time')\n",
    "plt.savefig('model1perf.jpg')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n",
      "2.01% of stocks bought\n"
     ]
    }
   ],
   "source": [
    "# I need to know how many predictions we make that we end up betting on\n",
    "total_buys = 0\n",
    "for point_ind in range(len(test_labels)):\n",
    "    expect = make_prediction(trained_model, test_features[point_ind])\n",
    "\n",
    "    if expect > confidence_cutoff:\n",
    "        total_buys += 1\n",
    "\n",
    "print(total_buys)\n",
    "print('{:2.2f}% of stocks bought'.format(total_buys / len(test_labels) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Analysis\n",
    "\n",
    "I am going to build a 9-10-10-1 fully connected network with a basic sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make our pytorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layer_1): Linear(in_features=9, out_features=10, bias=True)\n",
      "  (layer_2): Linear(in_features=10, out_features=8, bias=True)\n",
      "  (layer_3): Linear(in_features=8, out_features=5, bias=True)\n",
      "  (layer_4): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# let's build our model 5 layers\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input, h1, h2, h3, out):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(input, h1)\n",
    "        nn.init.uniform_(self.layer_1.weight)\n",
    "        self.layer_2 = nn.Linear(h1, h2)\n",
    "        nn.init.uniform_(self.layer_2.weight)\n",
    "        self.layer_3 = nn.Linear(h2, h3)\n",
    "        nn.init.uniform_(self.layer_3.weight)\n",
    "        self.layer_4 = nn.Linear(h3, out)\n",
    "        nn.init.uniform_(self.layer_4.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = torch.sigmoid(self.layer_1(x))\n",
    "        y = torch.sigmoid(self.layer_2(y))\n",
    "        y = torch.sigmoid(self.layer_3(y))\n",
    "        y = torch.sigmoid(self.layer_4(y))\n",
    "        return y\n",
    "        \n",
    "\n",
    "model = NeuralNetwork(9, 10, 8, 5, 1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layer_1): Linear(in_features=9, out_features=10, bias=True)\n",
      "  (layer_2): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# let's build our model 5 layers\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input, h1, out):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(input, h1)\n",
    "        nn.init.uniform_(self.layer_1.weight)\n",
    "        self.layer_2 = nn.Linear(h1, out)\n",
    "    def forward(self, x):\n",
    "        y = torch.sigmoid(self.layer_1(x))\n",
    "        y = torch.sigmoid(self.layer_2(y))\n",
    "        return y\n",
    "        \n",
    "\n",
    "model = NeuralNetwork(9, 10, 1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set our parameters like loss and learning rate\n",
    "learn_rate = 100\n",
    "batch_size = 32\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to make a special data object to pass\n",
    "# our data into our network\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.X = torch.from_numpy(x.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "train_data = Data(train_features, train_labels)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "\n",
    "test_data = Data(test_features, test_labels)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate the number of misclassifications in a set\n",
    "def calc_frac_wrong(cur_model, dataloader):\n",
    "    wrong = 0\n",
    "    total = 0\n",
    "    \n",
    "\n",
    "    for X, y in dataloader:\n",
    "        pred = cur_model(X)\n",
    "        predicted = np.where(pred < 0.5, 0, 1)\n",
    "        predicted = list(itertools.chain(*predicted))\n",
    "        wrong += (predicted != y.numpy()).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    return wrong / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, y in train_dataloader:\n",
    "#     print(model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 training error: 0.48095680901108406324\r"
     ]
    }
   ],
   "source": [
    "# let's train!\n",
    "\n",
    "num_epochs = 10\n",
    "losses = list()\n",
    "test_loss = list()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    curLoss = 0\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(X)\n",
    "        # print(pred)\n",
    "        loss = loss_fn(pred, y.unsqueeze(-1))\n",
    "        curLoss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(curLoss / len(train_dataloader))\n",
    "    \n",
    "    test_loss.append(calc_frac_wrong(model, test_dataloader))\n",
    "\n",
    "    print('Epoch {:d} training error: {:.20f}'.format(epoch, losses[-1]), end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 48.497457235321313362\n"
     ]
    }
   ],
   "source": [
    "incorrect = 0\n",
    "total = 0\n",
    "# Now let's evaluate our performance\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)\n",
    "        predicted = np.where(outputs < 0.5, 0, 1)\n",
    "        predicted = list(itertools.chain(*predicted))\n",
    "        total += y.size(0)\n",
    "        incorrect += (predicted != y.numpy()).sum().item()\n",
    "\n",
    "print('Final Error: {:2.18f}'.format(100 * incorrect / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the current training session\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(losses)), losses, color='orange')\n",
    "plt.plot(range(len(test_loss)), test_loss, color='blue')\n",
    "plt.xlabel('Epochs of training (whole data set)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Test and Training error over time')\n",
    "plt.savefig('model2perf.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Status\n",
    "\n",
    "## Logistic Regression\n",
    "I feel as though the Logistic Regression Model is likely being held back by \n",
    "1. Weaknesses in the data set\n",
    "2. Uncertainty\n",
    "\n",
    "The best bang for my buck improvements are likely\n",
    "\n",
    "1. Implementing abstension\n",
    "2. Build a new data set (time and money sink)\n",
    "3. Try out SVM\n",
    "\n",
    "## ANN\n",
    "The artificial neural network model might just be too deep or complicated. The current state of it is leading to overfitting. I'm getting training errors of like 38% and test errors of like 48% which is no bueno.\n",
    "\n",
    "1. Simplify model\n",
    "2. Implement Abstension\n",
    "3. Build new data set\n",
    "\n",
    "# Next steps\n",
    "I need some deliverables so I plan on making sure I have well documented evidence of my current results before moving on. I think I'll do things in this order.\n",
    "\n",
    "1. Build training error vs. testing error graph for both existing models.\n",
    "2. Add abstension to both models, make new graphs.\n",
    "3. Simplyify ANN\n",
    "4. Look into building new data set\n",
    "5. Make SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "I am going to make a Simple Vector machine and maybe add in the kernel trick too. We'll see ig. I haven't completely decided yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
