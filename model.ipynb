{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "I have successfully been able to build a sort of data set for this model. Now I want to try training on my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cupy-cuda11x (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cupy-cuda11x\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install cupy-cuda11x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037304</td>\n",
       "      <td>0.081930</td>\n",
       "      <td>68.692936</td>\n",
       "      <td>16.198334</td>\n",
       "      <td>3.792215</td>\n",
       "      <td>1.051065</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.366850</td>\n",
       "      <td>0.337912</td>\n",
       "      <td>8.135137</td>\n",
       "      <td>18.134324</td>\n",
       "      <td>3.515821</td>\n",
       "      <td>0.918390</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.108044</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.132520</td>\n",
       "      <td>0.173255</td>\n",
       "      <td>18.516105</td>\n",
       "      <td>6.586352</td>\n",
       "      <td>1.420348</td>\n",
       "      <td>0.761143</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.236057</td>\n",
       "      <td>0.019177</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071330</td>\n",
       "      <td>0.172091</td>\n",
       "      <td>29.580969</td>\n",
       "      <td>5.532793</td>\n",
       "      <td>1.205178</td>\n",
       "      <td>0.746057</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.436893</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.133795</td>\n",
       "      <td>0.211265</td>\n",
       "      <td>12.743329</td>\n",
       "      <td>4.438462</td>\n",
       "      <td>0.957572</td>\n",
       "      <td>0.715781</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.018786</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.265324</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>3.700391</td>\n",
       "      <td>0.383926</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.948329</td>\n",
       "      <td>4.281250</td>\n",
       "      <td>3.332726</td>\n",
       "      <td>0.322385</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>4.395096</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>0.792549</td>\n",
       "      <td>1.878183</td>\n",
       "      <td>2.160751</td>\n",
       "      <td>2.286619</td>\n",
       "      <td>0.339581</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.039290</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.440450</td>\n",
       "      <td>1.614583</td>\n",
       "      <td>2.256612</td>\n",
       "      <td>0.362996</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.056206</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.242760</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.904923</td>\n",
       "      <td>0.341091</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.257109</td>\n",
       "      <td>2.313978</td>\n",
       "      <td>0.037899</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                0.037304              0.081930  68.692936          16.198334   \n",
       "1                0.366850              0.337912   8.135137          18.134324   \n",
       "2                0.132520              0.173255  18.516105           6.586352   \n",
       "3                0.071330              0.172091  29.580969           5.532793   \n",
       "4                0.133795              0.211265  12.743329           4.438462   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           0.600000              1.265324   3.083333           3.700391   \n",
       "136444           0.400000              0.948329   4.281250           3.332726   \n",
       "136445           0.792549              1.878183   2.160751           2.286619   \n",
       "136446           1.200000              1.440450   1.614583           2.256612   \n",
       "136447           0.800000              1.242760   2.250000           2.904923   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       3.792215      1.051065       0.000404     0.111111  0.013801   CMCSA   \n",
       "1       3.515821      0.918390       0.000015     0.000485  0.108044   CMCSA   \n",
       "2       1.420348      0.761143       0.003187     0.236057  0.019177   CMCSA   \n",
       "3       1.205178      0.746057       0.003692     0.436893  0.010185   CMCSA   \n",
       "4       0.957572      0.715781       0.004574     0.233161  0.018786   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  0.383926      0.001104       0.000011     0.000135  0.031129  SNGR.L   \n",
       "136444  0.322385      0.001067       0.256648     4.395096  0.018825  SNGR.L   \n",
       "136445  0.339581      0.001007       0.000037     0.000320  0.039290  SNGR.L   \n",
       "136446  0.362996      0.000928       0.000067     0.000433  0.056206  SNGR.L   \n",
       "136447  0.341091      0.000955       0.257109     2.313978  0.037899  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# first we need to read the data in\n",
    "\n",
    "data_name = 'dataset.csv'\n",
    "\n",
    "all_data = pd.read_csv(data_name)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netIncomePerShare</th>\n",
       "      <th>freeCashFlowPerShare</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>priceToSalesRatio</th>\n",
       "      <th>pbRatio</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>payoutRatio</th>\n",
       "      <th>roe</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.789873</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518706</td>\n",
       "      <td>11.567448</td>\n",
       "      <td>8.644031</td>\n",
       "      <td>11.667213</td>\n",
       "      <td>-7.813165</td>\n",
       "      <td>9.288283</td>\n",
       "      <td>5.373945</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.789891</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567466</td>\n",
       "      <td>8.643982</td>\n",
       "      <td>11.667212</td>\n",
       "      <td>-11.112948</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374382</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567357</td>\n",
       "      <td>8.643613</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.748617</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373970</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.789875</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567347</td>\n",
       "      <td>8.643575</td>\n",
       "      <td>11.667211</td>\n",
       "      <td>-5.601492</td>\n",
       "      <td>9.288314</td>\n",
       "      <td>5.373929</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.789878</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567337</td>\n",
       "      <td>8.643531</td>\n",
       "      <td>11.667210</td>\n",
       "      <td>-5.387330</td>\n",
       "      <td>9.288295</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136443</th>\n",
       "      <td>9.789904</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567330</td>\n",
       "      <td>8.643430</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-11.419512</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374026</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136444</th>\n",
       "      <td>9.789893</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567326</td>\n",
       "      <td>8.643419</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.360050</td>\n",
       "      <td>9.288680</td>\n",
       "      <td>5.373969</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136445</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-10.202989</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374064</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136446</th>\n",
       "      <td>9.789938</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567316</td>\n",
       "      <td>8.643426</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-9.609178</td>\n",
       "      <td>9.288273</td>\n",
       "      <td>5.374142</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136447</th>\n",
       "      <td>9.789915</td>\n",
       "      <td>14.7112</td>\n",
       "      <td>18.518705</td>\n",
       "      <td>11.567322</td>\n",
       "      <td>8.643422</td>\n",
       "      <td>11.667204</td>\n",
       "      <td>-1.358257</td>\n",
       "      <td>9.288487</td>\n",
       "      <td>5.374057</td>\n",
       "      <td>SNGR.L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136448 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        netIncomePerShare  freeCashFlowPerShare    peRatio  priceToSalesRatio  \\\n",
       "0                9.789873               14.7112  18.518706          11.567448   \n",
       "1                9.789891               14.7112  18.518705          11.567466   \n",
       "2                9.789878               14.7112  18.518705          11.567357   \n",
       "3                9.789875               14.7112  18.518705          11.567347   \n",
       "4                9.789878               14.7112  18.518705          11.567337   \n",
       "...                   ...                   ...        ...                ...   \n",
       "136443           9.789904               14.7112  18.518705          11.567330   \n",
       "136444           9.789893               14.7112  18.518705          11.567326   \n",
       "136445           9.789915               14.7112  18.518705          11.567316   \n",
       "136446           9.789938               14.7112  18.518705          11.567316   \n",
       "136447           9.789915               14.7112  18.518705          11.567322   \n",
       "\n",
       "         pbRatio  debtToEquity  dividendYield  payoutRatio       roe  ticker  \\\n",
       "0       8.644031     11.667213      -7.813165     9.288283  5.373945   CMCSA   \n",
       "1       8.643982     11.667212     -11.112948     9.288273  5.374382   CMCSA   \n",
       "2       8.643613     11.667211      -5.748617     9.288295  5.373970   CMCSA   \n",
       "3       8.643575     11.667211      -5.601492     9.288314  5.373929   CMCSA   \n",
       "4       8.643531     11.667210      -5.387330     9.288295  5.373969   CMCSA   \n",
       "...          ...           ...            ...          ...       ...     ...   \n",
       "136443  8.643430     11.667204     -11.419512     9.288273  5.374026  SNGR.L   \n",
       "136444  8.643419     11.667204      -1.360050     9.288680  5.373969  SNGR.L   \n",
       "136445  8.643422     11.667204     -10.202989     9.288273  5.374064  SNGR.L   \n",
       "136446  8.643426     11.667204      -9.609178     9.288273  5.374142  SNGR.L   \n",
       "136447  8.643422     11.667204      -1.358257     9.288487  5.374057  SNGR.L   \n",
       "\n",
       "        label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "136443    0.0  \n",
       "136444    0.0  \n",
       "136445    1.0  \n",
       "136446    1.0  \n",
       "136447    0.0  \n",
       "\n",
       "[136448 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We may need to consolidate larger values by taking the log of our data before\n",
    "# trying to pull out outliers\n",
    "\n",
    "def log_data(data, cols_to_log=None, feature_size=9):\n",
    "    cols_for_logging = data.columns[:feature_size]\n",
    "\n",
    "    if cols_to_log:\n",
    "        cols_for_logging = cols_to_log\n",
    "\n",
    "    for col in cols_for_logging:\n",
    "        # we have to offset all data by the absolute value of it if its negative\n",
    "        info = data[col].describe()\n",
    "        if info['min'] < 0:\n",
    "            data[col] = data[col] + np.abs(info['min']) + 1\n",
    "\n",
    "        data[col] = np.log(data[col])\n",
    "    \n",
    "    return data\n",
    "\n",
    "all_data = log_data(all_data)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['netIncomePerShare', 'freeCashFlowPerShare', 'peRatio',\n",
      "       'priceToSalesRatio', 'pbRatio', 'debtToEquity', 'dividendYield',\n",
      "       'payoutRatio', 'roe'],\n",
      "      dtype='object')\n",
      "Old length: 136448\n",
      "New length: 108152\n"
     ]
    }
   ],
   "source": [
    "# We need to attempt to handle outliers in our data.\n",
    "# this removes outliers based on the interquartile range\n",
    "def remove_outliers_iqr(data, iqr_mod=1.5, feature_size=9):\n",
    "    cols_for_trimming = data.columns[:feature_size]\n",
    "\n",
    "    # we need to go through each feature and check the inter quartile ranges.\n",
    "    # we'll drop values with info outside of the multiplier on the range we\n",
    "    # provided\n",
    "    print('Old length: {:d}'.format(len(data)))\n",
    "    for col in cols_for_trimming:\n",
    "        info = data[col].describe()\n",
    "        range_add = (info['75%'] - info['25%']) * iqr_mod\n",
    "        data = data[(data[col] >= info['25%'] - range_add)]\n",
    "        data = data[(data[col] <= info['75%'] + range_add)]\n",
    "    print('New length: {:d}'.format(len(data)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# it might be useful to try making one for the z-score, but I don't know\n",
    "# if our distribution is normalized.\n",
    "\n",
    "all_data = remove_outliers_iqr(all_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.790051250741316\n",
      "14.711201294824106\n",
      "18.518706486141376\n",
      "11.568015320151485\n",
      "8.645939524629462\n",
      "11.667243503526587\n",
      "3.6137942376991385\n",
      "9.288617587790053\n",
      "5.374716997680058\n"
     ]
    }
   ],
   "source": [
    "# there are a couple of preliminary steps we need to take. \n",
    "\n",
    "# 1) handle outliers\n",
    "# 2) normalize all of the data (0 to 1)\n",
    "# 3) separate testing and training sets\n",
    "\n",
    "\n",
    "def normalize_data(data, feature_size=9):\n",
    "    cols_for_normalization = data.columns[:feature_size]\n",
    "    \n",
    "    for col in cols_for_normalization:\n",
    "        max = data[col].max()\n",
    "        min = data[col].min()\n",
    "        print(max)\n",
    "\n",
    "        data[col] = (data[col] - min) / (max - min)\n",
    "\n",
    "normalize_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 1., 1., 0.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate training and test sets\n",
    "\n",
    "def separate_sets(data, prop_test, prop_train=0, feature_size=9):\n",
    "\n",
    "    # we need to drop superfluous info like tickers\n",
    "    data = data.drop(['ticker'], axis=1)\n",
    "\n",
    "    if prop_train == 0 or prop_train + prop_test > 1:\n",
    "        prop_train = 1 - prop_test\n",
    "    \n",
    "    # we find out the ratio to take from the remaining portion\n",
    "    prop_train = (1 - prop_test) / prop_train\n",
    "    if prop_train > 1:\n",
    "        prop_train = 1\n",
    "\n",
    "    test_set = data.sample(frac=prop_test)\n",
    "    data = data.drop(test_set.index)\n",
    "    train_set = data.sample(frac=prop_train)\n",
    "\n",
    "    test_f = np.array(test_set[test_set.columns[:feature_size]])\n",
    "    test_l = np.concatenate(np.array(test_set[test_set.columns[feature_size:]]))\n",
    "\n",
    "    train_f = np.array(train_set[train_set.columns[:feature_size]])\n",
    "    train_l = np.concatenate(np.array(train_set[train_set.columns[feature_size:]]))\n",
    "\n",
    "    return (test_f, test_l, train_f, train_l)\n",
    "\n",
    "test_features, test_labels, train_features, train_labels = separate_sets(all_data, .2)\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "Now that we have theoretcially written everything needed to properly prep the data, we need to choose a model to implement. I think for now I will attempt to start off with basic logistic regression and increase complexity as needed.\n",
    "\n",
    "This means we will need a: \n",
    "* predictor\n",
    "* loss function\n",
    "* derivative of loss function\n",
    "* gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06029810285677426"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_prediction(weights, features):\n",
    "    return 1 / (1 + np.exp(-np.dot(weights[1:], features) - weights[0]))\n",
    "\n",
    "make_prediction([-.5] * 10, train_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Functions\n",
    "\n",
    "I didn't actually remember the math behind this so I looked up a good logistic regression algorithm loss and gradient of the loss function. That way I can guarantee, or at least better guarantee, that if something is wrong it's not this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4013924647922666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think a simple 1 or 0 single loss function might be the best for now\n",
    "# we can come back and change it later if need be\n",
    "\n",
    "# I got this from a youtube video. Math is hard. \n",
    "def single_loss_log(weights, features, label):\n",
    "    y_hat = make_prediction(weights, features)\n",
    "    return label * np.log(y_hat) + (1 - label) * np.log(1 - y_hat)\n",
    "\n",
    "def batch_loss(batch_start_ind, batch_size, loss_func, weights, train_f, train_l):\n",
    "    total_loss = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        total_loss += loss_func(weights, train_f[(point_ind + batch_start_ind) % len(train_f)], train_l[(point_ind + batch_start_ind) % len(train_l)])\n",
    "    return total_loss / batch_size\n",
    "\n",
    "single_loss_log([-.5] * 10, train_features[0], train_labels[0])\n",
    "batch_loss(0, len(train_labels), single_loss_log, [-.5] * 10, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42151374, -0.21774317, -0.21452721, -0.21214122, -0.21478777,\n",
       "       -0.21894443, -0.21521693, -0.20623745, -0.21490058, -0.21695565])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gradient(batch_start_ind, batch_size, weights, train_f, train_l):\n",
    "    total_diff_theta = np.array([0.0] * (len(weights) - 1))\n",
    "    total_diff_b = 0\n",
    "    for point_ind in range(batch_size):\n",
    "        y_diff = make_prediction(weights, train_f[(point_ind + batch_start_ind) % len(train_f)]) - train_l[(point_ind + batch_start_ind) % len(train_l)]\n",
    "        total_diff_theta +=  y_diff * train_f[(point_ind + batch_start_ind) % len(train_f)]\n",
    "        total_diff_b += y_diff\n",
    "    total_diff_theta = np.insert(total_diff_theta, 0, total_diff_b)\n",
    "\n",
    "    total_diff_theta /= batch_size\n",
    "\n",
    "    return total_diff_theta\n",
    "\n",
    "grad = batch_gradient(0, len(train_labels), [-.5] * 10, train_features, train_labels)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.32271759  0.59027348  0.25760673  0.12250442 -0.17481487  1.36849358\n",
      "  0.0649838  -0.03209296 -0.46060265  0.72063927]\n"
     ]
    }
   ],
   "source": [
    "def batch_descent(train_l, train_f, batch_size, step_size, threshold, weights=np.array([0] * 10)):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.8f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights\n",
    "\n",
    "trained_model = batch_descent(train_labels, train_features, len(train_labels), 2, .00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4521497919556172"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_error(test_f, test_l, weights):\n",
    "    total = 0\n",
    "    test_total = 0\n",
    "    for point_ind in range(len(test_l)):\n",
    "        expectation = make_prediction(weights, test_f[point_ind])\n",
    "        \n",
    "        test_total += test_l[point_ind]\n",
    "\n",
    "        if (expectation - .5) * ( test_l[point_ind] - .5) < 0:\n",
    "            total += 1\n",
    "    \n",
    "    return total / len(test_l)\n",
    "\n",
    "calc_error(test_features, test_labels, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.32271759  0.59027348  0.25760673  0.12250442 -0.17481487  1.36849358\n",
      "  0.0649838  -0.03209296 -0.46060265  0.72063927]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.48428109107720757,\n",
       " 0.4840499306518724,\n",
       " 0.48381877022653724,\n",
       " 0.4839112343966713,\n",
       " 0.48381877022653724,\n",
       " 0.483587609801202,\n",
       " 0.48335644937586686,\n",
       " 0.4831715210355987,\n",
       " 0.4831715210355987,\n",
       " 0.4828941285251965,\n",
       " 0.4820157189089228,\n",
       " 0.48104484512251505,\n",
       " 0.48067498844197876,\n",
       " 0.48058252427184467,\n",
       " 0.4801202034211743,\n",
       " 0.4800277392510402,\n",
       " 0.47928802588996766,\n",
       " 0.47905686546463244,\n",
       " 0.47896440129449835,\n",
       " 0.4783633841886269,\n",
       " 0.47831715210355985,\n",
       " 0.4774387424872862,\n",
       " 0.4766065649560795,\n",
       " 0.47554322699953766,\n",
       " 0.47535829865926954,\n",
       " 0.4749422098936662,\n",
       " 0.47406380027739253,\n",
       " 0.4749422098936662,\n",
       " 0.47397133610725845,\n",
       " 0.47313915857605177,\n",
       " 0.4723532131299122,\n",
       " 0.47212205270457697,\n",
       " 0.4715210355987055,\n",
       " 0.4711511789181692,\n",
       " 0.47055016181229775,\n",
       " 0.46916319926028666,\n",
       " 0.4688858067498844,\n",
       " 0.46856218215441514,\n",
       " 0.46865464632454923,\n",
       " 0.46856218215441514,\n",
       " 0.46819232547387885,\n",
       " 0.4673139158576052,\n",
       " 0.46675913083680076,\n",
       " 0.46680536292186775,\n",
       " 0.46703652334720297,\n",
       " 0.46680536292186775,\n",
       " 0.46708275543227,\n",
       " 0.4672214516874711,\n",
       " 0.46703652334720297,\n",
       " 0.46675913083680076,\n",
       " 0.4664817383263985,\n",
       " 0.4662043458159963,\n",
       " 0.46583448913546,\n",
       " 0.46500231160425337,\n",
       " 0.4652797041146556,\n",
       " 0.4655570966250578,\n",
       " 0.4656957928802589,\n",
       " 0.46541840036985666,\n",
       " 0.46551086453999074,\n",
       " 0.46523347202958854,\n",
       " 0.4652797041146556,\n",
       " 0.4647249190938511,\n",
       " 0.4647249190938511,\n",
       " 0.4644475265834489,\n",
       " 0.4644475265834489,\n",
       " 0.4641701340730467,\n",
       " 0.46375404530744335,\n",
       " 0.46393897364771153,\n",
       " 0.4641701340730467,\n",
       " 0.46384650947757744,\n",
       " 0.46384650947757744,\n",
       " 0.4638002773925104,\n",
       " 0.4636615811373093,\n",
       " 0.463291724456773,\n",
       " 0.46301433194637076,\n",
       " 0.4626907073509015,\n",
       " 0.46255201109570043,\n",
       " 0.46218215441516414,\n",
       " 0.46199722607489596,\n",
       " 0.4617660656495608,\n",
       " 0.4618585298196949,\n",
       " 0.46181229773462784,\n",
       " 0.4615811373092926,\n",
       " 0.46162736939435967,\n",
       " 0.4615811373092926,\n",
       " 0.4608414239482201,\n",
       " 0.4608414239482201,\n",
       " 0.46033287101248266,\n",
       " 0.4601017105871475,\n",
       " 0.4597318539066112,\n",
       " 0.459546925566343,\n",
       " 0.45959315765141007,\n",
       " 0.45959315765141007,\n",
       " 0.459546925566343,\n",
       " 0.45940822931114195,\n",
       " 0.4591770688858067,\n",
       " 0.4591770688858067,\n",
       " 0.45876098012020344,\n",
       " 0.45866851595006936,\n",
       " 0.45857605177993527,\n",
       " 0.4586222838650023,\n",
       " 0.4586222838650023,\n",
       " 0.45839112343966715,\n",
       " 0.4579750346740638,\n",
       " 0.45779010633379563,\n",
       " 0.45779010633379563,\n",
       " 0.45792880258899676,\n",
       " 0.4578363384188627,\n",
       " 0.4578825705039297,\n",
       " 0.45802126675913085,\n",
       " 0.45802126675913085,\n",
       " 0.4578825705039297,\n",
       " 0.45779010633379563,\n",
       " 0.45755894590846047,\n",
       " 0.45755894590846047,\n",
       " 0.4576051779935275,\n",
       " 0.4576051779935275,\n",
       " 0.45755894590846047,\n",
       " 0.45755894590846047,\n",
       " 0.45755894590846047,\n",
       " 0.45737401756819235,\n",
       " 0.45742024965325934,\n",
       " 0.45742024965325934,\n",
       " 0.45728155339805826,\n",
       " 0.4572353213129912,\n",
       " 0.4572353213129912,\n",
       " 0.45700416088765605,\n",
       " 0.4568192325473879,\n",
       " 0.4564493758668516,\n",
       " 0.45663430420711976,\n",
       " 0.4567267683772538,\n",
       " 0.4564493758668516,\n",
       " 0.45635691169671755,\n",
       " 0.45635691169671755,\n",
       " 0.4564493758668516,\n",
       " 0.4564493758668516,\n",
       " 0.4562182154415164,\n",
       " 0.45640314378178454,\n",
       " 0.4563106796116505,\n",
       " 0.4563106796116505,\n",
       " 0.4561719833564494,\n",
       " 0.45603328710124824,\n",
       " 0.45589459084604717,\n",
       " 0.45561719833564496,\n",
       " 0.455709662505779,\n",
       " 0.4559408229311142,\n",
       " 0.45589459084604717,\n",
       " 0.45598705501618125,\n",
       " 0.4558483587609801,\n",
       " 0.4558483587609801,\n",
       " 0.45575589459084603,\n",
       " 0.4554785020804438,\n",
       " 0.4554785020804438,\n",
       " 0.4554322699953768,\n",
       " 0.45538603791030974,\n",
       " 0.4553398058252427,\n",
       " 0.4555709662505779,\n",
       " 0.455663430420712,\n",
       " 0.4555709662505779,\n",
       " 0.455663430420712,\n",
       " 0.455663430420712,\n",
       " 0.4558483587609801,\n",
       " 0.4559408229311142,\n",
       " 0.45589459084604717,\n",
       " 0.4559408229311142,\n",
       " 0.4559408229311142,\n",
       " 0.4559408229311142,\n",
       " 0.45598705501618125,\n",
       " 0.455709662505779,\n",
       " 0.4554785020804438,\n",
       " 0.45538603791030974,\n",
       " 0.4552011095700416,\n",
       " 0.4552011095700416,\n",
       " 0.4552935737401757,\n",
       " 0.4554322699953768,\n",
       " 0.45538603791030974,\n",
       " 0.45552473416551087,\n",
       " 0.4555709662505779,\n",
       " 0.45561719833564496,\n",
       " 0.4558021266759131,\n",
       " 0.45561719833564496,\n",
       " 0.45561719833564496,\n",
       " 0.4555709662505779,\n",
       " 0.4554785020804438,\n",
       " 0.4554785020804438,\n",
       " 0.4554785020804438,\n",
       " 0.4555709662505779,\n",
       " 0.45552473416551087,\n",
       " 0.4554785020804438,\n",
       " 0.4555709662505779,\n",
       " 0.4555709662505779,\n",
       " 0.455663430420712,\n",
       " 0.455663430420712,\n",
       " 0.455709662505779,\n",
       " 0.4554785020804438,\n",
       " 0.4554322699953768,\n",
       " 0.4553398058252427,\n",
       " 0.4552935737401757,\n",
       " 0.4552011095700416,\n",
       " 0.4552011095700416,\n",
       " 0.4550624133148405,\n",
       " 0.4550624133148405,\n",
       " 0.45496994914470645,\n",
       " 0.45501618122977344,\n",
       " 0.45501618122977344,\n",
       " 0.45487748497457237,\n",
       " 0.4549237170596394,\n",
       " 0.4548312528895053,\n",
       " 0.4548312528895053,\n",
       " 0.45487748497457237,\n",
       " 0.4547850208044383,\n",
       " 0.45473878871937123,\n",
       " 0.4547850208044383,\n",
       " 0.45460009246417016,\n",
       " 0.454415164123902,\n",
       " 0.45450762829403607,\n",
       " 0.454461396208969,\n",
       " 0.45464632454923715,\n",
       " 0.4547850208044383,\n",
       " 0.45464632454923715,\n",
       " 0.45464632454923715,\n",
       " 0.45473878871937123,\n",
       " 0.45487748497457237,\n",
       " 0.45473878871937123,\n",
       " 0.45473878871937123,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.4549237170596394,\n",
       " 0.45487748497457237,\n",
       " 0.4549237170596394,\n",
       " 0.45496994914470645,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.4547850208044383,\n",
       " 0.45473878871937123,\n",
       " 0.45464632454923715,\n",
       " 0.4546925566343042,\n",
       " 0.4547850208044383,\n",
       " 0.4547850208044383,\n",
       " 0.45473878871937123,\n",
       " 0.45473878871937123,\n",
       " 0.4547850208044383,\n",
       " 0.4546925566343042,\n",
       " 0.4546925566343042,\n",
       " 0.45473878871937123,\n",
       " 0.45473878871937123,\n",
       " 0.4546925566343042,\n",
       " 0.45460009246417016,\n",
       " 0.45450762829403607,\n",
       " 0.45460009246417016,\n",
       " 0.45464632454923715,\n",
       " 0.45473878871937123,\n",
       " 0.4546925566343042,\n",
       " 0.45464632454923715,\n",
       " 0.45473878871937123,\n",
       " 0.45473878871937123,\n",
       " 0.45473878871937123,\n",
       " 0.45487748497457237,\n",
       " 0.4549237170596394,\n",
       " 0.45496994914470645,\n",
       " 0.45501618122977344,\n",
       " 0.45496994914470645,\n",
       " 0.45496994914470645,\n",
       " 0.45501618122977344,\n",
       " 0.4550624133148405,\n",
       " 0.45524734165510866,\n",
       " 0.45510864539990753,\n",
       " 0.45501618122977344,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.4549237170596394,\n",
       " 0.45496994914470645,\n",
       " 0.4549237170596394,\n",
       " 0.4550624133148405,\n",
       " 0.45501618122977344,\n",
       " 0.45501618122977344,\n",
       " 0.45510864539990753,\n",
       " 0.45501618122977344,\n",
       " 0.45501618122977344,\n",
       " 0.4550624133148405,\n",
       " 0.45487748497457237,\n",
       " 0.45496994914470645,\n",
       " 0.45501618122977344,\n",
       " 0.45510864539990753,\n",
       " 0.4550624133148405,\n",
       " 0.45510864539990753,\n",
       " 0.4551548774849746,\n",
       " 0.4551548774849746,\n",
       " 0.45510864539990753,\n",
       " 0.45510864539990753,\n",
       " 0.45496994914470645,\n",
       " 0.4549237170596394,\n",
       " 0.45501618122977344,\n",
       " 0.45501618122977344,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.45496994914470645,\n",
       " 0.4549237170596394,\n",
       " 0.4548312528895053,\n",
       " 0.45496994914470645,\n",
       " 0.4549237170596394,\n",
       " 0.4548312528895053,\n",
       " 0.4548312528895053,\n",
       " 0.4548312528895053,\n",
       " 0.4549237170596394,\n",
       " 0.4548312528895053,\n",
       " 0.4548312528895053,\n",
       " 0.45487748497457237,\n",
       " 0.4547850208044383,\n",
       " 0.4547850208044383,\n",
       " 0.4548312528895053,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.4549237170596394,\n",
       " 0.4549237170596394,\n",
       " 0.45487748497457237,\n",
       " 0.4549237170596394,\n",
       " 0.4549237170596394,\n",
       " 0.45501618122977344,\n",
       " 0.4550624133148405,\n",
       " 0.4552011095700416,\n",
       " 0.4550624133148405,\n",
       " 0.4550624133148405,\n",
       " 0.4550624133148405,\n",
       " 0.4550624133148405,\n",
       " 0.4549237170596394,\n",
       " 0.45487748497457237,\n",
       " 0.4549237170596394,\n",
       " 0.4549237170596394,\n",
       " 0.45496994914470645,\n",
       " 0.4547850208044383,\n",
       " 0.4547850208044383,\n",
       " 0.45487748497457237,\n",
       " 0.45496994914470645,\n",
       " 0.45487748497457237,\n",
       " 0.45487748497457237,\n",
       " 0.4547850208044383,\n",
       " 0.45473878871937123,\n",
       " 0.4547850208044383,\n",
       " 0.4547850208044383,\n",
       " 0.4548312528895053,\n",
       " 0.4547850208044383,\n",
       " 0.4547850208044383,\n",
       " 0.45464632454923715,\n",
       " 0.45464632454923715,\n",
       " 0.4546925566343042,\n",
       " 0.45464632454923715,\n",
       " 0.4546925566343042,\n",
       " 0.4546925566343042,\n",
       " 0.4545538603791031,\n",
       " 0.45450762829403607,\n",
       " 0.454461396208969,\n",
       " 0.45450762829403607,\n",
       " 0.4545538603791031,\n",
       " 0.454461396208969,\n",
       " 0.45460009246417016,\n",
       " 0.45464632454923715,\n",
       " 0.45460009246417016,\n",
       " 0.45460009246417016,\n",
       " 0.45460009246417016,\n",
       " 0.45450762829403607,\n",
       " 0.454461396208969,\n",
       " 0.454461396208969,\n",
       " 0.454415164123902,\n",
       " 0.454461396208969,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.45423023578363386,\n",
       " 0.4543226999537679,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.4541377716134998,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.45436893203883494,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.454415164123902,\n",
       " 0.454415164123902,\n",
       " 0.454461396208969,\n",
       " 0.454461396208969,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.45423023578363386,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.45436893203883494,\n",
       " 0.454415164123902,\n",
       " 0.454415164123902,\n",
       " 0.45450762829403607,\n",
       " 0.454461396208969,\n",
       " 0.454415164123902,\n",
       " 0.454461396208969,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.45436893203883494,\n",
       " 0.4543226999537679,\n",
       " 0.45423023578363386,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.4540453074433657,\n",
       " 0.45409153952843273,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.45436893203883494,\n",
       " 0.454461396208969,\n",
       " 0.454415164123902,\n",
       " 0.45436893203883494,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.4542764678687009,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541377716134998,\n",
       " 0.4541840036985668,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.4540453074433657,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.45409153952843273,\n",
       " 0.4541377716134998,\n",
       " 0.4540453074433657,\n",
       " 0.45409153952843273,\n",
       " 0.4540453074433657,\n",
       " 0.4540453074433657,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.45409153952843273,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4541840036985668,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541377716134998,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541377716134998,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4541377716134998,\n",
       " 0.45409153952843273,\n",
       " 0.4541377716134998,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.45436893203883494,\n",
       " 0.45436893203883494,\n",
       " 0.45436893203883494,\n",
       " 0.45436893203883494,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4542764678687009,\n",
       " 0.4541840036985668,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.4541377716134998,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4543226999537679,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4543226999537679,\n",
       " 0.45436893203883494,\n",
       " 0.454415164123902,\n",
       " 0.4543226999537679,\n",
       " 0.45436893203883494,\n",
       " 0.454415164123902,\n",
       " 0.454415164123902,\n",
       " 0.45436893203883494,\n",
       " 0.454415164123902,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.4542764678687009,\n",
       " 0.4541840036985668,\n",
       " 0.4541377716134998,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.4541377716134998,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.45409153952843273,\n",
       " 0.4540453074433657,\n",
       " 0.4540453074433657,\n",
       " 0.4540453074433657,\n",
       " 0.45409153952843273,\n",
       " 0.4540453074433657,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.4540453074433657,\n",
       " 0.4539528432732316,\n",
       " 0.4539066111881646,\n",
       " 0.4537679149329635,\n",
       " 0.4537679149329635,\n",
       " 0.4537679149329635,\n",
       " 0.45372168284789643,\n",
       " 0.4537679149329635,\n",
       " 0.4538141470180305,\n",
       " 0.4537679149329635,\n",
       " 0.45386037910309757,\n",
       " 0.4539066111881646,\n",
       " 0.4538141470180305,\n",
       " 0.4538141470180305,\n",
       " 0.4538141470180305,\n",
       " 0.45372168284789643,\n",
       " 0.4538141470180305,\n",
       " 0.45372168284789643,\n",
       " 0.4537679149329635,\n",
       " 0.45372168284789643,\n",
       " 0.4536754507628294,\n",
       " 0.45372168284789643,\n",
       " 0.45372168284789643,\n",
       " 0.4537679149329635,\n",
       " 0.4537679149329635,\n",
       " 0.4537679149329635,\n",
       " 0.4539066111881646,\n",
       " 0.45386037910309757,\n",
       " 0.4538141470180305,\n",
       " 0.4537679149329635,\n",
       " 0.4536754507628294,\n",
       " 0.4536754507628294,\n",
       " 0.45362921867776235,\n",
       " 0.45362921867776235,\n",
       " 0.4536754507628294,\n",
       " 0.45362921867776235,\n",
       " 0.45362921867776235,\n",
       " 0.4535829865926953,\n",
       " 0.4536754507628294,\n",
       " 0.45372168284789643,\n",
       " 0.4538141470180305,\n",
       " 0.45386037910309757,\n",
       " 0.45372168284789643,\n",
       " 0.4537679149329635,\n",
       " 0.4537679149329635,\n",
       " 0.45386037910309757,\n",
       " 0.4539066111881646,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.45399907535829864,\n",
       " 0.4540453074433657,\n",
       " 0.4540453074433657,\n",
       " 0.45409153952843273,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.45409153952843273,\n",
       " 0.4540453074433657,\n",
       " 0.4540453074433657,\n",
       " 0.4541377716134998,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.45436893203883494,\n",
       " 0.45436893203883494,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.454415164123902,\n",
       " 0.454415164123902,\n",
       " 0.454415164123902,\n",
       " 0.454415164123902,\n",
       " 0.45450762829403607,\n",
       " 0.4545538603791031,\n",
       " 0.45464632454923715,\n",
       " 0.45473878871937123,\n",
       " 0.4547850208044383,\n",
       " 0.4548312528895053,\n",
       " 0.4548312528895053,\n",
       " 0.45473878871937123,\n",
       " 0.45473878871937123,\n",
       " 0.45473878871937123,\n",
       " 0.4547850208044383,\n",
       " 0.45473878871937123,\n",
       " 0.4546925566343042,\n",
       " 0.45464632454923715,\n",
       " 0.4545538603791031,\n",
       " 0.4545538603791031,\n",
       " 0.45450762829403607,\n",
       " 0.4545538603791031,\n",
       " 0.4545538603791031,\n",
       " 0.45450762829403607,\n",
       " 0.45450762829403607,\n",
       " 0.454461396208969,\n",
       " 0.454461396208969,\n",
       " 0.454415164123902,\n",
       " 0.4543226999537679,\n",
       " 0.4543226999537679,\n",
       " 0.4542764678687009,\n",
       " 0.45423023578363386,\n",
       " 0.4542764678687009,\n",
       " 0.4542764678687009,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.45423023578363386,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.4541377716134998,\n",
       " 0.4540453074433657,\n",
       " 0.45399907535829864,\n",
       " 0.45399907535829864,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.4539066111881646,\n",
       " 0.4539066111881646,\n",
       " 0.45386037910309757,\n",
       " 0.45386037910309757,\n",
       " 0.45386037910309757,\n",
       " 0.4539066111881646,\n",
       " 0.45386037910309757,\n",
       " 0.4539066111881646,\n",
       " 0.4540453074433657,\n",
       " 0.4541377716134998,\n",
       " 0.4541377716134998,\n",
       " 0.4541840036985668,\n",
       " 0.4541840036985668,\n",
       " 0.45423023578363386,\n",
       " 0.4540453074433657,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.4540453074433657,\n",
       " 0.45399907535829864,\n",
       " 0.4539528432732316,\n",
       " 0.4539066111881646,\n",
       " 0.45386037910309757,\n",
       " 0.45386037910309757,\n",
       " 0.45386037910309757,\n",
       " 0.4539066111881646,\n",
       " 0.45386037910309757,\n",
       " 0.4539066111881646,\n",
       " 0.4539528432732316,\n",
       " 0.4540453074433657,\n",
       " 0.4540453074433657,\n",
       " 0.4540453074433657,\n",
       " 0.45399907535829864,\n",
       " 0.45399907535829864,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.4539528432732316,\n",
       " 0.45409153952843273,\n",
       " 0.45409153952843273,\n",
       " 0.4540453074433657,\n",
       " 0.45409153952843273,\n",
       " 0.45399907535829864,\n",
       " 0.45399907535829864,\n",
       " 0.45399907535829864,\n",
       " 0.4539528432732316,\n",
       " 0.4539066111881646,\n",
       " 0.4538141470180305,\n",
       " 0.4538141470180305,\n",
       " 0.4538141470180305,\n",
       " 0.45372168284789643,\n",
       " 0.4538141470180305,\n",
       " 0.4536754507628294,\n",
       " 0.45362921867776235,\n",
       " 0.45362921867776235,\n",
       " 0.45362921867776235,\n",
       " 0.4535829865926953,\n",
       " 0.4535829865926953,\n",
       " 0.4535367545076283,\n",
       " 0.4535829865926953,\n",
       " 0.4535829865926953,\n",
       " 0.4535829865926953,\n",
       " 0.4535829865926953,\n",
       " 0.45349052242256127,\n",
       " 0.4535367545076283,\n",
       " 0.45349052242256127,\n",
       " 0.45349052242256127,\n",
       " 0.45335182616736014,\n",
       " 0.45335182616736014,\n",
       " 0.4533055940822931,\n",
       " 0.45321312991215906,\n",
       " 0.453166897827092,\n",
       " 0.453166897827092,\n",
       " 0.453166897827092,\n",
       " 0.45321312991215906,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45321312991215906,\n",
       " 0.453166897827092,\n",
       " 0.45321312991215906,\n",
       " 0.453166897827092,\n",
       " 0.453166897827092,\n",
       " 0.453166897827092,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.453166897827092,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.453166897827092,\n",
       " 0.453166897827092,\n",
       " 0.453166897827092,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.45325936199722605,\n",
       " 0.4533055940822931,\n",
       " 0.45335182616736014,\n",
       " 0.45335182616736014,\n",
       " 0.4533055940822931,\n",
       " 0.4533055940822931,\n",
       " 0.4533055940822931,\n",
       " 0.4533055940822931,\n",
       " 0.4533055940822931,\n",
       " 0.45325936199722605,\n",
       " 0.453166897827092,\n",
       " 0.45321312991215906,\n",
       " 0.45321312991215906,\n",
       " 0.453166897827092,\n",
       " 0.45321312991215906,\n",
       " 0.453120665742025,\n",
       " 0.45307443365695793,\n",
       " 0.45298196948682384,\n",
       " 0.45288950531668976,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.45288950531668976,\n",
       " 0.45288950531668976,\n",
       " 0.45284327323162277,\n",
       " 0.4527970411465557,\n",
       " 0.4527970411465557,\n",
       " 0.45284327323162277,\n",
       " 0.45284327323162277,\n",
       " 0.45284327323162277,\n",
       " 0.4527970411465557,\n",
       " 0.4527970411465557,\n",
       " 0.45284327323162277,\n",
       " 0.45288950531668976,\n",
       " 0.4527970411465557,\n",
       " 0.4527970411465557,\n",
       " 0.4527970411465557,\n",
       " 0.45284327323162277,\n",
       " 0.45288950531668976,\n",
       " 0.45288950531668976,\n",
       " 0.45288950531668976,\n",
       " 0.45288950531668976,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.45307443365695793,\n",
       " 0.4530282015718909,\n",
       " 0.45307443365695793,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.45307443365695793,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.453166897827092,\n",
       " 0.45321312991215906,\n",
       " 0.45321312991215906,\n",
       " 0.45321312991215906,\n",
       " 0.45321312991215906,\n",
       " 0.45321312991215906,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.45298196948682384,\n",
       " 0.4529357374017568,\n",
       " 0.45288950531668976,\n",
       " 0.45288950531668976,\n",
       " 0.45288950531668976,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.4530282015718909,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4529357374017568,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.4530282015718909,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.4530282015718909,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.45307443365695793,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.453120665742025,\n",
       " 0.453166897827092,\n",
       " 0.453166897827092,\n",
       " 0.45307443365695793,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.45298196948682384,\n",
       " 0.4529357374017568,\n",
       " 0.4529357374017568,\n",
       " 0.45288950531668976,\n",
       " 0.45284327323162277,\n",
       " 0.4527508090614887,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.4526583448913546,\n",
       " 0.45270457697642164,\n",
       " 0.45270457697642164,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " 0.4527508090614887,\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to be able to make a graph of the error over epochs\n",
    "# so I am going to do that here\n",
    "def batch_descent_error_tracking(train_l, train_f, test_f, test_l, batch_size, step_size, threshold, weights=np.array([0] * 10)):\n",
    "    batch_ind = 0\n",
    "    grad_mag = threshold + 100\n",
    "    errors = list()\n",
    "\n",
    "    while grad_mag > threshold:\n",
    "        try:\n",
    "            grad = batch_gradient(batch_ind, batch_size, weights, train_f, train_l)\n",
    "\n",
    "            grad_mag = np.linalg.norm(grad)\n",
    "\n",
    "            # let's descend!\n",
    "            weights = weights - step_size * grad\n",
    "            errors.append(calc_error(test_f, test_l, weights))\n",
    "\n",
    "            batch_ind += batch_size\n",
    "            print('Num batches: {:d}. Gradient magnitude: {:.8f}'.format(batch_ind // batch_size, grad_mag), end='\\r', flush=True)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(weights)\n",
    "    return weights, errors\n",
    "\n",
    "trained_model, errors = batch_descent_error_tracking(train_labels, train_features, test_features, test_labels, len(train_labels), 2, .00001)\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we need to make a graph of this\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(errors)), errors)\n",
    "plt.xlabel('Epochs of training (whole data set)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Testing error over time')\n",
    "plt.savefig('model1perf.jpg')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Analysis\n",
    "\n",
    "I am going to build a 9-10-10-1 fully connected network with a basic sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's make the activation function for simplicity sake\n",
    "def activate(x):\n",
    "    return 1 / (1 + np.exp(-1 * x)) \n",
    "\n",
    "def active_grad(x):\n",
    "    return np.exp(-1 * x) / (1 + np.exp(-1 * x)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49420331 0.58369711 0.55093786 ... 0.46858582 0.4899978  0.47202463]\n",
      " [0.46292652 0.37304276 0.62724563 ... 0.47236457 0.5239749  0.45200056]\n",
      " [0.45098687 0.41837307 0.60661045 ... 0.40238183 0.46157213 0.42498836]\n",
      " ...\n",
      " [0.47999593 0.39616514 0.48014235 ... 0.3357062  0.44270399 0.51367435]\n",
      " [0.51059909 0.4223003  0.43497771 ... 0.61016178 0.52216708 0.64601989]\n",
      " [0.45259873 0.44015791 0.50073155 ... 0.45841361 0.466105   0.46836553]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# we need to prepare our data to be run through the network in the correct shape\n",
    "nt_labels = train_labels.reshape(len(train_labels), 1)\n",
    "nt_features = train_features\n",
    "print(nt_features)\n",
    "print(nt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll also select a starting seed\n",
    "# np.random.seed(2022)\n",
    "\n",
    "# now we should initialize our neurons in our layers\n",
    "layer1 = 2 * np.random.random((9, 10)) - 1 \n",
    "layer2 = 2 * np.random.random((10, 10)) - 1\n",
    "layer3 = 2 * np.random.random((10, 1)) - 1\n",
    "\n",
    "# I will write a function that trains for a set number of epochs\n",
    "def train_epochs(num_epochs, alpha, layer1, layer2, layer3, features, labels, printfreq=10):\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        l0 = features\n",
    "        l1 = activate(np.dot(l0, layer1))\n",
    "        l2 = activate(np.dot(l1, layer2))\n",
    "        l3 = activate(np.dot(l2, layer3))\n",
    "\n",
    "        if e % printfreq == 0:\n",
    "            l3_error = labels - l3 \n",
    "\n",
    "            # we should print our current training error every once in awhile\n",
    "            print(\"Epoch {:d} Error:{:2.4f}%\".format(e, np.mean(np.abs(l3_error)) * 100))\n",
    "\n",
    "        # let's go through and backpropagate\n",
    "\n",
    "        l3_grad = l3_error * active_grad(l3)\n",
    "        \n",
    "        l2_error = l3_grad.dot(layer3.T)\n",
    "\n",
    "        l2_grad = l2_error * active_grad(l2)\n",
    "\n",
    "        l1_error = l2_grad.dot(layer2.T)\n",
    "\n",
    "        l1_grad = l1_error * active_grad(l1)\n",
    "\n",
    "        # now we want to update our weights\n",
    "        layer3 = layer3 + alpha * l2.T.dot(l3_grad)\n",
    "        layer2 = layer2 + alpha * l1.T.dot(l2_grad)\n",
    "        layer1 = layer1 + alpha * l0.T.dot(l1_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Error:50.5623%\n",
      "Epoch 10 Error:48.2421%\n",
      "Epoch 20 Error:51.7579%\n",
      "Epoch 30 Error:48.2423%\n",
      "Epoch 40 Error:51.7577%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/629854626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/3374923772.py\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(num_epochs, alpha, layer1, layer2, layer3, features, labels, printfreq)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# let's go through and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0ml3_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactive_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0ml2_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lm/86rqpb914xzcnwfy0vfh1fth0000gp/T/ipykernel_2501/902923763.py\u001b[0m in \u001b[0;36mactive_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mactive_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_epochs(8000, .0001, layer1, layer2, layer3, nt_features, nt_labels, printfreq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 48.9585%\n"
     ]
    }
   ],
   "source": [
    "# let's check our test error\n",
    "ntest_labels = test_labels.reshape(len(test_labels), 1)\n",
    "ntest_features = test_features\n",
    "\n",
    "l0 = ntest_features\n",
    "l1 = activate(np.dot(l0, layer1))\n",
    "l2 = activate(np.dot(l1, layer2))\n",
    "l3 = activate(np.dot(l2, layer3))\n",
    "\n",
    "errors = ntest_labels - l3\n",
    "total_error = np.mean(np.abs(errors))\n",
    "print('Test Error: {:2.4f}%'.format(total_error * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
